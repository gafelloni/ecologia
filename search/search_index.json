{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"elapid \u00b6 Contemporary species distribution modeling tools for python. Documentation : earth-chris.github.io/elapid Source code : earth-chris/elapid Introduction \u00b6 elapid provides python support for species distribution modeling. This includes a custom implementation of Maxent and a suite of tools to simplify working with biogeography data. The name is an homage to A Biogeographic Analysis of Australian Elapid Snakes (H.A. Nix, 1986), the paper widely credited with defining the essential bioclimatic variables to use in species distribution modeling. It's also a snake pun (a python wrapper for mapping snake biogeography). Installation \u00b6 pip install elapid This should suffice for most linux/mac users, as there are available unix builds of the underlying python dependencies ( numpy , sklearn , glmnet , rasterio , etc.). Windows installs are more challenging. glmnet has to compile some fortran code on install, meaning you need to have a fortran compiler running (like MinGW-w64 or Cygwin ). You can review Windows install instructions with slightly more detail, or contribute a better solution, at this issue . Package design \u00b6 The amount and quality of bioegeographic data has increased dramatically over the past decade, as have cloud-based tools for working with it. elapid was designed to provide a set of modern, python-based tools for working with species occurrence records and environmental covariates to map different dimensions of a species' niche. elapid supports working with modern geospatial data formats and uses contemporary approaches to training statistical models. It uses sklearn conventions to fit and apply models, rasterio to handle raster operations, geopandas for vector operations, and processes data under the hood with numpy . It does the following things reasonably well: Point sampling Select random geographic point samples (aka background or pseudoabsence points) within polygons or rasters, handling nodata locations, as well as sampling from bias maps (using elapid.sample_geoseries() , elapid.sample_raster() , or elapid.sample_bias_file() ). Vector annotation Annotate point data with coincident raster data, creating GeoDataFrames with sample locations and co-aligned covariate values (using elapid.annotate() ). Zonal statistics Calculate zonal statistics from multi-band, multi-raster data into a single GeoDataFrame from one command (using elapid.zonal_stats() ). Feature transformations Transform covariate data into derivative features to expand data dimensionality (primarily the elapid.MaxentFeatureTransformer() , but see others under elapid.features ) Species distribution modeling Train and apply generic species distribution models (like elapid.MaxentModel() and elapid.NicheEnvelopeModel() ). Applying models to rasters Apply pixel-based models with a .predict() method to rasters (like training a RandomForestClassifier() and applying with elapid.apply_model_to_rasters() ). Cloud-native geo support Work with cloud- or web-hosted raster/vector data (on https:// , gs:// , s3:// , etc.). elapid requires some effort on the user's part to draw samples and extract covariate data. This is by design. Selecting background samples, splitting train/test data, and specifying model parameters are all critical modeling choices that have profound effects on model prediction and interpretation. This extra flexibility provides more control over the seemingly black-box approach of Maxent's java implementation, and enabling users to better tune and evaluate their models. Contact \u00b6","title":"Home"},{"location":"#elapid","text":"Contemporary species distribution modeling tools for python. Documentation : earth-chris.github.io/elapid Source code : earth-chris/elapid","title":"elapid"},{"location":"#introduction","text":"elapid provides python support for species distribution modeling. This includes a custom implementation of Maxent and a suite of tools to simplify working with biogeography data. The name is an homage to A Biogeographic Analysis of Australian Elapid Snakes (H.A. Nix, 1986), the paper widely credited with defining the essential bioclimatic variables to use in species distribution modeling. It's also a snake pun (a python wrapper for mapping snake biogeography).","title":"Introduction"},{"location":"#installation","text":"pip install elapid This should suffice for most linux/mac users, as there are available unix builds of the underlying python dependencies ( numpy , sklearn , glmnet , rasterio , etc.). Windows installs are more challenging. glmnet has to compile some fortran code on install, meaning you need to have a fortran compiler running (like MinGW-w64 or Cygwin ). You can review Windows install instructions with slightly more detail, or contribute a better solution, at this issue .","title":"Installation"},{"location":"#package-design","text":"The amount and quality of bioegeographic data has increased dramatically over the past decade, as have cloud-based tools for working with it. elapid was designed to provide a set of modern, python-based tools for working with species occurrence records and environmental covariates to map different dimensions of a species' niche. elapid supports working with modern geospatial data formats and uses contemporary approaches to training statistical models. It uses sklearn conventions to fit and apply models, rasterio to handle raster operations, geopandas for vector operations, and processes data under the hood with numpy . It does the following things reasonably well: Point sampling Select random geographic point samples (aka background or pseudoabsence points) within polygons or rasters, handling nodata locations, as well as sampling from bias maps (using elapid.sample_geoseries() , elapid.sample_raster() , or elapid.sample_bias_file() ). Vector annotation Annotate point data with coincident raster data, creating GeoDataFrames with sample locations and co-aligned covariate values (using elapid.annotate() ). Zonal statistics Calculate zonal statistics from multi-band, multi-raster data into a single GeoDataFrame from one command (using elapid.zonal_stats() ). Feature transformations Transform covariate data into derivative features to expand data dimensionality (primarily the elapid.MaxentFeatureTransformer() , but see others under elapid.features ) Species distribution modeling Train and apply generic species distribution models (like elapid.MaxentModel() and elapid.NicheEnvelopeModel() ). Applying models to rasters Apply pixel-based models with a .predict() method to rasters (like training a RandomForestClassifier() and applying with elapid.apply_model_to_rasters() ). Cloud-native geo support Work with cloud- or web-hosted raster/vector data (on https:// , gs:// , s3:// , etc.). elapid requires some effort on the user's part to draw samples and extract covariate data. This is by design. Selecting background samples, splitting train/test data, and specifying model parameters are all critical modeling choices that have profound effects on model prediction and interpretation. This extra flexibility provides more control over the seemingly black-box approach of Maxent's java implementation, and enabling users to better tune and evaluate their models.","title":"Package design"},{"location":"#contact","text":"","title":"Contact"},{"location":"examples/geo/","text":"Geospatial Data Support \u00b6 The following code snippets are guidelines for how to work with spatially-explicit datasets in elapid . A note on point-format data types \u00b6 Almost all of the elapid point sampling and indexing tools use geopandas.GeoDataFrame or geopandas.GeoSeries objects. The latter are the format of the geometry column for a GeoDataFrame . import geopandas as gpd gdf = gpd . read_file ( '/path/to/some/vector.shp' ) print ( type ( gdf . geometry )) > < class ' geopandas . geoseries . GeoSeries '> Several elapid routines return GeoSeries objects (like elapid.sample_raster() ) or GeoDataFrame objects (like elapid.annotate() ). It also includes tools for converting x/y data from list, numpy.ndarray , or pandas.DataFrame to GeoSeries . Working with X/Y data \u00b6 From CSVs Sometimes you don't have a vector of point-format location data. The java implementation of maxent uses csv files, for example. You can convert those using the elapid.xy_to_geoseries() function: import pandas as pd csv_path = \"/home/cba/ariolimax-californicus.csv\" df = pd . read_csv ( csv_path ) presence = elapid . xy_to_geoseries ( df . x , df . y , crs = \"EPSG:32610\" ) This assumes that the input CSV file has an x and a y column. Make sure you specify the projection of your x/y data. The default assumption is lat/lon, which in many cases is not correct. From arrays or lists You can also convert arbitrary arrays of x/y data: lons = [ - 122.49 , 151.0 ] lats = [ 37.79 , - 33.87 ] locations = elapid . xy_to_geoseries ( lons , lats ) print ( locations ) > 0 POINT ( - 122.49000 37.79000 ) > 1 POINT ( 151.00000 - 33.87000 ) > dtype : geometry Drawing point location samples \u00b6 In addition to species occurrence records ( y = 1 ), Maxent requires a set of pseudo-absence/background points ( y = 0 ). These are a random geographic sampling of where you might expect to find a species across the target landscape. From a raster's extent You can use elapid to create a random geographic sampling of points from unmasked locations within a raster's extent: count = 10000 # the number of points to generate pseudoabsence_points = elapid . sample_raster ( raster_path , count ) If you have a large raster that doesn't fit in memory, you can also set ignore_mask=True to use the rectangular bounds of the raster to draw samples. pseudoabsence_points = elapid . sample_raster ( raster_path , count , ignore_mask = True ) From a polygon vector Species occurrence records are often biased in their collection (collected near roads, in protected areas, etc.), so we typically need to be more precise in where we select pseudo-absence points. You could use a vector with a species range map to select records: range_path = \"/home/slug/ariolimax-californicus-range.shp\" pseudoabsence_points = elapid . sample_vector ( range_path , count ) This currently computes the spatial union of all polygons within a vector, compressing the geometry into a single MultiPolygon object to sample from. If you've already computed a polygon using geopandas, you can pass it instead to elapid.sample_geoseries() , which is what sample_vector() does under the hood. From a bias raster You could also pass a raster bias file, where the raster grid cells contain information on the probability of sampling an area: # assume covariance between vertebrate and invertebrate banana slugs bias_path = \"/home/slug/proximity-to-ucsc.tif\" pseudoabsence_points = sample_bias_file ( bias_path ) The grid cells can be an arbitrary range of values. What's important is that the values encode a linear range of numbers that are higher where you're more likely to draw a sample. The probability of drawing a sample is dependent on two factors: the range of values provided and the frequency of values across the dataset. So, for a raster with values of 1 and 2 , you're sampling probability for raster locations of 2 is twice that as 1 locations. If these occur in equal frequency (i.e. half the data are 1 valuess, half are 2 values), then you'll likely sample twice as many areas with 2 values. But if the frequency of 1 values is much greater than 2 values, you'll shift the distribution. But you're still more likely, on a per-draw basis, to draw samples from 2 locations. The above example prioritizes sampling frequency in the areas around UC Santa Cruz, home to all types of slug, based on the distance to the campus. Point Annotation \u00b6 Annotation refers to reading and storing raster values at the locations of a series of point occurrence records in a single GeoDataFrame table. Once you have your species presence and pseudo-absence records, you can annotate these records with the covariate data from each location. pseudoabsence_covariates = elapid . annotate ( pseudoabsence_points , list_of_raster_paths , drop_na = True , ) This function, since it's geographically indexed, doesn't require the point data and the raster data to be in the same projection. elapid handles reprojection and sampling on the fly. It also allows you to pass multiple raster files, which can be in different projections, extents, or grid sizes. This means you don't have to explicitly re-sample your raster data prior to analysis, which is always a chore. raster_paths = [ \"/home/slug/california-leaf-area-index.tif\" , # 1-band vegetation data \"/home/slug/global-cloud-cover.tif\" , # 3-band min, mean, max annual cloud cover \"/home/slug/usa-mean-temperature.tif\" , # 1-band mean temperature ] # this fake dataset has five raster bands total, so specify each band label labels = [ \"LAI\" , \"CLD-min\" , \"CLD-mean\" , \"CLD-max\" , \"TMP-mean\" , ] pseudoabsence_covariates = elapid . annotate ( pseudoabsence_points , raster_paths , labels = labels drop_na = True , ) If you don't specify the labels, elapid will assign ['b1', 'b2', ..., 'bn'] labels to each column. Setting drop_na=True requires that the raster datasets have nodata values assigned in their metadata. These point locations will be dropped from the output dataframe, which will have fewer rows than the input points. Zonal statistics \u00b6 In addition to the tools for working with Point data, elapid contains a routine for calculating zonal statistics from Polygon or MutliPolygon geometry types. This routine reads an array of raster data covering the extent of a polygon, masks the areas outside the polygon, and computes summary statistics such as the mean, standard deviation, and mode of the array. ecoregions = gpd . read_file ( '/path/to/california-ecoregions.shp' ) zs = elapid . zonal_stats ( ecoregions , raster_paths , labels = labels , mean = True , stdv = True , percentiles = [ 10 , 90 ], ) Which stats are reported is managed by a set of keywords ( count=True , sum=True , skew=True ). The all=True keyword is a shortcut to compute all of the stats. You'll still have to explicitly pass a list of percentiles , however, like this: zs = elapid . zonal_stats ( ecoregions , raster_paths , labels = labels , all = True , percentiles = [ 25 , 50 , 75 ], ) What sets the elapid zonal stats method apart from other zonal stats packages is it: handles reprojection on the fly, meaning the input vector/raster data don't need to be reprojected a priori handles mutli-band input, computing summary stats on all bands (instead of having to specify which band) handles multi-raster input, reading inputs in serial but creating a single output GeoDataFrame . Applying predictions to data \u00b6 Once you've fit a model (we're skipping a step here, but see the Maxent overview for an example), you can apply it to a set of raster covariates to produce gridded habitat suitability maps. elapid . apply_model_to_rasters ( model , raster_paths , output_path , template_idx = 0 , transform = \"cloglog\" , nodata = - 9999 , ) The list and band order of the rasters must match the order of the covariates used to train the model. It reads each dataset in a block-wise basis, applies the model, and writes gridded predictions. If the raster datasets are not consistent (different extents, resolutions, etc.), it wll re-project the data on the fly, with the grid size, extent and projection based on a 'template' raster. Use the template_idx keyword to specify the index of which raster file to use as the template ( template_idx=0 sets the first raster as the template). In the example above, it's important to set the template to the california-leaf-area-index.tif file. This is because this is the smallest extent with data, and it'll only read and apply the model to the usa and global datasets in the area covered by california . If you were to set the extent to usa-mean-temperature.tif , it would still technically function, but there would be a large region of nodata values where there's insufficient covariate coverage. Applying other model predictions The apply_model_to_rasters() function can be used to apply model predictions from any estimator with a model.predict() method. This includes the majority of sklearn model estimators. If you wanted to train and apply a Random Forest model, you'd use a pattern like this: import elapid from sklearn.ensemble import RandomForestClassifier x , y = elapid . load_sample_data () model = RandomForestClassifier () model . fit ( x , y ) input_rasters = [ '/path/to/raster1.tif' , '/path/to/raster2.tif' ] output_raster = 'rf-model-prediction-categorical.tif' elapid . apply_model_to_rasters ( model , input_rasters , output_raster , ) These models contain an additional method for estimating the continuous prediction probabilities. To write these out, set predict_proba=True . To use this option, however, you also have to specify the number of output raster bands. Since the sample data is a 2-class model, the output prediction probabilities are 2-band outputs, so we set count=2 . output_raster = 'rf-model-prediction-probabilities.tif' elapid . apply_model_to_rasters ( model , input_rasters , output_raster , predict_proba = True , count = 2 , )","title":"Working with Geospatial Data"},{"location":"examples/geo/#geospatial-data-support","text":"The following code snippets are guidelines for how to work with spatially-explicit datasets in elapid .","title":"Geospatial Data Support"},{"location":"examples/geo/#a-note-on-point-format-data-types","text":"Almost all of the elapid point sampling and indexing tools use geopandas.GeoDataFrame or geopandas.GeoSeries objects. The latter are the format of the geometry column for a GeoDataFrame . import geopandas as gpd gdf = gpd . read_file ( '/path/to/some/vector.shp' ) print ( type ( gdf . geometry )) > < class ' geopandas . geoseries . GeoSeries '> Several elapid routines return GeoSeries objects (like elapid.sample_raster() ) or GeoDataFrame objects (like elapid.annotate() ). It also includes tools for converting x/y data from list, numpy.ndarray , or pandas.DataFrame to GeoSeries .","title":"A note on point-format data types"},{"location":"examples/geo/#working-with-xy-data","text":"From CSVs Sometimes you don't have a vector of point-format location data. The java implementation of maxent uses csv files, for example. You can convert those using the elapid.xy_to_geoseries() function: import pandas as pd csv_path = \"/home/cba/ariolimax-californicus.csv\" df = pd . read_csv ( csv_path ) presence = elapid . xy_to_geoseries ( df . x , df . y , crs = \"EPSG:32610\" ) This assumes that the input CSV file has an x and a y column. Make sure you specify the projection of your x/y data. The default assumption is lat/lon, which in many cases is not correct. From arrays or lists You can also convert arbitrary arrays of x/y data: lons = [ - 122.49 , 151.0 ] lats = [ 37.79 , - 33.87 ] locations = elapid . xy_to_geoseries ( lons , lats ) print ( locations ) > 0 POINT ( - 122.49000 37.79000 ) > 1 POINT ( 151.00000 - 33.87000 ) > dtype : geometry","title":"Working with X/Y data"},{"location":"examples/geo/#drawing-point-location-samples","text":"In addition to species occurrence records ( y = 1 ), Maxent requires a set of pseudo-absence/background points ( y = 0 ). These are a random geographic sampling of where you might expect to find a species across the target landscape. From a raster's extent You can use elapid to create a random geographic sampling of points from unmasked locations within a raster's extent: count = 10000 # the number of points to generate pseudoabsence_points = elapid . sample_raster ( raster_path , count ) If you have a large raster that doesn't fit in memory, you can also set ignore_mask=True to use the rectangular bounds of the raster to draw samples. pseudoabsence_points = elapid . sample_raster ( raster_path , count , ignore_mask = True ) From a polygon vector Species occurrence records are often biased in their collection (collected near roads, in protected areas, etc.), so we typically need to be more precise in where we select pseudo-absence points. You could use a vector with a species range map to select records: range_path = \"/home/slug/ariolimax-californicus-range.shp\" pseudoabsence_points = elapid . sample_vector ( range_path , count ) This currently computes the spatial union of all polygons within a vector, compressing the geometry into a single MultiPolygon object to sample from. If you've already computed a polygon using geopandas, you can pass it instead to elapid.sample_geoseries() , which is what sample_vector() does under the hood. From a bias raster You could also pass a raster bias file, where the raster grid cells contain information on the probability of sampling an area: # assume covariance between vertebrate and invertebrate banana slugs bias_path = \"/home/slug/proximity-to-ucsc.tif\" pseudoabsence_points = sample_bias_file ( bias_path ) The grid cells can be an arbitrary range of values. What's important is that the values encode a linear range of numbers that are higher where you're more likely to draw a sample. The probability of drawing a sample is dependent on two factors: the range of values provided and the frequency of values across the dataset. So, for a raster with values of 1 and 2 , you're sampling probability for raster locations of 2 is twice that as 1 locations. If these occur in equal frequency (i.e. half the data are 1 valuess, half are 2 values), then you'll likely sample twice as many areas with 2 values. But if the frequency of 1 values is much greater than 2 values, you'll shift the distribution. But you're still more likely, on a per-draw basis, to draw samples from 2 locations. The above example prioritizes sampling frequency in the areas around UC Santa Cruz, home to all types of slug, based on the distance to the campus.","title":"Drawing point location samples"},{"location":"examples/geo/#point-annotation","text":"Annotation refers to reading and storing raster values at the locations of a series of point occurrence records in a single GeoDataFrame table. Once you have your species presence and pseudo-absence records, you can annotate these records with the covariate data from each location. pseudoabsence_covariates = elapid . annotate ( pseudoabsence_points , list_of_raster_paths , drop_na = True , ) This function, since it's geographically indexed, doesn't require the point data and the raster data to be in the same projection. elapid handles reprojection and sampling on the fly. It also allows you to pass multiple raster files, which can be in different projections, extents, or grid sizes. This means you don't have to explicitly re-sample your raster data prior to analysis, which is always a chore. raster_paths = [ \"/home/slug/california-leaf-area-index.tif\" , # 1-band vegetation data \"/home/slug/global-cloud-cover.tif\" , # 3-band min, mean, max annual cloud cover \"/home/slug/usa-mean-temperature.tif\" , # 1-band mean temperature ] # this fake dataset has five raster bands total, so specify each band label labels = [ \"LAI\" , \"CLD-min\" , \"CLD-mean\" , \"CLD-max\" , \"TMP-mean\" , ] pseudoabsence_covariates = elapid . annotate ( pseudoabsence_points , raster_paths , labels = labels drop_na = True , ) If you don't specify the labels, elapid will assign ['b1', 'b2', ..., 'bn'] labels to each column. Setting drop_na=True requires that the raster datasets have nodata values assigned in their metadata. These point locations will be dropped from the output dataframe, which will have fewer rows than the input points.","title":"Point Annotation"},{"location":"examples/geo/#zonal-statistics","text":"In addition to the tools for working with Point data, elapid contains a routine for calculating zonal statistics from Polygon or MutliPolygon geometry types. This routine reads an array of raster data covering the extent of a polygon, masks the areas outside the polygon, and computes summary statistics such as the mean, standard deviation, and mode of the array. ecoregions = gpd . read_file ( '/path/to/california-ecoregions.shp' ) zs = elapid . zonal_stats ( ecoregions , raster_paths , labels = labels , mean = True , stdv = True , percentiles = [ 10 , 90 ], ) Which stats are reported is managed by a set of keywords ( count=True , sum=True , skew=True ). The all=True keyword is a shortcut to compute all of the stats. You'll still have to explicitly pass a list of percentiles , however, like this: zs = elapid . zonal_stats ( ecoregions , raster_paths , labels = labels , all = True , percentiles = [ 25 , 50 , 75 ], ) What sets the elapid zonal stats method apart from other zonal stats packages is it: handles reprojection on the fly, meaning the input vector/raster data don't need to be reprojected a priori handles mutli-band input, computing summary stats on all bands (instead of having to specify which band) handles multi-raster input, reading inputs in serial but creating a single output GeoDataFrame .","title":"Zonal statistics"},{"location":"examples/geo/#applying-predictions-to-data","text":"Once you've fit a model (we're skipping a step here, but see the Maxent overview for an example), you can apply it to a set of raster covariates to produce gridded habitat suitability maps. elapid . apply_model_to_rasters ( model , raster_paths , output_path , template_idx = 0 , transform = \"cloglog\" , nodata = - 9999 , ) The list and band order of the rasters must match the order of the covariates used to train the model. It reads each dataset in a block-wise basis, applies the model, and writes gridded predictions. If the raster datasets are not consistent (different extents, resolutions, etc.), it wll re-project the data on the fly, with the grid size, extent and projection based on a 'template' raster. Use the template_idx keyword to specify the index of which raster file to use as the template ( template_idx=0 sets the first raster as the template). In the example above, it's important to set the template to the california-leaf-area-index.tif file. This is because this is the smallest extent with data, and it'll only read and apply the model to the usa and global datasets in the area covered by california . If you were to set the extent to usa-mean-temperature.tif , it would still technically function, but there would be a large region of nodata values where there's insufficient covariate coverage. Applying other model predictions The apply_model_to_rasters() function can be used to apply model predictions from any estimator with a model.predict() method. This includes the majority of sklearn model estimators. If you wanted to train and apply a Random Forest model, you'd use a pattern like this: import elapid from sklearn.ensemble import RandomForestClassifier x , y = elapid . load_sample_data () model = RandomForestClassifier () model . fit ( x , y ) input_rasters = [ '/path/to/raster1.tif' , '/path/to/raster2.tif' ] output_raster = 'rf-model-prediction-categorical.tif' elapid . apply_model_to_rasters ( model , input_rasters , output_raster , ) These models contain an additional method for estimating the continuous prediction probabilities. To write these out, set predict_proba=True . To use this option, however, you also have to specify the number of output raster bands. Since the sample data is a 2-class model, the output prediction probabilities are 2-band outputs, so we set count=2 . output_raster = 'rf-model-prediction-probabilities.tif' elapid . apply_model_to_rasters ( model , input_rasters , output_raster , predict_proba = True , count = 2 , )","title":"Applying predictions to data"},{"location":"examples/maxent-simple/","text":"Here's an example end-to-end example for training a Maxent model (using dummy paths to demonstrate functionality). This is to demonstrate the simplest pattern of model training. Full model training and evaluation should include creating train/test splits, cross-validation, feature selection, etc. These are not covered here. import elapid vector_path = \"/home/slug/ariolimax-californicus.shp\" raster_path = \"/home/slug/california-climate-veg.tif\" output_raster_path = \"/home/slug/ariolimax-californicus-habitat.tif\" output_model_path = \"/home/slug/ariolimax-claifornicus-model.ela\" # sample the raster values for background point locations pseudoabsence_points = elapid . saple_raster ( raster_path , count = 5000 ) # read the raster covariates at each point location presence = elapid . annotate ( vector_path , raster_path ) pseudoabsence = elapid . annotate ( pseudoabsence_points , raster_path ) # merge the datasets into one dataframe pseudoabsence [ 'presence' ] = 0 presence [ 'presence' ] = 1 merged = presence . append ( pseudoabsence ) . reset_index ( drop = True ) x = merged . drop ( columns = [ 'presence' ]) y = merged [ 'presence' ] # train the model model = elapid . MaxentModel () model . fit ( x , y ) # apply it to the full extent and save the model for later elapid . apply_model_to_rasters ( model , raster_path , output_raster_path , transform = \"cloglog\" ) elapid . save_object ( model , output_model_path ) To work with this saved model later, you can run: model = elapid . load_object ( model_path )","title":"A Simple Maxent Model"},{"location":"geo/annotation/","text":"Annotation refers to reading and storing raster values at the locations of a series of point occurrence records. Once you have your species presence and pseudo-absence records, you can annotate these records with the covariate data from each location. pseudoabsence_covariates = elapid . raster_values_from_geoseries ( pseudoabsence_points , raster_path , drop_na = True , ) This could also be done with raster_values_from_vector(vector_path, raster_path) if you haven't already loaded the geoseries data into memory. This function, since it's geographically indexed, doesn't require the point data and the raster data to be in the same projection. elapid handles reprojection and sampling on the fly. It also allows you to pass multiple raster files, which can be in different projections, extents, or grid sizes. This means you don't have to explicitly re-sample your raster data prior to analysis, which is always a chore. raster_paths = [ \"/home/slug/california-leaf-area-index.tif\" , # 1-band vegetation data \"/home/slug/global-cloud-cover.tif\" , # 3-band min, mean, max annual cloud cover \"/home/slug/usa-mean-temperature.tif\" , # 1-band mean temperature ] # since you have five raster bands total, specify each band label labels = [ \"LAI\" , \"CLD-min\" , \"CLD-mean\" , \"CLD-max\" , \"TMP-mean\" , ] pseudoabsence_covariates = elapid . raster_values_from_geoseries ( pseudoabsence_points , raster_paths , labels = labels drop_na = True , ) If you don't specify the labels, elapid will assign ['band_001', 'band_002', ...] .","title":"Annotation"},{"location":"geo/prediction/","text":"Once you've fit a model, you can apply it to a set of raster covariates to produce gridded suitability maps. elapid . apply_model_to_rasters ( model , raster_paths , output_path , template_idx = 0 , transform = \"cloglog\" , nodata =- 9999 , ) The list and band order of the rasters must match the order of the covariates used to train the model. It reads each dataset in a block-wise basis, applies the model, and writes gridded predictions. If the raster datasets are not consistent (different extents, resolutions, etc.), it wll re-project the data on the fly, with the grid size, extent and projection based on a 'template' raster. Use the template_idx keyword to specify the index of which raster file to use as the template ( template_idx=0 sets the first raster as the template). In the example above, it's important to set the template to the california-leaf-area-index.tif file. This is because this is the smallest extent with data, and it'll only read and apply the model to the usa and global datasets in the area covered by california . If you were to set the extent to usa-mean-temperature.tif , it would still technically function, but there would be a large region of nodata values where there's insufficient covariate coverage.","title":"Prediction"},{"location":"geo/pseudoabsence/","text":"In addition to species occurrence records, maxent requires a set of pseudo-absence (i.e. background) points. These are a random geographic samping of where you might expect to find a species. From a raster's extent \u00b6 You can use elapid to create a random geographic sampling of points from unmasked locations within a raster's extent: count = 10000 # the number of points to generate pseudoabsence_points = elapid . pseudoabsence_from_raster ( raster_path , count ) From a vector polygon \u00b6 Species occurrence records are often biased in their collection (collected near roads, in protected areas, etc.), so we typically need to be more precise in where we select pseudo-absence points. You could use a vector with a species range map to select records: range_path = \"/home/slug/ariolimax-californicus-range.shp\" pseudoabsence_points = elapid . pseudoabsence_from_vector ( range_path , count ) If you've already computed a polygon using geopandas, you can pass it instead to elapid.pseudoabsence_from_geoseries() , which is what pseudoabsence_from_vector() does under the hood. From a bias raster \u00b6 You could also pass a raster bias file, where the raster grid cells contain information on the probability of sampling an area: # assume covariance between vertebrate and invertebrate banana slugs bias_path = \"/home/slug/proximity-to-ucsc.tif\" pseudoabsence_points = pseudoabsence_from_bias_file ( bias_path ) The grid cells can be an arbitrary range of values. What's important is that the values encode a linear range of numbers that are higher where you're more likely to draw a sample. The probability of drawing a sample is dependent on two factors: the range of values provided and the frequency of values across the dataset. So, for a raster with values of 1 and 2 , you're sampling probability for raster locations of 2 is twice that as 1 locations. If these occur in equal frequency (i.e. half the data are 1 valuess, half are 2 values), then you'll likely sample twice as many areas with 2 values. But if the frequency of 1 values is much greater than 2 values, you'll shift the distribution. But you're still more likely, on a per-draw basis, to draw samples from 2 locations. The above example prioritizes sampling frequency in the areas around UC Santa Cruz, home to all types of slug, based on the distance to the campus.","title":"Pseudoabsence"},{"location":"geo/pseudoabsence/#from-a-rasters-extent","text":"You can use elapid to create a random geographic sampling of points from unmasked locations within a raster's extent: count = 10000 # the number of points to generate pseudoabsence_points = elapid . pseudoabsence_from_raster ( raster_path , count )","title":"From a raster's extent"},{"location":"geo/pseudoabsence/#from-a-vector-polygon","text":"Species occurrence records are often biased in their collection (collected near roads, in protected areas, etc.), so we typically need to be more precise in where we select pseudo-absence points. You could use a vector with a species range map to select records: range_path = \"/home/slug/ariolimax-californicus-range.shp\" pseudoabsence_points = elapid . pseudoabsence_from_vector ( range_path , count ) If you've already computed a polygon using geopandas, you can pass it instead to elapid.pseudoabsence_from_geoseries() , which is what pseudoabsence_from_vector() does under the hood.","title":"From a vector polygon"},{"location":"geo/pseudoabsence/#from-a-bias-raster","text":"You could also pass a raster bias file, where the raster grid cells contain information on the probability of sampling an area: # assume covariance between vertebrate and invertebrate banana slugs bias_path = \"/home/slug/proximity-to-ucsc.tif\" pseudoabsence_points = pseudoabsence_from_bias_file ( bias_path ) The grid cells can be an arbitrary range of values. What's important is that the values encode a linear range of numbers that are higher where you're more likely to draw a sample. The probability of drawing a sample is dependent on two factors: the range of values provided and the frequency of values across the dataset. So, for a raster with values of 1 and 2 , you're sampling probability for raster locations of 2 is twice that as 1 locations. If these occur in equal frequency (i.e. half the data are 1 valuess, half are 2 values), then you'll likely sample twice as many areas with 2 values. But if the frequency of 1 values is much greater than 2 values, you'll shift the distribution. But you're still more likely, on a per-draw basis, to draw samples from 2 locations. The above example prioritizes sampling frequency in the areas around UC Santa Cruz, home to all types of slug, based on the distance to the campus.","title":"From a bias raster"},{"location":"geo/working-xy/","text":"Almost all of the data sampling and indexing uses geopandas.GeoSeries objects. These are the format of the geometry column for a GeoDataFrame . import geopandas as gpd gdf = gpd . read_file ( vector_path ) print ( type ( gdf . geometry )) > < class ' geopandas . geoseries . GeoSeries '> From CSVs \u00b6 Sometimes you don't have a vector of point-format location data. The java implementation of maxent uses csv files, for example. You can work with those using the xy_to_geoseries function: import pandas as pd csv_path = \"/home/cba/ariolimax-californicus.csv\" df = pd . read_csv ( csv_path ) presence = elapid . xy_to_geoseries ( df . x , df . y , crs = \"EPSG:32610\" ) Make sure you specify the projection of your x/y data. The default assumption is lat/lon, which in many cases is not correct. From arrays or lists \u00b6 You can also convert arbitrary arrays of x/y data: lons = [ - 122.49 , 151.0 ] lats = [ 37.79 , - 33.87 ] locations = elapid . xy_to_geoseries ( lons , lats ) print ( locations ) > 0 POINT ( - 122.49000 37.79000 ) > 1 POINT ( 151.00000 - 33.87000 ) > dtype : geometry","title":"Working xy"},{"location":"geo/working-xy/#from-csvs","text":"Sometimes you don't have a vector of point-format location data. The java implementation of maxent uses csv files, for example. You can work with those using the xy_to_geoseries function: import pandas as pd csv_path = \"/home/cba/ariolimax-californicus.csv\" df = pd . read_csv ( csv_path ) presence = elapid . xy_to_geoseries ( df . x , df . y , crs = \"EPSG:32610\" ) Make sure you specify the projection of your x/y data. The default assumption is lat/lon, which in many cases is not correct.","title":"From CSVs"},{"location":"geo/working-xy/#from-arrays-or-lists","text":"You can also convert arbitrary arrays of x/y data: lons = [ - 122.49 , 151.0 ] lats = [ 37.79 , - 33.87 ] locations = elapid . xy_to_geoseries ( lons , lats ) print ( locations ) > 0 POINT ( - 122.49000 37.79000 ) > 1 POINT ( 151.00000 - 33.87000 ) > dtype : geometry","title":"From arrays or lists"},{"location":"module/features/","text":"elapid.features \u00b6 Functions to transform covariate data into complex model features. CategoricalTransformer ( BaseEstimator ) \u00b6 Applies one-hot encoding to categorical covariate datasets. Source code in elapid/features.py class CategoricalTransformer ( BaseEstimator ): \"\"\"Applies one-hot encoding to categorical covariate datasets.\"\"\" estimators_ : list = None def __init__ ( self ): pass def fit ( self , x : ArrayLike ): \"\"\"Compute the minimum and maximum for scaling. Args: x: array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. Returns: None. Updates the transformer with feature fitting parameters. \"\"\" self . estimators_ = [] x = np . array ( x ) if x . ndim == 1 : estimator = OneHotEncoder ( dtype = np . uint8 , sparse = False ) self . estimators_ . append ( estimator . fit ( x . reshape ( - 1 , 1 ))) else : nrows , ncols = x . shape for col in range ( ncols ): xsub = x [:, col ] . reshape ( - 1 , 1 ) estimator = OneHotEncoder ( dtype = np . uint8 , sparse = False ) self . estimators_ . append ( estimator . fit ( xsub )) def transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Scale covariates according to the feature range. Args: x: array-like of shape (n_samples, n_features) Input data that will be transformed. Returns: ndarray with transformed data. \"\"\" x = np . array ( x ) if x . ndim == 1 : estimator = self . estimators_ [ 0 ] return estimator . transform ( x . reshape ( - 1 , 1 )) else : class_data = [] nrows , ncols = x . shape for col in range ( ncols ): xsub = x [:, col ] . reshape ( - 1 , 1 ) estimator = self . estimators_ [ col ] class_data . append ( estimator . transform ( xsub )) return np . concatenate ( class_data , axis = 1 ) def fit_transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Fits scaler to x and returns transformed features. Args: x: array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform. Returns: ndarray with transformed data. \"\"\" self . fit ( x ) return self . transform ( x ) fit ( self , x ) \u00b6 Compute the minimum and maximum for scaling. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. required Returns: Type Description None. Updates the transformer with feature fitting parameters. Source code in elapid/features.py def fit ( self , x : ArrayLike ): \"\"\"Compute the minimum and maximum for scaling. Args: x: array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. Returns: None. Updates the transformer with feature fitting parameters. \"\"\" self . estimators_ = [] x = np . array ( x ) if x . ndim == 1 : estimator = OneHotEncoder ( dtype = np . uint8 , sparse = False ) self . estimators_ . append ( estimator . fit ( x . reshape ( - 1 , 1 ))) else : nrows , ncols = x . shape for col in range ( ncols ): xsub = x [:, col ] . reshape ( - 1 , 1 ) estimator = OneHotEncoder ( dtype = np . uint8 , sparse = False ) self . estimators_ . append ( estimator . fit ( xsub )) fit_transform ( self , x ) \u00b6 Fits scaler to x and returns transformed features. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform. required Returns: Type Description ndarray ndarray with transformed data. Source code in elapid/features.py def fit_transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Fits scaler to x and returns transformed features. Args: x: array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform. Returns: ndarray with transformed data. \"\"\" self . fit ( x ) return self . transform ( x ) transform ( self , x ) \u00b6 Scale covariates according to the feature range. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) Input data that will be transformed. required Returns: Type Description ndarray ndarray with transformed data. Source code in elapid/features.py def transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Scale covariates according to the feature range. Args: x: array-like of shape (n_samples, n_features) Input data that will be transformed. Returns: ndarray with transformed data. \"\"\" x = np . array ( x ) if x . ndim == 1 : estimator = self . estimators_ [ 0 ] return estimator . transform ( x . reshape ( - 1 , 1 )) else : class_data = [] nrows , ncols = x . shape for col in range ( ncols ): xsub = x [:, col ] . reshape ( - 1 , 1 ) estimator = self . estimators_ [ col ] class_data . append ( estimator . transform ( xsub )) return np . concatenate ( class_data , axis = 1 ) CumulativeTransformer ( QuantileTransformer ) \u00b6 Applies a percentile-based transform to estimate cumulative suitability. Source code in elapid/features.py class CumulativeTransformer ( QuantileTransformer ): \"\"\"Applies a percentile-based transform to estimate cumulative suitability.\"\"\" def __init__ ( self ): super () . __init__ ( n_quantiles = 100 , output_distribution = \"uniform\" ) HingeTransformer ( BaseEstimator ) \u00b6 Fits hinge transformations to an array of covariates. Source code in elapid/features.py class HingeTransformer ( BaseEstimator ): \"\"\"Fits hinge transformations to an array of covariates.\"\"\" n_hinges_ : int = None mins_ : np . ndarray = None maxs_ : np . ndarray = None hinge_indices_ : np . ndarray = None def __init__ ( self , n_hinges : int = MaxentConfig . n_hinge_features ): self . n_hinges_ = n_hinges def fit ( self , x : ArrayLike ): \"\"\"Compute the minimum and maximum for scaling. Args: x: array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. Returns: None. Updates the transformer with feature fitting parameters. \"\"\" x = np . array ( x ) self . mins_ = x . min ( axis = 0 ) self . maxs_ = x . max ( axis = 0 ) self . hinge_indices_ = np . linspace ( self . mins_ , self . maxs_ , self . n_hinges_ ) def transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Scale covariates according to the feature range. Args: x: array-like of shape (n_samples, n_features) Input data that will be transformed. Returns: ndarray with transformed data. \"\"\" x = np . array ( x ) xarr = repeat_array ( x , self . n_hinges_ - 1 , axis =- 1 ) lharr = repeat_array ( self . hinge_indices_ [: - 1 ] . transpose (), len ( x ), axis = 0 ) rharr = repeat_array ( self . hinge_indices_ [ 1 :] . transpose (), len ( x ), axis = 0 ) lh = left_hinge ( xarr , lharr , self . maxs_ ) rh = right_hinge ( xarr , self . mins_ , rharr ) return np . concatenate (( lh , rh ), axis = 2 ) . reshape ( x . shape [ 0 ], - 1 ) def fit_transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Fits scaler to x and returns transformed features. Args: x: array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform. Returns: ndarray with transformed data. \"\"\" self . fit ( x ) return self . transform ( x ) fit ( self , x ) \u00b6 Compute the minimum and maximum for scaling. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. required Returns: Type Description None. Updates the transformer with feature fitting parameters. Source code in elapid/features.py def fit ( self , x : ArrayLike ): \"\"\"Compute the minimum and maximum for scaling. Args: x: array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. Returns: None. Updates the transformer with feature fitting parameters. \"\"\" x = np . array ( x ) self . mins_ = x . min ( axis = 0 ) self . maxs_ = x . max ( axis = 0 ) self . hinge_indices_ = np . linspace ( self . mins_ , self . maxs_ , self . n_hinges_ ) fit_transform ( self , x ) \u00b6 Fits scaler to x and returns transformed features. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform. required Returns: Type Description ndarray ndarray with transformed data. Source code in elapid/features.py def fit_transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Fits scaler to x and returns transformed features. Args: x: array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform. Returns: ndarray with transformed data. \"\"\" self . fit ( x ) return self . transform ( x ) transform ( self , x ) \u00b6 Scale covariates according to the feature range. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) Input data that will be transformed. required Returns: Type Description ndarray ndarray with transformed data. Source code in elapid/features.py def transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Scale covariates according to the feature range. Args: x: array-like of shape (n_samples, n_features) Input data that will be transformed. Returns: ndarray with transformed data. \"\"\" x = np . array ( x ) xarr = repeat_array ( x , self . n_hinges_ - 1 , axis =- 1 ) lharr = repeat_array ( self . hinge_indices_ [: - 1 ] . transpose (), len ( x ), axis = 0 ) rharr = repeat_array ( self . hinge_indices_ [ 1 :] . transpose (), len ( x ), axis = 0 ) lh = left_hinge ( xarr , lharr , self . maxs_ ) rh = right_hinge ( xarr , self . mins_ , rharr ) return np . concatenate (( lh , rh ), axis = 2 ) . reshape ( x . shape [ 0 ], - 1 ) LinearTransformer ( MinMaxScaler ) \u00b6 Applies linear feature transformations to rescale features from 0-1. Source code in elapid/features.py class LinearTransformer ( MinMaxScaler ): \"\"\"Applies linear feature transformations to rescale features from 0-1.\"\"\" clamp : bool = None feature_range : None def __init__ ( self , clamp : bool = MaxentConfig . clamp , feature_range : Tuple [ float , float ] = ( 0.0 , 1.0 ), ): self . clamp = clamp self . feature_range = feature_range super () . __init__ ( clip = clamp , feature_range = feature_range ) MaxentFeatureTransformer ( BaseEstimator ) \u00b6 Transforms covariate data into maxent-format feature data. Source code in elapid/features.py class MaxentFeatureTransformer ( BaseEstimator ): \"\"\"Transforms covariate data into maxent-format feature data.\"\"\" feature_types_ : list = None clamp_ : bool = None n_hinge_features_ : int = None n_threshold_features_ : int = None categorical_ : list = None continuous_ : list = None categorical_pd_ : list = None continuous_pd_ : list = None labels_ : list = None estimators_ : dict = { \"linear\" : None , \"quadratic\" : None , \"product\" : None , \"threshold\" : None , \"hinge\" : None , \"categorical\" : None , } feature_names_ : list = None def __init__ ( self , feature_types : Union [ str , list ] = MaxentConfig . feature_types , clamp : bool = MaxentConfig . clamp , n_hinge_features : int = MaxentConfig . n_hinge_features , n_threshold_features : int = MaxentConfig . n_threshold_features , ): \"\"\"Computes features based on the maxent feature types specified (like linear, quadratic, hinge). Implemented using sklearn conventions (with `.fit()` and `.transform()` functions. Args: feature_types: list of maxent features to generate. clamp: set feature values to global mins/maxs during prediction n_hinge_features: number of hinge knots to generate n_threshold_features: nuber of threshold features to generate \"\"\" self . feature_types_ = validate_feature_types ( feature_types ) self . clamp_ = validate_boolean ( clamp ) self . n_hinge_features_ = validate_numeric_scalar ( n_hinge_features ) self . n_threshold_features_ = validate_numeric_scalar ( n_threshold_features ) def _format_covariate_data ( self , x : ArrayLike ) -> Tuple [ np . array , np . array ]: \"\"\"Reads input x data and formats it to consistent array dtypes. Args: x: array-like of shape (n_samples, n_features) Returns: (continuous, categorical) tuple of ndarrays with continuous and categorical covariate data. \"\"\" if isinstance ( x , np . ndarray ): if self . categorical_ is None : con = x cat = None else : con = x [:, self . continuous_ ] cat = x [:, self . categorical_ ] elif isinstance ( x , pd . DataFrame ): con = x [ self . continuous_pd_ ] . to_numpy () if len ( self . categorical_pd_ ) > 0 : cat = x [ self . categorical_pd_ ] . to_numpy () else : cat = None else : raise TypeError ( f \"Unsupported x dtype: { type ( x ) } . Must be pd.DataFrame or np.array\" ) return con , cat def _format_labels_and_dtypes ( self , x : ArrayLike , categorical : list = None , labels : list = None ) -> None : \"\"\"Read input x data and lists of categorical data indices and band labels to format and store this info for later indexing. Args: s: array-like of shape (n_samples, n_features) categorical: indices indicating which x columns are categorical labels: covariate column labels. ignored if x is a pandas DataFrame \"\"\" if isinstance ( x , np . ndarray ): nrows , ncols = x . shape if categorical is None : continuous = list ( range ( ncols )) else : continuous = list ( set ( range ( ncols )) . difference ( set ( categorical ))) self . labels_ = labels or make_band_labels ( ncols ) self . categorical_ = categorical self . continuous_ = continuous elif isinstance ( x , pd . DataFrame ): x . drop ([ \"geometry\" ], axis = 1 , errors = \"ignore\" , inplace = True ) self . labels_ = labels or list ( x . columns ) # store both pandas and numpy indexing of these values self . continuous_pd_ = list ( x . select_dtypes ( exclude = \"category\" ) . columns ) self . categorical_pd_ = list ( x . select_dtypes ( include = \"category\" ) . columns ) all_columns = list ( x . columns ) self . continuous_ = [ all_columns . index ( item ) for item in self . continuous_pd_ if item in all_columns ] if len ( self . categorical_pd_ ) != 0 : self . categorical_ = [ all_columns . index ( item ) for item in self . categorical_pd_ if item in all_columns ] else : self . categorical_ = None def fit ( self , x : ArrayLike , categorical : list = None , labels : list = None ) -> None : \"\"\"Compute the minimum and maximum for scaling. Args: x: array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. categorical: indices indicating which x columns are categorical labels: covariate column labels. ignored if x is a pandas DataFrame Returns: None. Updates the transformer with feature fitting parameters. \"\"\" self . _format_labels_and_dtypes ( x , categorical = categorical , labels = labels ) con , cat = self . _format_covariate_data ( x ) nrows , ncols = con . shape feature_names = [] if \"linear\" in self . feature_types_ : estimator = LinearTransformer ( clamp = self . clamp_ ) estimator . fit ( con ) self . estimators_ [ \"linear\" ] = estimator feature_names += [ \"linear\" ] * estimator . n_features_in_ if \"quadratic\" in self . feature_types_ : estimator = QuadraticTransformer ( clamp = self . clamp_ ) estimator . fit ( con ) self . estimators_ [ \"quadratic\" ] = estimator feature_names += [ \"quadratic\" ] * estimator . estimator . n_features_in_ if \"product\" in self . feature_types_ : estimator = ProductTransformer ( clamp = self . clamp_ ) estimator . fit ( con ) self . estimators_ [ \"product\" ] = estimator feature_names += [ \"product\" ] * estimator . estimator . n_features_in_ if \"threshold\" in self . feature_types_ : estimator = ThresholdTransformer ( n_thresholds = self . n_threshold_features_ ) estimator . fit ( con ) self . estimators_ [ \"threshold\" ] = estimator feature_names += [ \"threshold\" ] * ( estimator . n_thresholds_ * ncols ) if \"hinge\" in self . feature_types_ : estimator = HingeTransformer ( n_hinges = self . n_hinge_features_ ) estimator . fit ( con ) self . estimators_ [ \"hinge\" ] = estimator feature_names += [ \"hinge\" ] * (( estimator . n_hinges_ - 1 ) * 2 * ncols ) if cat is not None : estimator = CategoricalTransformer () estimator . fit ( cat ) self . estimators_ [ \"categorical\" ] = estimator for est in estimator . estimators_ : feature_names += [ \"categorical\" ] * len ( est . categories_ [ 0 ]) self . feature_names_ = feature_names def transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Scale covariates according to the feature range. Args: x: array-like of shape (n_samples, n_features) Input data that will be transformed. Returns: ndarray with transformed data. \"\"\" con , cat = self . _format_covariate_data ( x ) features = [] if \"linear\" in self . feature_types_ : features . append ( self . estimators_ [ \"linear\" ] . transform ( con )) if \"quadratic\" in self . feature_types_ : features . append ( self . estimators_ [ \"quadratic\" ] . transform ( con )) if \"product\" in self . feature_types_ : features . append ( self . estimators_ [ \"product\" ] . transform ( con )) if \"threshold\" in self . feature_types_ : features . append ( self . estimators_ [ \"threshold\" ] . transform ( con )) if \"hinge\" in self . feature_types_ : features . append ( self . estimators_ [ \"hinge\" ] . transform ( con )) if cat is not None : features . append ( self . estimators_ [ \"categorical\" ] . transform ( cat )) return np . concatenate ( features , axis = 1 ) def fit_transform ( self , x : ArrayLike , categorical : list = None , labels : list = None ) -> np . ndarray : \"\"\"Fits scaler to x and returns transformed features. Args: x: array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform. Returns: ndarray with transformed data. \"\"\" self . fit ( x , categorical = categorical , labels = labels ) return self . transform ( x ) __init__ ( self , feature_types = [ 'linear' , 'hinge' , 'product' ], clamp = True , n_hinge_features = 10 , n_threshold_features = 10 ) special \u00b6 Computes features based on the maxent feature types specified (like linear, quadratic, hinge). Implemented using sklearn conventions (with .fit() and .transform() functions. Parameters: Name Type Description Default feature_types Union[str, list] list of maxent features to generate. ['linear', 'hinge', 'product'] clamp bool set feature values to global mins/maxs during prediction True n_hinge_features int number of hinge knots to generate 10 n_threshold_features int nuber of threshold features to generate 10 Source code in elapid/features.py def __init__ ( self , feature_types : Union [ str , list ] = MaxentConfig . feature_types , clamp : bool = MaxentConfig . clamp , n_hinge_features : int = MaxentConfig . n_hinge_features , n_threshold_features : int = MaxentConfig . n_threshold_features , ): \"\"\"Computes features based on the maxent feature types specified (like linear, quadratic, hinge). Implemented using sklearn conventions (with `.fit()` and `.transform()` functions. Args: feature_types: list of maxent features to generate. clamp: set feature values to global mins/maxs during prediction n_hinge_features: number of hinge knots to generate n_threshold_features: nuber of threshold features to generate \"\"\" self . feature_types_ = validate_feature_types ( feature_types ) self . clamp_ = validate_boolean ( clamp ) self . n_hinge_features_ = validate_numeric_scalar ( n_hinge_features ) self . n_threshold_features_ = validate_numeric_scalar ( n_threshold_features ) fit ( self , x , categorical = None , labels = None ) \u00b6 Compute the minimum and maximum for scaling. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. required categorical list indices indicating which x columns are categorical None labels list covariate column labels. ignored if x is a pandas DataFrame None Returns: Type Description None None. Updates the transformer with feature fitting parameters. Source code in elapid/features.py def fit ( self , x : ArrayLike , categorical : list = None , labels : list = None ) -> None : \"\"\"Compute the minimum and maximum for scaling. Args: x: array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. categorical: indices indicating which x columns are categorical labels: covariate column labels. ignored if x is a pandas DataFrame Returns: None. Updates the transformer with feature fitting parameters. \"\"\" self . _format_labels_and_dtypes ( x , categorical = categorical , labels = labels ) con , cat = self . _format_covariate_data ( x ) nrows , ncols = con . shape feature_names = [] if \"linear\" in self . feature_types_ : estimator = LinearTransformer ( clamp = self . clamp_ ) estimator . fit ( con ) self . estimators_ [ \"linear\" ] = estimator feature_names += [ \"linear\" ] * estimator . n_features_in_ if \"quadratic\" in self . feature_types_ : estimator = QuadraticTransformer ( clamp = self . clamp_ ) estimator . fit ( con ) self . estimators_ [ \"quadratic\" ] = estimator feature_names += [ \"quadratic\" ] * estimator . estimator . n_features_in_ if \"product\" in self . feature_types_ : estimator = ProductTransformer ( clamp = self . clamp_ ) estimator . fit ( con ) self . estimators_ [ \"product\" ] = estimator feature_names += [ \"product\" ] * estimator . estimator . n_features_in_ if \"threshold\" in self . feature_types_ : estimator = ThresholdTransformer ( n_thresholds = self . n_threshold_features_ ) estimator . fit ( con ) self . estimators_ [ \"threshold\" ] = estimator feature_names += [ \"threshold\" ] * ( estimator . n_thresholds_ * ncols ) if \"hinge\" in self . feature_types_ : estimator = HingeTransformer ( n_hinges = self . n_hinge_features_ ) estimator . fit ( con ) self . estimators_ [ \"hinge\" ] = estimator feature_names += [ \"hinge\" ] * (( estimator . n_hinges_ - 1 ) * 2 * ncols ) if cat is not None : estimator = CategoricalTransformer () estimator . fit ( cat ) self . estimators_ [ \"categorical\" ] = estimator for est in estimator . estimators_ : feature_names += [ \"categorical\" ] * len ( est . categories_ [ 0 ]) self . feature_names_ = feature_names fit_transform ( self , x , categorical = None , labels = None ) \u00b6 Fits scaler to x and returns transformed features. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform. required Returns: Type Description ndarray ndarray with transformed data. Source code in elapid/features.py def fit_transform ( self , x : ArrayLike , categorical : list = None , labels : list = None ) -> np . ndarray : \"\"\"Fits scaler to x and returns transformed features. Args: x: array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform. Returns: ndarray with transformed data. \"\"\" self . fit ( x , categorical = categorical , labels = labels ) return self . transform ( x ) transform ( self , x ) \u00b6 Scale covariates according to the feature range. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) Input data that will be transformed. required Returns: Type Description ndarray ndarray with transformed data. Source code in elapid/features.py def transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Scale covariates according to the feature range. Args: x: array-like of shape (n_samples, n_features) Input data that will be transformed. Returns: ndarray with transformed data. \"\"\" con , cat = self . _format_covariate_data ( x ) features = [] if \"linear\" in self . feature_types_ : features . append ( self . estimators_ [ \"linear\" ] . transform ( con )) if \"quadratic\" in self . feature_types_ : features . append ( self . estimators_ [ \"quadratic\" ] . transform ( con )) if \"product\" in self . feature_types_ : features . append ( self . estimators_ [ \"product\" ] . transform ( con )) if \"threshold\" in self . feature_types_ : features . append ( self . estimators_ [ \"threshold\" ] . transform ( con )) if \"hinge\" in self . feature_types_ : features . append ( self . estimators_ [ \"hinge\" ] . transform ( con )) if cat is not None : features . append ( self . estimators_ [ \"categorical\" ] . transform ( cat )) return np . concatenate ( features , axis = 1 ) ProductTransformer ( BaseEstimator ) \u00b6 Computes the column-wise product of an array of input features, rescaling from 0-1. Source code in elapid/features.py class ProductTransformer ( BaseEstimator ): \"\"\"Computes the column-wise product of an array of input features, rescaling from 0-1.\"\"\" clamp : bool = None feature_range : Tuple [ float , float ] = None estimator : BaseEstimator = None def __init__ ( self , clamp : bool = MaxentConfig . clamp , feature_range : Tuple [ float , float ] = ( 0.0 , 1.0 ), ): self . clamp = clamp self . feature_range = feature_range self . estimator = MinMaxScaler ( clip = self . clamp , feature_range = self . feature_range ) def fit ( self , x : ArrayLike ): \"\"\"Compute the minimum and maximum for scaling. Args: x: array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. Returns: None. Updates the transformer with feature fitting parameters. \"\"\" self . estimator . fit ( column_product ( np . array ( x ))) def transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Scale covariates according to the feature range. Args: x: array-like of shape (n_samples, n_features) Input data that will be transformed. Returns: ndarray with transformed data. \"\"\" return self . estimator . transform ( column_product ( np . array ( x ))) def fit_transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Fits scaler to x and returns transformed features. Args: x: array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform. Returns: ndarray with transformed data. \"\"\" self . fit ( x ) return self . transform ( x ) fit ( self , x ) \u00b6 Compute the minimum and maximum for scaling. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. required Returns: Type Description None. Updates the transformer with feature fitting parameters. Source code in elapid/features.py def fit ( self , x : ArrayLike ): \"\"\"Compute the minimum and maximum for scaling. Args: x: array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. Returns: None. Updates the transformer with feature fitting parameters. \"\"\" self . estimator . fit ( column_product ( np . array ( x ))) fit_transform ( self , x ) \u00b6 Fits scaler to x and returns transformed features. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform. required Returns: Type Description ndarray ndarray with transformed data. Source code in elapid/features.py def fit_transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Fits scaler to x and returns transformed features. Args: x: array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform. Returns: ndarray with transformed data. \"\"\" self . fit ( x ) return self . transform ( x ) transform ( self , x ) \u00b6 Scale covariates according to the feature range. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) Input data that will be transformed. required Returns: Type Description ndarray ndarray with transformed data. Source code in elapid/features.py def transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Scale covariates according to the feature range. Args: x: array-like of shape (n_samples, n_features) Input data that will be transformed. Returns: ndarray with transformed data. \"\"\" return self . estimator . transform ( column_product ( np . array ( x ))) QuadraticTransformer ( BaseEstimator ) \u00b6 Applies quadtratic feature transformations and rescales features from 0-1. Source code in elapid/features.py class QuadraticTransformer ( BaseEstimator ): \"\"\"Applies quadtratic feature transformations and rescales features from 0-1.\"\"\" clamp : bool = None feature_range : Tuple [ float , float ] = None estimator : BaseEstimator = None def __init__ ( self , clamp : bool = MaxentConfig . clamp , feature_range : Tuple [ float , float ] = ( 0.0 , 1.0 ), ): self . clamp = clamp self . feature_range = feature_range self . estimator = MinMaxScaler ( clip = self . clamp , feature_range = self . feature_range ) def fit ( self , x : ArrayLike ) -> None : \"\"\"Compute the minimum and maximum for scaling. Args: x: array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. Returns: None. Updates the transformer with feature fitting parameters. \"\"\" self . estimator . fit ( np . array ( x ) ** 2 ) def transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Scale covariates according to the feature range. Args: x: array-like of shape (n_samples, n_features) Input data that will be transformed. Returns: ndarray with transformed data. \"\"\" return self . estimator . transform ( np . array ( x ) ** 2 ) def fit_transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Fits scaler to x and returns transformed features. Args: x: array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform. Returns: ndarray with transformed data. \"\"\" self . fit ( x ) return self . estimator . transform ( np . array ( x ) ** 2 ) def inverse_transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Revert from transformed features to original covariate values. Args: x: array-like of shape (n_xamples, n_features) Transformed feature data to convert to covariate data. Returns: ndarray with unscaled covariate values. \"\"\" return self . estimator . inverse_transform ( np . array ( x )) ** 0.5 fit ( self , x ) \u00b6 Compute the minimum and maximum for scaling. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. required Returns: Type Description None None. Updates the transformer with feature fitting parameters. Source code in elapid/features.py def fit ( self , x : ArrayLike ) -> None : \"\"\"Compute the minimum and maximum for scaling. Args: x: array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. Returns: None. Updates the transformer with feature fitting parameters. \"\"\" self . estimator . fit ( np . array ( x ) ** 2 ) fit_transform ( self , x ) \u00b6 Fits scaler to x and returns transformed features. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform. required Returns: Type Description ndarray ndarray with transformed data. Source code in elapid/features.py def fit_transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Fits scaler to x and returns transformed features. Args: x: array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform. Returns: ndarray with transformed data. \"\"\" self . fit ( x ) return self . estimator . transform ( np . array ( x ) ** 2 ) inverse_transform ( self , x ) \u00b6 Revert from transformed features to original covariate values. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_xamples, n_features) Transformed feature data to convert to covariate data. required Returns: Type Description ndarray ndarray with unscaled covariate values. Source code in elapid/features.py def inverse_transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Revert from transformed features to original covariate values. Args: x: array-like of shape (n_xamples, n_features) Transformed feature data to convert to covariate data. Returns: ndarray with unscaled covariate values. \"\"\" return self . estimator . inverse_transform ( np . array ( x )) ** 0.5 transform ( self , x ) \u00b6 Scale covariates according to the feature range. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) Input data that will be transformed. required Returns: Type Description ndarray ndarray with transformed data. Source code in elapid/features.py def transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Scale covariates according to the feature range. Args: x: array-like of shape (n_samples, n_features) Input data that will be transformed. Returns: ndarray with transformed data. \"\"\" return self . estimator . transform ( np . array ( x ) ** 2 ) ThresholdTransformer ( BaseEstimator ) \u00b6 Applies binary thresholds to each covariate based on n evenly-spaced thresholds across it's min/max range. Source code in elapid/features.py class ThresholdTransformer ( BaseEstimator ): \"\"\"Applies binary thresholds to each covariate based on n evenly-spaced thresholds across it's min/max range.\"\"\" n_thresholds_ : int = None mins_ : np . ndarray = None maxs_ : np . ndarray = None threshold_indices_ : np . ndarray = None def __init__ ( self , n_thresholds : int = MaxentConfig . n_threshold_features ): self . n_thresholds_ = n_thresholds def fit ( self , x : ArrayLike ): \"\"\"Compute the minimum and maximum for scaling. Args: x: array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. Returns: None. Updates the transformer with feature fitting parameters. \"\"\" x = np . array ( x ) self . mins_ = x . min ( axis = 0 ) self . maxs_ = x . max ( axis = 0 ) self . threshold_indices_ = np . linspace ( self . mins_ , self . maxs_ , self . n_thresholds_ ) def transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Scale covariates according to the feature range. Args: x: array-like of shape (n_samples, n_features) Input data that will be transformed. Returns: ndarray with transformed data. \"\"\" x = np . array ( x ) xarr = repeat_array ( x , len ( self . threshold_indices_ ), axis =- 1 ) tarr = repeat_array ( self . threshold_indices_ . transpose (), len ( x ), axis = 0 ) thresh = ( xarr > tarr ) . reshape ( x . shape [ 0 ], - 1 ) return thresh . astype ( np . uint8 ) def fit_transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Fits scaler to x and returns transformed features. Args: x: array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform. Returns: ndarray with transformed data. \"\"\" self . fit ( x ) return self . transform ( x ) fit ( self , x ) \u00b6 Compute the minimum and maximum for scaling. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. required Returns: Type Description None. Updates the transformer with feature fitting parameters. Source code in elapid/features.py def fit ( self , x : ArrayLike ): \"\"\"Compute the minimum and maximum for scaling. Args: x: array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. Returns: None. Updates the transformer with feature fitting parameters. \"\"\" x = np . array ( x ) self . mins_ = x . min ( axis = 0 ) self . maxs_ = x . max ( axis = 0 ) self . threshold_indices_ = np . linspace ( self . mins_ , self . maxs_ , self . n_thresholds_ ) fit_transform ( self , x ) \u00b6 Fits scaler to x and returns transformed features. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform. required Returns: Type Description ndarray ndarray with transformed data. Source code in elapid/features.py def fit_transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Fits scaler to x and returns transformed features. Args: x: array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform. Returns: ndarray with transformed data. \"\"\" self . fit ( x ) return self . transform ( x ) transform ( self , x ) \u00b6 Scale covariates according to the feature range. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) Input data that will be transformed. required Returns: Type Description ndarray ndarray with transformed data. Source code in elapid/features.py def transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Scale covariates according to the feature range. Args: x: array-like of shape (n_samples, n_features) Input data that will be transformed. Returns: ndarray with transformed data. \"\"\" x = np . array ( x ) xarr = repeat_array ( x , len ( self . threshold_indices_ ), axis =- 1 ) tarr = repeat_array ( self . threshold_indices_ . transpose (), len ( x ), axis = 0 ) thresh = ( xarr > tarr ) . reshape ( x . shape [ 0 ], - 1 ) return thresh . astype ( np . uint8 ) column_product ( array ) \u00b6 Computes the column-wise product of a 2D array. Parameters: Name Type Description Default array ndarray array-like of shape (n_samples, n_features) required Returns: Type Description ndarray ndarray with of shape (n_samples, factorial(n_features-1)) Source code in elapid/features.py def column_product ( array : np . ndarray ) -> np . ndarray : \"\"\"Computes the column-wise product of a 2D array. Args: array: array-like of shape (n_samples, n_features) Returns: ndarray with of shape (n_samples, factorial(n_features-1)) \"\"\" nrows , ncols = array . shape if ncols == 1 : return array else : products = [] for xstart in range ( 0 , ncols - 1 ): products . append ( array [:, xstart ] . reshape ( nrows , 1 ) * array [:, xstart + 1 :]) return np . concatenate ( products , axis = 1 ) compute_lambdas ( y , weights , reg , n_lambdas = 100 ) \u00b6 Computes lambda parameter values for elastic lasso fits. Parameters: Name Type Description Default y Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples,) with binary presence/background (1/0) values required weights Union[numpy.ndarray, pandas.core.frame.DataFrame] per-sample model weights required reg Union[numpy.ndarray, pandas.core.frame.DataFrame] per-feature regularization coefficients required n_lambdas int number of lambda values to estimate 100 Returns: Type Description lambdas Array of lambda scores of length n_lambda Source code in elapid/features.py def compute_lambdas ( y : ArrayLike , weights : ArrayLike , reg : ArrayLike , n_lambdas : int = MaxentConfig . n_lambdas ) -> np . ndarray : \"\"\"Computes lambda parameter values for elastic lasso fits. Args: y: array-like of shape (n_samples,) with binary presence/background (1/0) values weights: per-sample model weights reg: per-feature regularization coefficients n_lambdas: number of lambda values to estimate Returns: lambdas: Array of lambda scores of length n_lambda \"\"\" n_presence = np . sum ( y ) mean_regularization = np . mean ( reg ) total_weight = np . sum ( weights ) seed_range = np . linspace ( 4 , 0 , n_lambdas ) lambdas = 10 ** ( seed_range ) * mean_regularization * ( n_presence / total_weight ) return lambdas compute_regularization ( y , z , feature_labels , beta_multiplier = 1.0 , beta_lqp = 1.0 , beta_threshold = 1.0 , beta_hinge = 1.0 , beta_categorical = 1.0 ) \u00b6 Computes variable regularization values for all feature data. Parameters: Name Type Description Default y Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples,) with binary presence/background (1/0) values required z ndarray model features (transformations applied to covariates) required feature_labels List[str] list of length n_features, with labels identifying each column's feature type with options [\"linear\", \"quadratic\", \"product\", \"threshold\", \"hinge\", \"categorical\"] required beta_multiplier float scaler for all regularization parameters. higher values exclude more features 1.0 beta_lqp float scaler for linear, quadratic and product feature regularization 1.0 beta_threshold float scaler for threshold feature regularization 1.0 beta_hinge float scaler for hinge feature regularization 1.0 beta_categorical float scaler for categorical feature regularization 1.0 Returns: Type Description max_reg Array with per-feature regularization parameters Source code in elapid/features.py def compute_regularization ( y : ArrayLike , z : np . ndarray , feature_labels : List [ str ], beta_multiplier : float = MaxentConfig . beta_multiplier , beta_lqp : float = MaxentConfig . beta_lqp , beta_threshold : float = MaxentConfig . beta_threshold , beta_hinge : float = MaxentConfig . beta_hinge , beta_categorical : float = MaxentConfig . beta_hinge , ) -> np . ndarray : \"\"\"Computes variable regularization values for all feature data. Args: y: array-like of shape (n_samples,) with binary presence/background (1/0) values z: model features (transformations applied to covariates) feature_labels: list of length n_features, with labels identifying each column's feature type with options [\"linear\", \"quadratic\", \"product\", \"threshold\", \"hinge\", \"categorical\"] beta_multiplier: scaler for all regularization parameters. higher values exclude more features beta_lqp: scaler for linear, quadratic and product feature regularization beta_threshold: scaler for threshold feature regularization beta_hinge: scaler for hinge feature regularization beta_categorical: scaler for categorical feature regularization Returns: max_reg: Array with per-feature regularization parameters \"\"\" # compute regularization based on presence-only locations z1 = z [ y == 1 ] nrows , ncols = z1 . shape labels = np . array ( feature_labels ) nlabels = len ( feature_labels ) assert nlabels == ncols , f \"number of feature_labels ( { nlabels } ) must match number of features ( { ncols } )\" # create arrays to store the regularization params base_regularization = np . zeros ( ncols ) hinge_regularization = np . zeros ( ncols ) threshold_regularization = np . zeros ( ncols ) # use a different reg table based on the features set if \"product\" in labels : table_lqp = RegularizationConfig . product elif \"quadratic\" in labels : table_lqp = RegularizationConfig . quadratic else : table_lqp = RegularizationConfig . linear if \"linear\" in labels : linear_idxs = labels == \"linear\" fr_max , fr_min = table_lqp multiplier = beta_lqp ap = np . interp ( nrows , fr_max , fr_min ) reg = multiplier * ap / np . sqrt ( nrows ) base_regularization [ linear_idxs ] = reg if \"quadratic\" in labels : quadratic_idxs = labels == \"quadratic\" fr_max , fr_min = table_lqp multiplier = beta_lqp ap = np . interp ( nrows , fr_max , fr_min ) reg = multiplier * ap / np . sqrt ( nrows ) base_regularization [ quadratic_idxs ] = reg if \"product\" in labels : product_idxs = labels == \"product\" fr_max , fr_min = table_lqp multiplier = beta_lqp ap = np . interp ( nrows , fr_max , fr_min ) reg = multiplier * ap / np . sqrt ( nrows ) base_regularization [ product_idxs ] = reg if \"threshold\" in labels : threshold_idxs = labels == \"threshold\" fr_max , fr_min = RegularizationConfig . threshold multiplier = beta_threshold ap = np . interp ( nrows , fr_max , fr_min ) reg = multiplier * ap / np . sqrt ( nrows ) base_regularization [ threshold_idxs ] = reg # increase regularization for uniform threshlold values all_zeros = np . all ( z1 == 0 , axis = 0 ) all_ones = np . all ( z1 == 1 , axis = 0 ) threshold_regularization [ all_zeros ] = 1 threshold_regularization [ all_ones ] = 1 if \"hinge\" in labels : hinge_idxs = labels == \"hinge\" fr_max , fr_min = RegularizationConfig . hinge multiplier = beta_hinge ap = np . interp ( nrows , fr_max , fr_min ) reg = multiplier * ap / np . sqrt ( nrows ) base_regularization [ hinge_idxs ] = reg # increase regularization for extreme hinge values hinge_std = np . std ( z1 [:, hinge_idxs ], ddof = 1 , axis = 0 ) hinge_sqrt = np . zeros ( len ( hinge_std )) + ( 1 / np . sqrt ( nrows )) std = np . max (( hinge_std , hinge_sqrt ), axis = 0 ) hinge_regularization [ hinge_idxs ] = ( 0.5 * std ) / np . sqrt ( nrows ) if \"categorical\" in labels : categorical_idxs = labels == \"categorical\" fr_max , fr_min = RegularizationConfig . categorical multiplier = beta_categorical ap = np . interp ( nrows , fr_max , fr_min ) reg = multiplier * ap / np . sqrt ( nrows ) base_regularization [ categorical_idxs ] = reg # compute the maximum regularization based on a few different approaches default_regularization = 0.001 * ( np . max ( z , axis = 0 ) - np . min ( z , axis = 0 )) variance_regularization = np . std ( z1 , ddof = 1 , axis = 0 ) * base_regularization max_regularization = np . max ( ( default_regularization , variance_regularization , hinge_regularization , threshold_regularization ), axis = 0 ) # apply the final scaling factor max_regularization *= beta_multiplier return max_regularization compute_weights ( y , pbr = 100 ) \u00b6 Compute Maxent-format per-sample model weights. Parameters: Name Type Description Default y Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples,) with binary presence/background (1/0) values required pbr int presence-to-background weight ratio. pbr=100 sets background samples to 1/100 weight of presence samples. 100 Returns: Type Description weights array with glmnet-formatted sample weights Source code in elapid/features.py def compute_weights ( y : ArrayLike , pbr : int = 100 ) -> np . ndarray : \"\"\"Compute Maxent-format per-sample model weights. Args: y: array-like of shape (n_samples,) with binary presence/background (1/0) values pbr: presence-to-background weight ratio. pbr=100 sets background samples to 1/100 weight of presence samples. Returns: weights: array with glmnet-formatted sample weights \"\"\" weights = np . array ( y + ( 1 - y ) * pbr ) return weights left_hinge ( x , mn , mx ) \u00b6 Computes hinge transformation values. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] Array-like of covariate values required mn float Minimum covariate value to fit hinges to required mx float Maximum covariate value to fit hinges to required Returns: Type Description ndarray Array of hinge features Source code in elapid/features.py def left_hinge ( x : ArrayLike , mn : float , mx : float ) -> np . ndarray : \"\"\"Computes hinge transformation values. Args: x: Array-like of covariate values mn: Minimum covariate value to fit hinges to mx: Maximum covariate value to fit hinges to Returns: Array of hinge features \"\"\" return np . minimum ( 1 , np . maximum ( 0 , ( x - mn ) / ( repeat_array ( mx , mn . shape [ - 1 ], axis = 1 ) - mn ))) right_hinge ( x , mn , mx ) \u00b6 Computes hinge transformation values. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] Array-like of covariate values required mn float Minimum covariate value to fit hinges to required mx float Maximum covariate value to fit hinges to required Returns: Type Description ndarray Array of hinge features Source code in elapid/features.py def right_hinge ( x : ArrayLike , mn : float , mx : float ) -> np . ndarray : \"\"\"Computes hinge transformation values. Args: x: Array-like of covariate values mn: Minimum covariate value to fit hinges to mx: Maximum covariate value to fit hinges to Returns: Array of hinge features \"\"\" mn_broadcast = repeat_array ( mn , mx . shape [ - 1 ], axis = 1 ) return np . minimum ( 1 , np . maximum ( 0 , ( x - mn_broadcast ) / ( mx - mn_broadcast )))","title":"elapid.features"},{"location":"module/features/#elapidfeatures","text":"Functions to transform covariate data into complex model features.","title":"elapid.features"},{"location":"module/features/#elapid.features.CategoricalTransformer","text":"Applies one-hot encoding to categorical covariate datasets. Source code in elapid/features.py class CategoricalTransformer ( BaseEstimator ): \"\"\"Applies one-hot encoding to categorical covariate datasets.\"\"\" estimators_ : list = None def __init__ ( self ): pass def fit ( self , x : ArrayLike ): \"\"\"Compute the minimum and maximum for scaling. Args: x: array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. Returns: None. Updates the transformer with feature fitting parameters. \"\"\" self . estimators_ = [] x = np . array ( x ) if x . ndim == 1 : estimator = OneHotEncoder ( dtype = np . uint8 , sparse = False ) self . estimators_ . append ( estimator . fit ( x . reshape ( - 1 , 1 ))) else : nrows , ncols = x . shape for col in range ( ncols ): xsub = x [:, col ] . reshape ( - 1 , 1 ) estimator = OneHotEncoder ( dtype = np . uint8 , sparse = False ) self . estimators_ . append ( estimator . fit ( xsub )) def transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Scale covariates according to the feature range. Args: x: array-like of shape (n_samples, n_features) Input data that will be transformed. Returns: ndarray with transformed data. \"\"\" x = np . array ( x ) if x . ndim == 1 : estimator = self . estimators_ [ 0 ] return estimator . transform ( x . reshape ( - 1 , 1 )) else : class_data = [] nrows , ncols = x . shape for col in range ( ncols ): xsub = x [:, col ] . reshape ( - 1 , 1 ) estimator = self . estimators_ [ col ] class_data . append ( estimator . transform ( xsub )) return np . concatenate ( class_data , axis = 1 ) def fit_transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Fits scaler to x and returns transformed features. Args: x: array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform. Returns: ndarray with transformed data. \"\"\" self . fit ( x ) return self . transform ( x )","title":"CategoricalTransformer"},{"location":"module/features/#elapid.features.CategoricalTransformer.fit","text":"Compute the minimum and maximum for scaling. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. required Returns: Type Description None. Updates the transformer with feature fitting parameters. Source code in elapid/features.py def fit ( self , x : ArrayLike ): \"\"\"Compute the minimum and maximum for scaling. Args: x: array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. Returns: None. Updates the transformer with feature fitting parameters. \"\"\" self . estimators_ = [] x = np . array ( x ) if x . ndim == 1 : estimator = OneHotEncoder ( dtype = np . uint8 , sparse = False ) self . estimators_ . append ( estimator . fit ( x . reshape ( - 1 , 1 ))) else : nrows , ncols = x . shape for col in range ( ncols ): xsub = x [:, col ] . reshape ( - 1 , 1 ) estimator = OneHotEncoder ( dtype = np . uint8 , sparse = False ) self . estimators_ . append ( estimator . fit ( xsub ))","title":"fit()"},{"location":"module/features/#elapid.features.CategoricalTransformer.fit_transform","text":"Fits scaler to x and returns transformed features. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform. required Returns: Type Description ndarray ndarray with transformed data. Source code in elapid/features.py def fit_transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Fits scaler to x and returns transformed features. Args: x: array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform. Returns: ndarray with transformed data. \"\"\" self . fit ( x ) return self . transform ( x )","title":"fit_transform()"},{"location":"module/features/#elapid.features.CategoricalTransformer.transform","text":"Scale covariates according to the feature range. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) Input data that will be transformed. required Returns: Type Description ndarray ndarray with transformed data. Source code in elapid/features.py def transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Scale covariates according to the feature range. Args: x: array-like of shape (n_samples, n_features) Input data that will be transformed. Returns: ndarray with transformed data. \"\"\" x = np . array ( x ) if x . ndim == 1 : estimator = self . estimators_ [ 0 ] return estimator . transform ( x . reshape ( - 1 , 1 )) else : class_data = [] nrows , ncols = x . shape for col in range ( ncols ): xsub = x [:, col ] . reshape ( - 1 , 1 ) estimator = self . estimators_ [ col ] class_data . append ( estimator . transform ( xsub )) return np . concatenate ( class_data , axis = 1 )","title":"transform()"},{"location":"module/features/#elapid.features.CumulativeTransformer","text":"Applies a percentile-based transform to estimate cumulative suitability. Source code in elapid/features.py class CumulativeTransformer ( QuantileTransformer ): \"\"\"Applies a percentile-based transform to estimate cumulative suitability.\"\"\" def __init__ ( self ): super () . __init__ ( n_quantiles = 100 , output_distribution = \"uniform\" )","title":"CumulativeTransformer"},{"location":"module/features/#elapid.features.HingeTransformer","text":"Fits hinge transformations to an array of covariates. Source code in elapid/features.py class HingeTransformer ( BaseEstimator ): \"\"\"Fits hinge transformations to an array of covariates.\"\"\" n_hinges_ : int = None mins_ : np . ndarray = None maxs_ : np . ndarray = None hinge_indices_ : np . ndarray = None def __init__ ( self , n_hinges : int = MaxentConfig . n_hinge_features ): self . n_hinges_ = n_hinges def fit ( self , x : ArrayLike ): \"\"\"Compute the minimum and maximum for scaling. Args: x: array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. Returns: None. Updates the transformer with feature fitting parameters. \"\"\" x = np . array ( x ) self . mins_ = x . min ( axis = 0 ) self . maxs_ = x . max ( axis = 0 ) self . hinge_indices_ = np . linspace ( self . mins_ , self . maxs_ , self . n_hinges_ ) def transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Scale covariates according to the feature range. Args: x: array-like of shape (n_samples, n_features) Input data that will be transformed. Returns: ndarray with transformed data. \"\"\" x = np . array ( x ) xarr = repeat_array ( x , self . n_hinges_ - 1 , axis =- 1 ) lharr = repeat_array ( self . hinge_indices_ [: - 1 ] . transpose (), len ( x ), axis = 0 ) rharr = repeat_array ( self . hinge_indices_ [ 1 :] . transpose (), len ( x ), axis = 0 ) lh = left_hinge ( xarr , lharr , self . maxs_ ) rh = right_hinge ( xarr , self . mins_ , rharr ) return np . concatenate (( lh , rh ), axis = 2 ) . reshape ( x . shape [ 0 ], - 1 ) def fit_transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Fits scaler to x and returns transformed features. Args: x: array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform. Returns: ndarray with transformed data. \"\"\" self . fit ( x ) return self . transform ( x )","title":"HingeTransformer"},{"location":"module/features/#elapid.features.HingeTransformer.fit","text":"Compute the minimum and maximum for scaling. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. required Returns: Type Description None. Updates the transformer with feature fitting parameters. Source code in elapid/features.py def fit ( self , x : ArrayLike ): \"\"\"Compute the minimum and maximum for scaling. Args: x: array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. Returns: None. Updates the transformer with feature fitting parameters. \"\"\" x = np . array ( x ) self . mins_ = x . min ( axis = 0 ) self . maxs_ = x . max ( axis = 0 ) self . hinge_indices_ = np . linspace ( self . mins_ , self . maxs_ , self . n_hinges_ )","title":"fit()"},{"location":"module/features/#elapid.features.HingeTransformer.fit_transform","text":"Fits scaler to x and returns transformed features. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform. required Returns: Type Description ndarray ndarray with transformed data. Source code in elapid/features.py def fit_transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Fits scaler to x and returns transformed features. Args: x: array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform. Returns: ndarray with transformed data. \"\"\" self . fit ( x ) return self . transform ( x )","title":"fit_transform()"},{"location":"module/features/#elapid.features.HingeTransformer.transform","text":"Scale covariates according to the feature range. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) Input data that will be transformed. required Returns: Type Description ndarray ndarray with transformed data. Source code in elapid/features.py def transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Scale covariates according to the feature range. Args: x: array-like of shape (n_samples, n_features) Input data that will be transformed. Returns: ndarray with transformed data. \"\"\" x = np . array ( x ) xarr = repeat_array ( x , self . n_hinges_ - 1 , axis =- 1 ) lharr = repeat_array ( self . hinge_indices_ [: - 1 ] . transpose (), len ( x ), axis = 0 ) rharr = repeat_array ( self . hinge_indices_ [ 1 :] . transpose (), len ( x ), axis = 0 ) lh = left_hinge ( xarr , lharr , self . maxs_ ) rh = right_hinge ( xarr , self . mins_ , rharr ) return np . concatenate (( lh , rh ), axis = 2 ) . reshape ( x . shape [ 0 ], - 1 )","title":"transform()"},{"location":"module/features/#elapid.features.LinearTransformer","text":"Applies linear feature transformations to rescale features from 0-1. Source code in elapid/features.py class LinearTransformer ( MinMaxScaler ): \"\"\"Applies linear feature transformations to rescale features from 0-1.\"\"\" clamp : bool = None feature_range : None def __init__ ( self , clamp : bool = MaxentConfig . clamp , feature_range : Tuple [ float , float ] = ( 0.0 , 1.0 ), ): self . clamp = clamp self . feature_range = feature_range super () . __init__ ( clip = clamp , feature_range = feature_range )","title":"LinearTransformer"},{"location":"module/features/#elapid.features.MaxentFeatureTransformer","text":"Transforms covariate data into maxent-format feature data. Source code in elapid/features.py class MaxentFeatureTransformer ( BaseEstimator ): \"\"\"Transforms covariate data into maxent-format feature data.\"\"\" feature_types_ : list = None clamp_ : bool = None n_hinge_features_ : int = None n_threshold_features_ : int = None categorical_ : list = None continuous_ : list = None categorical_pd_ : list = None continuous_pd_ : list = None labels_ : list = None estimators_ : dict = { \"linear\" : None , \"quadratic\" : None , \"product\" : None , \"threshold\" : None , \"hinge\" : None , \"categorical\" : None , } feature_names_ : list = None def __init__ ( self , feature_types : Union [ str , list ] = MaxentConfig . feature_types , clamp : bool = MaxentConfig . clamp , n_hinge_features : int = MaxentConfig . n_hinge_features , n_threshold_features : int = MaxentConfig . n_threshold_features , ): \"\"\"Computes features based on the maxent feature types specified (like linear, quadratic, hinge). Implemented using sklearn conventions (with `.fit()` and `.transform()` functions. Args: feature_types: list of maxent features to generate. clamp: set feature values to global mins/maxs during prediction n_hinge_features: number of hinge knots to generate n_threshold_features: nuber of threshold features to generate \"\"\" self . feature_types_ = validate_feature_types ( feature_types ) self . clamp_ = validate_boolean ( clamp ) self . n_hinge_features_ = validate_numeric_scalar ( n_hinge_features ) self . n_threshold_features_ = validate_numeric_scalar ( n_threshold_features ) def _format_covariate_data ( self , x : ArrayLike ) -> Tuple [ np . array , np . array ]: \"\"\"Reads input x data and formats it to consistent array dtypes. Args: x: array-like of shape (n_samples, n_features) Returns: (continuous, categorical) tuple of ndarrays with continuous and categorical covariate data. \"\"\" if isinstance ( x , np . ndarray ): if self . categorical_ is None : con = x cat = None else : con = x [:, self . continuous_ ] cat = x [:, self . categorical_ ] elif isinstance ( x , pd . DataFrame ): con = x [ self . continuous_pd_ ] . to_numpy () if len ( self . categorical_pd_ ) > 0 : cat = x [ self . categorical_pd_ ] . to_numpy () else : cat = None else : raise TypeError ( f \"Unsupported x dtype: { type ( x ) } . Must be pd.DataFrame or np.array\" ) return con , cat def _format_labels_and_dtypes ( self , x : ArrayLike , categorical : list = None , labels : list = None ) -> None : \"\"\"Read input x data and lists of categorical data indices and band labels to format and store this info for later indexing. Args: s: array-like of shape (n_samples, n_features) categorical: indices indicating which x columns are categorical labels: covariate column labels. ignored if x is a pandas DataFrame \"\"\" if isinstance ( x , np . ndarray ): nrows , ncols = x . shape if categorical is None : continuous = list ( range ( ncols )) else : continuous = list ( set ( range ( ncols )) . difference ( set ( categorical ))) self . labels_ = labels or make_band_labels ( ncols ) self . categorical_ = categorical self . continuous_ = continuous elif isinstance ( x , pd . DataFrame ): x . drop ([ \"geometry\" ], axis = 1 , errors = \"ignore\" , inplace = True ) self . labels_ = labels or list ( x . columns ) # store both pandas and numpy indexing of these values self . continuous_pd_ = list ( x . select_dtypes ( exclude = \"category\" ) . columns ) self . categorical_pd_ = list ( x . select_dtypes ( include = \"category\" ) . columns ) all_columns = list ( x . columns ) self . continuous_ = [ all_columns . index ( item ) for item in self . continuous_pd_ if item in all_columns ] if len ( self . categorical_pd_ ) != 0 : self . categorical_ = [ all_columns . index ( item ) for item in self . categorical_pd_ if item in all_columns ] else : self . categorical_ = None def fit ( self , x : ArrayLike , categorical : list = None , labels : list = None ) -> None : \"\"\"Compute the minimum and maximum for scaling. Args: x: array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. categorical: indices indicating which x columns are categorical labels: covariate column labels. ignored if x is a pandas DataFrame Returns: None. Updates the transformer with feature fitting parameters. \"\"\" self . _format_labels_and_dtypes ( x , categorical = categorical , labels = labels ) con , cat = self . _format_covariate_data ( x ) nrows , ncols = con . shape feature_names = [] if \"linear\" in self . feature_types_ : estimator = LinearTransformer ( clamp = self . clamp_ ) estimator . fit ( con ) self . estimators_ [ \"linear\" ] = estimator feature_names += [ \"linear\" ] * estimator . n_features_in_ if \"quadratic\" in self . feature_types_ : estimator = QuadraticTransformer ( clamp = self . clamp_ ) estimator . fit ( con ) self . estimators_ [ \"quadratic\" ] = estimator feature_names += [ \"quadratic\" ] * estimator . estimator . n_features_in_ if \"product\" in self . feature_types_ : estimator = ProductTransformer ( clamp = self . clamp_ ) estimator . fit ( con ) self . estimators_ [ \"product\" ] = estimator feature_names += [ \"product\" ] * estimator . estimator . n_features_in_ if \"threshold\" in self . feature_types_ : estimator = ThresholdTransformer ( n_thresholds = self . n_threshold_features_ ) estimator . fit ( con ) self . estimators_ [ \"threshold\" ] = estimator feature_names += [ \"threshold\" ] * ( estimator . n_thresholds_ * ncols ) if \"hinge\" in self . feature_types_ : estimator = HingeTransformer ( n_hinges = self . n_hinge_features_ ) estimator . fit ( con ) self . estimators_ [ \"hinge\" ] = estimator feature_names += [ \"hinge\" ] * (( estimator . n_hinges_ - 1 ) * 2 * ncols ) if cat is not None : estimator = CategoricalTransformer () estimator . fit ( cat ) self . estimators_ [ \"categorical\" ] = estimator for est in estimator . estimators_ : feature_names += [ \"categorical\" ] * len ( est . categories_ [ 0 ]) self . feature_names_ = feature_names def transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Scale covariates according to the feature range. Args: x: array-like of shape (n_samples, n_features) Input data that will be transformed. Returns: ndarray with transformed data. \"\"\" con , cat = self . _format_covariate_data ( x ) features = [] if \"linear\" in self . feature_types_ : features . append ( self . estimators_ [ \"linear\" ] . transform ( con )) if \"quadratic\" in self . feature_types_ : features . append ( self . estimators_ [ \"quadratic\" ] . transform ( con )) if \"product\" in self . feature_types_ : features . append ( self . estimators_ [ \"product\" ] . transform ( con )) if \"threshold\" in self . feature_types_ : features . append ( self . estimators_ [ \"threshold\" ] . transform ( con )) if \"hinge\" in self . feature_types_ : features . append ( self . estimators_ [ \"hinge\" ] . transform ( con )) if cat is not None : features . append ( self . estimators_ [ \"categorical\" ] . transform ( cat )) return np . concatenate ( features , axis = 1 ) def fit_transform ( self , x : ArrayLike , categorical : list = None , labels : list = None ) -> np . ndarray : \"\"\"Fits scaler to x and returns transformed features. Args: x: array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform. Returns: ndarray with transformed data. \"\"\" self . fit ( x , categorical = categorical , labels = labels ) return self . transform ( x )","title":"MaxentFeatureTransformer"},{"location":"module/features/#elapid.features.MaxentFeatureTransformer.__init__","text":"Computes features based on the maxent feature types specified (like linear, quadratic, hinge). Implemented using sklearn conventions (with .fit() and .transform() functions. Parameters: Name Type Description Default feature_types Union[str, list] list of maxent features to generate. ['linear', 'hinge', 'product'] clamp bool set feature values to global mins/maxs during prediction True n_hinge_features int number of hinge knots to generate 10 n_threshold_features int nuber of threshold features to generate 10 Source code in elapid/features.py def __init__ ( self , feature_types : Union [ str , list ] = MaxentConfig . feature_types , clamp : bool = MaxentConfig . clamp , n_hinge_features : int = MaxentConfig . n_hinge_features , n_threshold_features : int = MaxentConfig . n_threshold_features , ): \"\"\"Computes features based on the maxent feature types specified (like linear, quadratic, hinge). Implemented using sklearn conventions (with `.fit()` and `.transform()` functions. Args: feature_types: list of maxent features to generate. clamp: set feature values to global mins/maxs during prediction n_hinge_features: number of hinge knots to generate n_threshold_features: nuber of threshold features to generate \"\"\" self . feature_types_ = validate_feature_types ( feature_types ) self . clamp_ = validate_boolean ( clamp ) self . n_hinge_features_ = validate_numeric_scalar ( n_hinge_features ) self . n_threshold_features_ = validate_numeric_scalar ( n_threshold_features )","title":"__init__()"},{"location":"module/features/#elapid.features.MaxentFeatureTransformer.fit","text":"Compute the minimum and maximum for scaling. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. required categorical list indices indicating which x columns are categorical None labels list covariate column labels. ignored if x is a pandas DataFrame None Returns: Type Description None None. Updates the transformer with feature fitting parameters. Source code in elapid/features.py def fit ( self , x : ArrayLike , categorical : list = None , labels : list = None ) -> None : \"\"\"Compute the minimum and maximum for scaling. Args: x: array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. categorical: indices indicating which x columns are categorical labels: covariate column labels. ignored if x is a pandas DataFrame Returns: None. Updates the transformer with feature fitting parameters. \"\"\" self . _format_labels_and_dtypes ( x , categorical = categorical , labels = labels ) con , cat = self . _format_covariate_data ( x ) nrows , ncols = con . shape feature_names = [] if \"linear\" in self . feature_types_ : estimator = LinearTransformer ( clamp = self . clamp_ ) estimator . fit ( con ) self . estimators_ [ \"linear\" ] = estimator feature_names += [ \"linear\" ] * estimator . n_features_in_ if \"quadratic\" in self . feature_types_ : estimator = QuadraticTransformer ( clamp = self . clamp_ ) estimator . fit ( con ) self . estimators_ [ \"quadratic\" ] = estimator feature_names += [ \"quadratic\" ] * estimator . estimator . n_features_in_ if \"product\" in self . feature_types_ : estimator = ProductTransformer ( clamp = self . clamp_ ) estimator . fit ( con ) self . estimators_ [ \"product\" ] = estimator feature_names += [ \"product\" ] * estimator . estimator . n_features_in_ if \"threshold\" in self . feature_types_ : estimator = ThresholdTransformer ( n_thresholds = self . n_threshold_features_ ) estimator . fit ( con ) self . estimators_ [ \"threshold\" ] = estimator feature_names += [ \"threshold\" ] * ( estimator . n_thresholds_ * ncols ) if \"hinge\" in self . feature_types_ : estimator = HingeTransformer ( n_hinges = self . n_hinge_features_ ) estimator . fit ( con ) self . estimators_ [ \"hinge\" ] = estimator feature_names += [ \"hinge\" ] * (( estimator . n_hinges_ - 1 ) * 2 * ncols ) if cat is not None : estimator = CategoricalTransformer () estimator . fit ( cat ) self . estimators_ [ \"categorical\" ] = estimator for est in estimator . estimators_ : feature_names += [ \"categorical\" ] * len ( est . categories_ [ 0 ]) self . feature_names_ = feature_names","title":"fit()"},{"location":"module/features/#elapid.features.MaxentFeatureTransformer.fit_transform","text":"Fits scaler to x and returns transformed features. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform. required Returns: Type Description ndarray ndarray with transformed data. Source code in elapid/features.py def fit_transform ( self , x : ArrayLike , categorical : list = None , labels : list = None ) -> np . ndarray : \"\"\"Fits scaler to x and returns transformed features. Args: x: array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform. Returns: ndarray with transformed data. \"\"\" self . fit ( x , categorical = categorical , labels = labels ) return self . transform ( x )","title":"fit_transform()"},{"location":"module/features/#elapid.features.MaxentFeatureTransformer.transform","text":"Scale covariates according to the feature range. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) Input data that will be transformed. required Returns: Type Description ndarray ndarray with transformed data. Source code in elapid/features.py def transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Scale covariates according to the feature range. Args: x: array-like of shape (n_samples, n_features) Input data that will be transformed. Returns: ndarray with transformed data. \"\"\" con , cat = self . _format_covariate_data ( x ) features = [] if \"linear\" in self . feature_types_ : features . append ( self . estimators_ [ \"linear\" ] . transform ( con )) if \"quadratic\" in self . feature_types_ : features . append ( self . estimators_ [ \"quadratic\" ] . transform ( con )) if \"product\" in self . feature_types_ : features . append ( self . estimators_ [ \"product\" ] . transform ( con )) if \"threshold\" in self . feature_types_ : features . append ( self . estimators_ [ \"threshold\" ] . transform ( con )) if \"hinge\" in self . feature_types_ : features . append ( self . estimators_ [ \"hinge\" ] . transform ( con )) if cat is not None : features . append ( self . estimators_ [ \"categorical\" ] . transform ( cat )) return np . concatenate ( features , axis = 1 )","title":"transform()"},{"location":"module/features/#elapid.features.ProductTransformer","text":"Computes the column-wise product of an array of input features, rescaling from 0-1. Source code in elapid/features.py class ProductTransformer ( BaseEstimator ): \"\"\"Computes the column-wise product of an array of input features, rescaling from 0-1.\"\"\" clamp : bool = None feature_range : Tuple [ float , float ] = None estimator : BaseEstimator = None def __init__ ( self , clamp : bool = MaxentConfig . clamp , feature_range : Tuple [ float , float ] = ( 0.0 , 1.0 ), ): self . clamp = clamp self . feature_range = feature_range self . estimator = MinMaxScaler ( clip = self . clamp , feature_range = self . feature_range ) def fit ( self , x : ArrayLike ): \"\"\"Compute the minimum and maximum for scaling. Args: x: array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. Returns: None. Updates the transformer with feature fitting parameters. \"\"\" self . estimator . fit ( column_product ( np . array ( x ))) def transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Scale covariates according to the feature range. Args: x: array-like of shape (n_samples, n_features) Input data that will be transformed. Returns: ndarray with transformed data. \"\"\" return self . estimator . transform ( column_product ( np . array ( x ))) def fit_transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Fits scaler to x and returns transformed features. Args: x: array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform. Returns: ndarray with transformed data. \"\"\" self . fit ( x ) return self . transform ( x )","title":"ProductTransformer"},{"location":"module/features/#elapid.features.ProductTransformer.fit","text":"Compute the minimum and maximum for scaling. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. required Returns: Type Description None. Updates the transformer with feature fitting parameters. Source code in elapid/features.py def fit ( self , x : ArrayLike ): \"\"\"Compute the minimum and maximum for scaling. Args: x: array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. Returns: None. Updates the transformer with feature fitting parameters. \"\"\" self . estimator . fit ( column_product ( np . array ( x )))","title":"fit()"},{"location":"module/features/#elapid.features.ProductTransformer.fit_transform","text":"Fits scaler to x and returns transformed features. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform. required Returns: Type Description ndarray ndarray with transformed data. Source code in elapid/features.py def fit_transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Fits scaler to x and returns transformed features. Args: x: array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform. Returns: ndarray with transformed data. \"\"\" self . fit ( x ) return self . transform ( x )","title":"fit_transform()"},{"location":"module/features/#elapid.features.ProductTransformer.transform","text":"Scale covariates according to the feature range. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) Input data that will be transformed. required Returns: Type Description ndarray ndarray with transformed data. Source code in elapid/features.py def transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Scale covariates according to the feature range. Args: x: array-like of shape (n_samples, n_features) Input data that will be transformed. Returns: ndarray with transformed data. \"\"\" return self . estimator . transform ( column_product ( np . array ( x )))","title":"transform()"},{"location":"module/features/#elapid.features.QuadraticTransformer","text":"Applies quadtratic feature transformations and rescales features from 0-1. Source code in elapid/features.py class QuadraticTransformer ( BaseEstimator ): \"\"\"Applies quadtratic feature transformations and rescales features from 0-1.\"\"\" clamp : bool = None feature_range : Tuple [ float , float ] = None estimator : BaseEstimator = None def __init__ ( self , clamp : bool = MaxentConfig . clamp , feature_range : Tuple [ float , float ] = ( 0.0 , 1.0 ), ): self . clamp = clamp self . feature_range = feature_range self . estimator = MinMaxScaler ( clip = self . clamp , feature_range = self . feature_range ) def fit ( self , x : ArrayLike ) -> None : \"\"\"Compute the minimum and maximum for scaling. Args: x: array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. Returns: None. Updates the transformer with feature fitting parameters. \"\"\" self . estimator . fit ( np . array ( x ) ** 2 ) def transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Scale covariates according to the feature range. Args: x: array-like of shape (n_samples, n_features) Input data that will be transformed. Returns: ndarray with transformed data. \"\"\" return self . estimator . transform ( np . array ( x ) ** 2 ) def fit_transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Fits scaler to x and returns transformed features. Args: x: array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform. Returns: ndarray with transformed data. \"\"\" self . fit ( x ) return self . estimator . transform ( np . array ( x ) ** 2 ) def inverse_transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Revert from transformed features to original covariate values. Args: x: array-like of shape (n_xamples, n_features) Transformed feature data to convert to covariate data. Returns: ndarray with unscaled covariate values. \"\"\" return self . estimator . inverse_transform ( np . array ( x )) ** 0.5","title":"QuadraticTransformer"},{"location":"module/features/#elapid.features.QuadraticTransformer.fit","text":"Compute the minimum and maximum for scaling. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. required Returns: Type Description None None. Updates the transformer with feature fitting parameters. Source code in elapid/features.py def fit ( self , x : ArrayLike ) -> None : \"\"\"Compute the minimum and maximum for scaling. Args: x: array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. Returns: None. Updates the transformer with feature fitting parameters. \"\"\" self . estimator . fit ( np . array ( x ) ** 2 )","title":"fit()"},{"location":"module/features/#elapid.features.QuadraticTransformer.fit_transform","text":"Fits scaler to x and returns transformed features. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform. required Returns: Type Description ndarray ndarray with transformed data. Source code in elapid/features.py def fit_transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Fits scaler to x and returns transformed features. Args: x: array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform. Returns: ndarray with transformed data. \"\"\" self . fit ( x ) return self . estimator . transform ( np . array ( x ) ** 2 )","title":"fit_transform()"},{"location":"module/features/#elapid.features.QuadraticTransformer.inverse_transform","text":"Revert from transformed features to original covariate values. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_xamples, n_features) Transformed feature data to convert to covariate data. required Returns: Type Description ndarray ndarray with unscaled covariate values. Source code in elapid/features.py def inverse_transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Revert from transformed features to original covariate values. Args: x: array-like of shape (n_xamples, n_features) Transformed feature data to convert to covariate data. Returns: ndarray with unscaled covariate values. \"\"\" return self . estimator . inverse_transform ( np . array ( x )) ** 0.5","title":"inverse_transform()"},{"location":"module/features/#elapid.features.QuadraticTransformer.transform","text":"Scale covariates according to the feature range. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) Input data that will be transformed. required Returns: Type Description ndarray ndarray with transformed data. Source code in elapid/features.py def transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Scale covariates according to the feature range. Args: x: array-like of shape (n_samples, n_features) Input data that will be transformed. Returns: ndarray with transformed data. \"\"\" return self . estimator . transform ( np . array ( x ) ** 2 )","title":"transform()"},{"location":"module/features/#elapid.features.ThresholdTransformer","text":"Applies binary thresholds to each covariate based on n evenly-spaced thresholds across it's min/max range. Source code in elapid/features.py class ThresholdTransformer ( BaseEstimator ): \"\"\"Applies binary thresholds to each covariate based on n evenly-spaced thresholds across it's min/max range.\"\"\" n_thresholds_ : int = None mins_ : np . ndarray = None maxs_ : np . ndarray = None threshold_indices_ : np . ndarray = None def __init__ ( self , n_thresholds : int = MaxentConfig . n_threshold_features ): self . n_thresholds_ = n_thresholds def fit ( self , x : ArrayLike ): \"\"\"Compute the minimum and maximum for scaling. Args: x: array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. Returns: None. Updates the transformer with feature fitting parameters. \"\"\" x = np . array ( x ) self . mins_ = x . min ( axis = 0 ) self . maxs_ = x . max ( axis = 0 ) self . threshold_indices_ = np . linspace ( self . mins_ , self . maxs_ , self . n_thresholds_ ) def transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Scale covariates according to the feature range. Args: x: array-like of shape (n_samples, n_features) Input data that will be transformed. Returns: ndarray with transformed data. \"\"\" x = np . array ( x ) xarr = repeat_array ( x , len ( self . threshold_indices_ ), axis =- 1 ) tarr = repeat_array ( self . threshold_indices_ . transpose (), len ( x ), axis = 0 ) thresh = ( xarr > tarr ) . reshape ( x . shape [ 0 ], - 1 ) return thresh . astype ( np . uint8 ) def fit_transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Fits scaler to x and returns transformed features. Args: x: array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform. Returns: ndarray with transformed data. \"\"\" self . fit ( x ) return self . transform ( x )","title":"ThresholdTransformer"},{"location":"module/features/#elapid.features.ThresholdTransformer.fit","text":"Compute the minimum and maximum for scaling. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. required Returns: Type Description None. Updates the transformer with feature fitting parameters. Source code in elapid/features.py def fit ( self , x : ArrayLike ): \"\"\"Compute the minimum and maximum for scaling. Args: x: array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. Returns: None. Updates the transformer with feature fitting parameters. \"\"\" x = np . array ( x ) self . mins_ = x . min ( axis = 0 ) self . maxs_ = x . max ( axis = 0 ) self . threshold_indices_ = np . linspace ( self . mins_ , self . maxs_ , self . n_thresholds_ )","title":"fit()"},{"location":"module/features/#elapid.features.ThresholdTransformer.fit_transform","text":"Fits scaler to x and returns transformed features. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform. required Returns: Type Description ndarray ndarray with transformed data. Source code in elapid/features.py def fit_transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Fits scaler to x and returns transformed features. Args: x: array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform. Returns: ndarray with transformed data. \"\"\" self . fit ( x ) return self . transform ( x )","title":"fit_transform()"},{"location":"module/features/#elapid.features.ThresholdTransformer.transform","text":"Scale covariates according to the feature range. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) Input data that will be transformed. required Returns: Type Description ndarray ndarray with transformed data. Source code in elapid/features.py def transform ( self , x : ArrayLike ) -> np . ndarray : \"\"\"Scale covariates according to the feature range. Args: x: array-like of shape (n_samples, n_features) Input data that will be transformed. Returns: ndarray with transformed data. \"\"\" x = np . array ( x ) xarr = repeat_array ( x , len ( self . threshold_indices_ ), axis =- 1 ) tarr = repeat_array ( self . threshold_indices_ . transpose (), len ( x ), axis = 0 ) thresh = ( xarr > tarr ) . reshape ( x . shape [ 0 ], - 1 ) return thresh . astype ( np . uint8 )","title":"transform()"},{"location":"module/features/#elapid.features.column_product","text":"Computes the column-wise product of a 2D array. Parameters: Name Type Description Default array ndarray array-like of shape (n_samples, n_features) required Returns: Type Description ndarray ndarray with of shape (n_samples, factorial(n_features-1)) Source code in elapid/features.py def column_product ( array : np . ndarray ) -> np . ndarray : \"\"\"Computes the column-wise product of a 2D array. Args: array: array-like of shape (n_samples, n_features) Returns: ndarray with of shape (n_samples, factorial(n_features-1)) \"\"\" nrows , ncols = array . shape if ncols == 1 : return array else : products = [] for xstart in range ( 0 , ncols - 1 ): products . append ( array [:, xstart ] . reshape ( nrows , 1 ) * array [:, xstart + 1 :]) return np . concatenate ( products , axis = 1 )","title":"column_product()"},{"location":"module/features/#elapid.features.compute_lambdas","text":"Computes lambda parameter values for elastic lasso fits. Parameters: Name Type Description Default y Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples,) with binary presence/background (1/0) values required weights Union[numpy.ndarray, pandas.core.frame.DataFrame] per-sample model weights required reg Union[numpy.ndarray, pandas.core.frame.DataFrame] per-feature regularization coefficients required n_lambdas int number of lambda values to estimate 100 Returns: Type Description lambdas Array of lambda scores of length n_lambda Source code in elapid/features.py def compute_lambdas ( y : ArrayLike , weights : ArrayLike , reg : ArrayLike , n_lambdas : int = MaxentConfig . n_lambdas ) -> np . ndarray : \"\"\"Computes lambda parameter values for elastic lasso fits. Args: y: array-like of shape (n_samples,) with binary presence/background (1/0) values weights: per-sample model weights reg: per-feature regularization coefficients n_lambdas: number of lambda values to estimate Returns: lambdas: Array of lambda scores of length n_lambda \"\"\" n_presence = np . sum ( y ) mean_regularization = np . mean ( reg ) total_weight = np . sum ( weights ) seed_range = np . linspace ( 4 , 0 , n_lambdas ) lambdas = 10 ** ( seed_range ) * mean_regularization * ( n_presence / total_weight ) return lambdas","title":"compute_lambdas()"},{"location":"module/features/#elapid.features.compute_regularization","text":"Computes variable regularization values for all feature data. Parameters: Name Type Description Default y Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples,) with binary presence/background (1/0) values required z ndarray model features (transformations applied to covariates) required feature_labels List[str] list of length n_features, with labels identifying each column's feature type with options [\"linear\", \"quadratic\", \"product\", \"threshold\", \"hinge\", \"categorical\"] required beta_multiplier float scaler for all regularization parameters. higher values exclude more features 1.0 beta_lqp float scaler for linear, quadratic and product feature regularization 1.0 beta_threshold float scaler for threshold feature regularization 1.0 beta_hinge float scaler for hinge feature regularization 1.0 beta_categorical float scaler for categorical feature regularization 1.0 Returns: Type Description max_reg Array with per-feature regularization parameters Source code in elapid/features.py def compute_regularization ( y : ArrayLike , z : np . ndarray , feature_labels : List [ str ], beta_multiplier : float = MaxentConfig . beta_multiplier , beta_lqp : float = MaxentConfig . beta_lqp , beta_threshold : float = MaxentConfig . beta_threshold , beta_hinge : float = MaxentConfig . beta_hinge , beta_categorical : float = MaxentConfig . beta_hinge , ) -> np . ndarray : \"\"\"Computes variable regularization values for all feature data. Args: y: array-like of shape (n_samples,) with binary presence/background (1/0) values z: model features (transformations applied to covariates) feature_labels: list of length n_features, with labels identifying each column's feature type with options [\"linear\", \"quadratic\", \"product\", \"threshold\", \"hinge\", \"categorical\"] beta_multiplier: scaler for all regularization parameters. higher values exclude more features beta_lqp: scaler for linear, quadratic and product feature regularization beta_threshold: scaler for threshold feature regularization beta_hinge: scaler for hinge feature regularization beta_categorical: scaler for categorical feature regularization Returns: max_reg: Array with per-feature regularization parameters \"\"\" # compute regularization based on presence-only locations z1 = z [ y == 1 ] nrows , ncols = z1 . shape labels = np . array ( feature_labels ) nlabels = len ( feature_labels ) assert nlabels == ncols , f \"number of feature_labels ( { nlabels } ) must match number of features ( { ncols } )\" # create arrays to store the regularization params base_regularization = np . zeros ( ncols ) hinge_regularization = np . zeros ( ncols ) threshold_regularization = np . zeros ( ncols ) # use a different reg table based on the features set if \"product\" in labels : table_lqp = RegularizationConfig . product elif \"quadratic\" in labels : table_lqp = RegularizationConfig . quadratic else : table_lqp = RegularizationConfig . linear if \"linear\" in labels : linear_idxs = labels == \"linear\" fr_max , fr_min = table_lqp multiplier = beta_lqp ap = np . interp ( nrows , fr_max , fr_min ) reg = multiplier * ap / np . sqrt ( nrows ) base_regularization [ linear_idxs ] = reg if \"quadratic\" in labels : quadratic_idxs = labels == \"quadratic\" fr_max , fr_min = table_lqp multiplier = beta_lqp ap = np . interp ( nrows , fr_max , fr_min ) reg = multiplier * ap / np . sqrt ( nrows ) base_regularization [ quadratic_idxs ] = reg if \"product\" in labels : product_idxs = labels == \"product\" fr_max , fr_min = table_lqp multiplier = beta_lqp ap = np . interp ( nrows , fr_max , fr_min ) reg = multiplier * ap / np . sqrt ( nrows ) base_regularization [ product_idxs ] = reg if \"threshold\" in labels : threshold_idxs = labels == \"threshold\" fr_max , fr_min = RegularizationConfig . threshold multiplier = beta_threshold ap = np . interp ( nrows , fr_max , fr_min ) reg = multiplier * ap / np . sqrt ( nrows ) base_regularization [ threshold_idxs ] = reg # increase regularization for uniform threshlold values all_zeros = np . all ( z1 == 0 , axis = 0 ) all_ones = np . all ( z1 == 1 , axis = 0 ) threshold_regularization [ all_zeros ] = 1 threshold_regularization [ all_ones ] = 1 if \"hinge\" in labels : hinge_idxs = labels == \"hinge\" fr_max , fr_min = RegularizationConfig . hinge multiplier = beta_hinge ap = np . interp ( nrows , fr_max , fr_min ) reg = multiplier * ap / np . sqrt ( nrows ) base_regularization [ hinge_idxs ] = reg # increase regularization for extreme hinge values hinge_std = np . std ( z1 [:, hinge_idxs ], ddof = 1 , axis = 0 ) hinge_sqrt = np . zeros ( len ( hinge_std )) + ( 1 / np . sqrt ( nrows )) std = np . max (( hinge_std , hinge_sqrt ), axis = 0 ) hinge_regularization [ hinge_idxs ] = ( 0.5 * std ) / np . sqrt ( nrows ) if \"categorical\" in labels : categorical_idxs = labels == \"categorical\" fr_max , fr_min = RegularizationConfig . categorical multiplier = beta_categorical ap = np . interp ( nrows , fr_max , fr_min ) reg = multiplier * ap / np . sqrt ( nrows ) base_regularization [ categorical_idxs ] = reg # compute the maximum regularization based on a few different approaches default_regularization = 0.001 * ( np . max ( z , axis = 0 ) - np . min ( z , axis = 0 )) variance_regularization = np . std ( z1 , ddof = 1 , axis = 0 ) * base_regularization max_regularization = np . max ( ( default_regularization , variance_regularization , hinge_regularization , threshold_regularization ), axis = 0 ) # apply the final scaling factor max_regularization *= beta_multiplier return max_regularization","title":"compute_regularization()"},{"location":"module/features/#elapid.features.compute_weights","text":"Compute Maxent-format per-sample model weights. Parameters: Name Type Description Default y Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples,) with binary presence/background (1/0) values required pbr int presence-to-background weight ratio. pbr=100 sets background samples to 1/100 weight of presence samples. 100 Returns: Type Description weights array with glmnet-formatted sample weights Source code in elapid/features.py def compute_weights ( y : ArrayLike , pbr : int = 100 ) -> np . ndarray : \"\"\"Compute Maxent-format per-sample model weights. Args: y: array-like of shape (n_samples,) with binary presence/background (1/0) values pbr: presence-to-background weight ratio. pbr=100 sets background samples to 1/100 weight of presence samples. Returns: weights: array with glmnet-formatted sample weights \"\"\" weights = np . array ( y + ( 1 - y ) * pbr ) return weights","title":"compute_weights()"},{"location":"module/features/#elapid.features.left_hinge","text":"Computes hinge transformation values. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] Array-like of covariate values required mn float Minimum covariate value to fit hinges to required mx float Maximum covariate value to fit hinges to required Returns: Type Description ndarray Array of hinge features Source code in elapid/features.py def left_hinge ( x : ArrayLike , mn : float , mx : float ) -> np . ndarray : \"\"\"Computes hinge transformation values. Args: x: Array-like of covariate values mn: Minimum covariate value to fit hinges to mx: Maximum covariate value to fit hinges to Returns: Array of hinge features \"\"\" return np . minimum ( 1 , np . maximum ( 0 , ( x - mn ) / ( repeat_array ( mx , mn . shape [ - 1 ], axis = 1 ) - mn )))","title":"left_hinge()"},{"location":"module/features/#elapid.features.right_hinge","text":"Computes hinge transformation values. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] Array-like of covariate values required mn float Minimum covariate value to fit hinges to required mx float Maximum covariate value to fit hinges to required Returns: Type Description ndarray Array of hinge features Source code in elapid/features.py def right_hinge ( x : ArrayLike , mn : float , mx : float ) -> np . ndarray : \"\"\"Computes hinge transformation values. Args: x: Array-like of covariate values mn: Minimum covariate value to fit hinges to mx: Maximum covariate value to fit hinges to Returns: Array of hinge features \"\"\" mn_broadcast = repeat_array ( mn , mx . shape [ - 1 ], axis = 1 ) return np . minimum ( 1 , np . maximum ( 0 , ( x - mn_broadcast ) / ( mx - mn_broadcast )))","title":"right_hinge()"},{"location":"module/geo/","text":"elapid.geo \u00b6 Geospatial data operations like reading/writing/indexing raster and vector data. annotate ( points , raster_paths , labels = None , drop_na = True ) \u00b6 Read raster values for each point in a vector and append as new columns. Parameters: Name Type Description Default points Union[str, geopandas.geoseries.GeoSeries, geopandas.geodataframe.GeoDataFrame] path to a point-format vector, OR GeoDataFrame with point locations, OR GeoSeries (e.g., gdf['geometry']) with point locations required raster_paths Union[str, list] raster paths to extract pixel values from. required labels list band name labels. number of labels should match the total number of bands across all raster_paths. None drop_na bool drop all records with no-data values. True Returns: Type Description gdf GeoDataFrame annotated with the pixel values from each raster Source code in elapid/geo.py def annotate ( points : Union [ str , gpd . GeoSeries , gpd . GeoDataFrame ], raster_paths : Union [ str , list ], labels : list = None , drop_na : bool = True , ): \"\"\"Read raster values for each point in a vector and append as new columns. Args: points: path to a point-format vector, OR GeoDataFrame with point locations, OR GeoSeries (e.g., gdf['geometry']) with point locations raster_paths: raster paths to extract pixel values from. labels: band name labels. number of labels should match the total number of bands across all raster_paths. drop_na: drop all records with no-data values. Returns: gdf: GeoDataFrame annotated with the pixel values from each raster \"\"\" # format the inputs raster_paths = to_iterable ( raster_paths ) labels = format_band_labels ( raster_paths , labels ) # read raster values based on the points dtype if isinstance ( points , gpd . GeoSeries ): gdf = annotate_geoseries ( points , raster_paths , labels = labels , drop_na = drop_na , ) elif isinstance ( points , gpd . GeoDataFrame ) or isinstance ( points , pd . DataFrame ): gdf = annotate_geoseries ( points . geometry , raster_paths , labels = labels , drop_na = drop_na , ) # append annotations to the input dataframe gdf = pd . concat ([ points , gdf . drop ([ \"geometry\" ], axis = 1 , errors = \"ignore\" )], axis = 1 ) elif os . path . isfile ( points ): gdf = annotate_vector ( points , raster_paths , labels = labels , drop_na = drop_na ) else : raise TypeError ( \"points arg must be a valid path, GeoDataFrame, or GeoSeries\" ) return gdf annotate_geoseries ( points , raster_paths , labels = None , drop_na = True , dtype = None ) \u00b6 Reads and stores pixel values from rasters using point locations. Parameters: Name Type Description Default points GeoSeries GeoSeries with point locations. required raster_paths list rasters to extract pixel values from. required labels list band labels. must match the total number of bands for all raster_paths. None drop_na bool drop records with no-data values. True dtype str output column data type. uses the first raster's dtype by default. None Returns: Type Description gdf GeoDataFrame annotated with the pixel values from each raster Source code in elapid/geo.py def annotate_geoseries ( points : gpd . GeoSeries , raster_paths : list , labels : list = None , drop_na : bool = True , dtype : str = None ) -> gpd . GeoDataFrame : \"\"\"Reads and stores pixel values from rasters using point locations. Args: points: GeoSeries with point locations. raster_paths: rasters to extract pixel values from. labels: band labels. must match the total number of bands for all raster_paths. drop_na: drop records with no-data values. dtype: output column data type. uses the first raster's dtype by default. Returns: gdf: GeoDataFrame annotated with the pixel values from each raster \"\"\" # format the inputs raster_paths = to_iterable ( raster_paths ) labels = format_band_labels ( raster_paths , labels ) # get the dataset dimensions n_rasters = len ( raster_paths ) n_points = len ( points ) # create arrays and flags for updating raster_values = [] valid_idxs = [] nodata_flag = False # annotate each point with the pixel values for each raster for raster_idx , raster_path in tqdm ( enumerate ( raster_paths ), desc = \"Raster\" , total = n_rasters , ** tqdm_opts ): with rio . open ( raster_path , \"r\" ) as src : # reproject points to match raster and convert to a dataframe if not crs_match ( points . crs , src . crs ): points = points . to_crs ( src . crs ) # use the first rasters dtype for the output array if not set if raster_idx == 0 and dtype is None : dtype = src . dtypes [ 0 ] # get the raster row/col indices for each point and the respective read windows xys = [( point . x , point . y ) for point in points ] # read each pixel value samples = src . sample ( xys , masked = False ) # assign to an output array outarr = np . zeros (( n_points , src . count ), dtype = dtype ) for idx , sample in enumerate ( samples ): outarr [ idx ] = sample # identify nodata points to remove later if drop_na and src . nodata is not None : nodata_flag = True valid_idxs . append ( outarr [:, 0 ] != src . nodata ) raster_values . append ( outarr ) # merge the arrays from each raster values = np . concatenate ( raster_values , axis = 1 , dtype = dtype ) if nodata_flag : valid = np . max ( valid_idxs , axis = 0 ) values = values [ valid , :] points = points . iloc [ valid ] points . index = range ( valid . sum ()) # convert to a geodataframe gdf = gpd . GeoDataFrame ( values , geometry = points . geometry , columns = labels ) return gdf annotate_vector ( vector_path , raster_paths , labels = None , drop_na = True ) \u00b6 Reads and stores pixel values from rasters using a point-format vector file. Parameters: Name Type Description Default vector_path str path to a vector file (shp, geojson, etc) required raster_paths list raster paths to extract pixel values from required labels list band name labels. should match the total number of bands across all raster_paths None drop_na bool drop all records with no-data values True Returns: Type Description gdf GeoDataFrame annotated with the pixel values from each raster Source code in elapid/geo.py def annotate_vector ( vector_path : str , raster_paths : list , labels : list = None , drop_na : bool = True , ) -> gpd . GeoDataFrame : \"\"\"Reads and stores pixel values from rasters using a point-format vector file. Args: vector_path: path to a vector file (shp, geojson, etc) raster_paths: raster paths to extract pixel values from labels: band name labels. should match the total number of bands across all raster_paths drop_na: drop all records with no-data values Returns: gdf: GeoDataFrame annotated with the pixel values from each raster \"\"\" # format the inputs raster_paths = to_iterable ( raster_paths ) labels = format_band_labels ( raster_paths , labels ) gdf = gpd . read_file ( vector_path ) raster_df = annotate_geoseries ( gdf . geometry , raster_paths , labels , drop_na ) gdf = pd . concat ([ gdf , raster_df . drop ([ \"geometry\" ], axis = 1 , errors = \"ignore\" )], axis = 1 ) return gdf apply_model_to_array ( model , array , nodata , nodata_idx , count = 1 , dtype = 'float32' , predict_proba = False , ** kwargs ) \u00b6 Applies a model to an array of covariates. Covariate array should be of shape (nbands, nrows, ncols). Parameters: Name Type Description Default model BaseEstimator object with a model.predict() function required array ndarray array of shape (nbands, nrows, ncols) with pixel values required nodata float numeric nodata value to apply to the output array required nodata_idx int array of bools with shape (nbands, nrows, ncols) containing nodata locations required count int number of bands in the prediction output 1 dtype str prediction array dtype 'float32' predict_proba bool use model.predict_proba() instead of model.predict() False **kwargs additonal keywords to pass to model.predict(). For MaxentModels, this would include transform=\"logistic\" {} Returns: Type Description ypred_window Array of shape (nrows, ncols) with model predictions Source code in elapid/geo.py def apply_model_to_array ( model : BaseEstimator , array : np . ndarray , nodata : float , nodata_idx : int , count : int = 1 , dtype : str = \"float32\" , predict_proba : bool = False , ** kwargs , ) -> np . ndarray : \"\"\"Applies a model to an array of covariates. Covariate array should be of shape (nbands, nrows, ncols). Args: model: object with a `model.predict()` function array: array of shape (nbands, nrows, ncols) with pixel values nodata: numeric nodata value to apply to the output array nodata_idx: array of bools with shape (nbands, nrows, ncols) containing nodata locations count: number of bands in the prediction output dtype: prediction array dtype predict_proba: use model.predict_proba() instead of model.predict() **kwargs: additonal keywords to pass to model.predict(). For MaxentModels, this would include transform=\"logistic\" Returns: ypred_window: Array of shape (nrows, ncols) with model predictions \"\"\" # only apply to valid pixels valid = ~ nodata_idx . any ( axis = 0 ) covariates = array [:, valid ] . transpose () ypred = model . predict ( covariates , ** kwargs ) if not predict_proba else model . predict_proba ( covariates , ** kwargs ) # reshape to the original window size rows , cols = valid . shape ypred_window = np . zeros (( count , rows , cols ), dtype = dtype ) + nodata ypred_window [:, valid ] = ypred . transpose () return ypred_window apply_model_to_rasters ( model , raster_paths , output_path , resampling =< Resampling . average : 5 > , count = 1 , dtype = 'float32' , nodata =- 9999 , driver = 'GTiff' , compress = 'deflate' , bigtiff = True , template_idx = 0 , windowed = True , predict_proba = False , ignore_sklearn = True , ** kwargs ) \u00b6 Applies a trained model to a list of raster datasets. The list and band order of the rasters must match the order of the covariates used to train the model. It reads each dataset block-by-block, applies the model, and writes gridded predictions. If the raster datasets are not consistent (different extents, resolutions, etc.), it wll re-project the data on the fly, with the grid size, extent and projection based on a 'template' raster. Parameters: Name Type Description Default model BaseEstimator object with a model.predict() function required raster_paths list raster paths of covariates to apply the model to required output_path str path to the output file to create required resampling Enum resampling algorithm to apply to on-the-fly reprojection from rasterio.enums.Resampling <Resampling.average: 5> count int number of bands in the prediction output 1 dtype str the output raster data type 'float32' nodata float output nodata value -9999 driver str output raster format from rasterio.drivers.raster_driver_extensions() 'GTiff' compress str compression to apply to the output file 'deflate' bigtiff bool specify the output file as a bigtiff (for rasters > 2GB) True template_idx int index of the raster file to use as a template. template_idx=0 sets the first raster as template 0 windowed bool apply the model using windowed read/write slower, but more memory efficient True predict_proba bool use model.predict_proba() instead of model.predict() False ignore_sklearn bool silence sklearn warning messages True **kwargs additonal keywords to pass to model.predict() For MaxentModels, this would include transform=\"logistic\" {} Returns: Type Description None saves model predictions to disk. Source code in elapid/geo.py def apply_model_to_rasters ( model : BaseEstimator , raster_paths : list , output_path : str , resampling : rio . enums . Enum = rio . enums . Resampling . average , count : int = 1 , dtype : str = \"float32\" , nodata : float = - 9999 , driver : str = \"GTiff\" , compress : str = \"deflate\" , bigtiff : bool = True , template_idx : int = 0 , windowed : bool = True , predict_proba : bool = False , ignore_sklearn : bool = True , ** kwargs , ) -> None : \"\"\"Applies a trained model to a list of raster datasets. The list and band order of the rasters must match the order of the covariates used to train the model. It reads each dataset block-by-block, applies the model, and writes gridded predictions. If the raster datasets are not consistent (different extents, resolutions, etc.), it wll re-project the data on the fly, with the grid size, extent and projection based on a 'template' raster. Args: model: object with a model.predict() function raster_paths: raster paths of covariates to apply the model to output_path: path to the output file to create resampling: resampling algorithm to apply to on-the-fly reprojection from rasterio.enums.Resampling count: number of bands in the prediction output dtype: the output raster data type nodata: output nodata value driver: output raster format from rasterio.drivers.raster_driver_extensions() compress: compression to apply to the output file bigtiff: specify the output file as a bigtiff (for rasters > 2GB) template_idx: index of the raster file to use as a template. template_idx=0 sets the first raster as template windowed: apply the model using windowed read/write slower, but more memory efficient predict_proba: use model.predict_proba() instead of model.predict() ignore_sklearn: silence sklearn warning messages **kwargs: additonal keywords to pass to model.predict() For MaxentModels, this would include transform=\"logistic\" Returns: None: saves model predictions to disk. \"\"\" # make sure the raster_paths are iterable raster_paths = to_iterable ( raster_paths ) # get and set template parameters windows , dst_profile = create_output_raster_profile ( raster_paths , template_idx , count = count , windowed = windowed , nodata = nodata , compress = compress , driver = driver , bigtiff = bigtiff , ) # get the bands and indexes for each covariate raster nbands , band_idx = get_raster_band_indexes ( raster_paths ) # check whether the raster paths are aligned to determine how the data are read aligned = check_raster_alignment ( raster_paths ) # set a dummy nodata variable if none is set # (acutal nodata reads handled by rasterios src.read(masked=True) method) nodata = nodata or 0 # turn off sklearn warnings if ignore_sklearn : warnings . filterwarnings ( \"ignore\" , category = UserWarning ) # open all rasters to read from later srcs = [ rio . open ( raster_path ) for raster_path in raster_paths ] # use warped VRT reads to align all rasters pixel-pixel if not aligned if not aligned : vrt_options = { \"resampling\" : resampling , \"transform\" : dst_profile [ \"transform\" ], \"crs\" : dst_profile [ \"crs\" ], \"height\" : dst_profile [ \"height\" ], \"width\" : dst_profile [ \"width\" ], } srcs = [ rio . vrt . WarpedVRT ( src , ** vrt_options ) for src in srcs ] # read and reproject blocks from each data source and write predictions to disk with rio . open ( output_path , \"w\" , ** dst_profile ) as dst : for window in tqdm ( windows , desc = \"Window\" , ** tqdm_opts ): # create stacked arrays to handle multi-raster, multi-band inputs # that may have different nodata locations covariates = np . zeros (( nbands , window . height , window . width ), dtype = np . float32 ) nodata_idx = np . ones_like ( covariates , dtype = bool ) try : for i , src in enumerate ( srcs ): data = src . read ( window = window , masked = True ) covariates [ band_idx [ i ] : band_idx [ i + 1 ]] = data nodata_idx [ band_idx [ i ] : band_idx [ i + 1 ]] = data . mask # skip blocks full of no-data if data . mask . all (): raise NoDataException () predictions = apply_model_to_array ( model , covariates , nodata , nodata_idx , count = count , dtype = dtype , predict_proba = predict_proba , ** kwargs , ) dst . write ( predictions , window = window ) except NoDataException : continue crs_match ( crs1 , crs2 ) \u00b6 Evaluates whether two coordinate reference systems are the same. Parameters: Name Type Description Default crs1 Union[pyproj.crs.crs.CRS, str] the first CRS, from a rasterio dataset, a GeoDataFrame, or a string with projection parameters. required crs2 Union[pyproj.crs.crs.CRS, str] the second CRS, from the same sources above. required Returns: Type Description matches Boolean for whether the CRS match. Source code in elapid/geo.py def crs_match ( crs1 : CRSType , crs2 : CRSType ) -> bool : \"\"\"Evaluates whether two coordinate reference systems are the same. Args: crs1: the first CRS, from a rasterio dataset, a GeoDataFrame, or a string with projection parameters. crs2: the second CRS, from the same sources above. Returns: matches: Boolean for whether the CRS match. \"\"\" # normalize string inputs via rasterio if type ( crs1 ) is str : crs1 = string_to_crs ( crs1 ) if type ( crs2 ) is str : crs2 = string_to_crs ( crs2 ) matches = crs1 == crs2 return matches parse_crs_string ( string ) \u00b6 Parses a string to determine the CRS/spatial projection format. Parameters: Name Type Description Default string str a string with CRS/projection data. required Returns: Type Description crs_type Str in [\"wkt\", \"proj4\", \"epsg\", \"string\"]. Source code in elapid/geo.py def parse_crs_string ( string : str ) -> str : \"\"\"Parses a string to determine the CRS/spatial projection format. Args: string: a string with CRS/projection data. Returns: crs_type: Str in [\"wkt\", \"proj4\", \"epsg\", \"string\"]. \"\"\" if \"epsg:\" in string . lower (): return \"epsg\" elif \"+proj\" in string : return \"proj4\" elif \"SPHEROID\" in string : return \"wkt\" else : return \"string\" read_raster_from_polygon ( src , poly ) \u00b6 Read valid pixel values from all locations inside a polygon Uses the polygon as a mask in addition to the existing raster mask Parameters: Name Type Description Default src DatasetReader an open rasterio dataset to read from required poly Union[shapely.geometry.polygon.Polygon, shapely.geometry.multipolygon.MultiPolygon] a shapely Polygon or MultiPolygon required Returns: Type Description MaskedArray masked array of shape (nbands, nrows, ncols) Source code in elapid/geo.py def read_raster_from_polygon ( src : rio . DatasetReader , poly : Union [ Polygon , MultiPolygon ]) -> np . ma . MaskedArray : \"\"\"Read valid pixel values from all locations inside a polygon Uses the polygon as a mask in addition to the existing raster mask Args: src: an open rasterio dataset to read from poly: a shapely Polygon or MultiPolygon Returns: masked array of shape (nbands, nrows, ncols) \"\"\" # get the read parameters window = rio . windows . from_bounds ( * poly . bounds , src . transform ) transform = rio . windows . transform ( window , src . transform ) # get the data data = src . read ( window = window , masked = True , boundless = True ) bands , rows , cols = data . shape poly_mask = geometry_mask ([ poly ], transform = transform , out_shape = ( rows , cols )) # update the mask data [:, poly_mask ] = np . ma . masked return data sample_bias_file ( raster_path , count , ignore_mask = False ) \u00b6 Creates a semi-random geographic sampling of points weighted towards biased areas. Parameters: Name Type Description Default raster_path str raster bias file path to sample from. pixel values can be in arbitrary range, but must be odered low -> high probability required count int total number of samples to generate required ignore_mask bool sample from the full extent of the raster instead of unmasked areas only False Returns: Type Description points Point geometry geoseries Source code in elapid/geo.py def sample_bias_file ( raster_path : str , count : int , ignore_mask : bool = False ) -> gpd . GeoSeries : \"\"\"Creates a semi-random geographic sampling of points weighted towards biased areas. Args: raster_path: raster bias file path to sample from. pixel values can be in arbitrary range, but must be odered low -> high probability count: total number of samples to generate ignore_mask: sample from the full extent of the raster instead of unmasked areas only Returns: points: Point geometry geoseries \"\"\" with rio . open ( raster_path ) as src : if src . nodata is None or ignore_mask : data = src . read ( 1 ) rows , cols = np . where ( data ) values = data . flatten () probabilities = ( values - values . min ()) / ( values - values . min ()) . sum () samples = np . random . choice ( len ( rows ), size = count , p = probabilities ) else : data = src . read ( 1 , masked = True ) rows , cols = np . where ( ~ data . mask ) values = data [ rows , cols ] probabilities = ( values - values . min ()) / ( values - values . min ()) . sum () samples = np . random . choice ( len ( rows ), size = count , p = probabilities ) xy = np . zeros (( count , 2 )) for i , sample in enumerate ( samples ): xy [ i ] = src . xy ( rows [ sample ], cols [ sample ]) points = xy_to_geoseries ( xy [:, 0 ], xy [:, 1 ], crs = src . crs ) return points sample_geoseries ( geoseries , count , overestimate = 2 ) \u00b6 Creates random geographic point samples inside a polygon/multipolygon Parameters: Name Type Description Default geoseries GeoSeries geometry dataset (e.g. gdf['geometry'] ) with polygons/multipolygons required count int number of samples to generate required overestimate float scaler to generate extra samples to toss points outside of the polygon/inside it's bounds 2 Returns: Type Description points Point geometry geoseries Source code in elapid/geo.py def sample_geoseries ( geoseries : gpd . GeoSeries , count : int , overestimate : float = 2 ) -> gpd . GeoSeries : \"\"\"Creates random geographic point samples inside a polygon/multipolygon Args: geoseries: geometry dataset (e.g. `gdf['geometry']`) with polygons/multipolygons count: number of samples to generate overestimate: scaler to generate extra samples to toss points outside of the polygon/inside it's bounds Returns: points: Point geometry geoseries \"\"\" if type ( geoseries ) is gpd . GeoDataFrame : geoseries = geoseries . geometry polygon = geoseries . unary_union xmin , ymin , xmax , ymax = polygon . bounds ratio = polygon . area / polygon . envelope . area samples = np . random . uniform (( xmin , ymin ), ( xmax , ymax ), ( int ( count / ratio * overestimate ), 2 )) multipoint = MultiPoint ( samples ) multipoint = multipoint . intersection ( polygon ) sample_array = np . zeros (( len ( multipoint . geoms ), 2 )) for idx , point in enumerate ( multipoint . geoms ): sample_array [ idx ] = ( point . x , point . y ) xy = sample_array [ np . random . choice ( len ( sample_array ), count )] points = xy_to_geoseries ( xy [:, 0 ], xy [:, 1 ], crs = geoseries . crs ) return points sample_raster ( raster_path , count , ignore_mask = False ) \u00b6 Creates a random geographic sampling of points based on a raster's extent. Selects from unmasked locations if the rasters nodata value is set. Parameters: Name Type Description Default raster_path str raster file path to sample locations from required count int number of samples to generate required ignore_mask bool sample from the full extent of the raster instead of unmasked areas only False Returns: Type Description points Point geometry geoseries Source code in elapid/geo.py def sample_raster ( raster_path : str , count : int , ignore_mask : bool = False ) -> gpd . GeoSeries : \"\"\"Creates a random geographic sampling of points based on a raster's extent. Selects from unmasked locations if the rasters nodata value is set. Args: raster_path: raster file path to sample locations from count: number of samples to generate ignore_mask: sample from the full extent of the raster instead of unmasked areas only Returns: points: Point geometry geoseries \"\"\" # handle masked vs unmasked data differently with rio . open ( raster_path ) as src : if src . nodata is None or ignore_mask : xmin , ymin , xmax , ymax = src . bounds xy = np . random . uniform (( xmin , ymin ), ( xmax , ymax ), ( count , 2 )) else : masked = src . read_masks ( 1 ) rows , cols = np . where ( masked == 255 ) samples = np . random . randint ( 0 , len ( rows ), count ) xy = np . zeros (( count , 2 )) for i , sample in enumerate ( samples ): xy [ i ] = src . xy ( rows [ sample ], cols [ sample ]) points = xy_to_geoseries ( xy [:, 0 ], xy [:, 1 ], crs = src . crs ) return points sample_vector ( vector_path , count , overestimate = 2 ) \u00b6 Creates a random geographic sampling of points inside of a polygon/multipolygon type vector file. Parameters: Name Type Description Default vector_path str path to a vector file (shp, geojson, etc) required count int number of samples to generate required overestimate float scaler to generate extra samples to toss points outside of the polygon/inside it's bounds 2 Returns: Type Description points Point geometry geoseries Source code in elapid/geo.py def sample_vector ( vector_path : str , count : int , overestimate : float = 2 ) -> gpd . GeoSeries : \"\"\"Creates a random geographic sampling of points inside of a polygon/multipolygon type vector file. Args: vector_path: path to a vector file (shp, geojson, etc) count: number of samples to generate overestimate: scaler to generate extra samples to toss points outside of the polygon/inside it's bounds Returns: points: Point geometry geoseries \"\"\" gdf = gpd . read_file ( vector_path ) return sample_geoseries ( gdf . geometry , count , overestimate = overestimate ) string_to_crs ( string ) \u00b6 Converts a crs/projection string to a pyproj-readable CRS object Parameters: Name Type Description Default string str a crs/projection string. required crs_type the type of crs/projection string, in [\"wkt\", \"proj4\", \"epsg\", \"string\"]. required Returns: Type Description crs the coordinate reference system Source code in elapid/geo.py def string_to_crs ( string : str ) -> rio . crs . CRS : \"\"\"Converts a crs/projection string to a pyproj-readable CRS object Args: string: a crs/projection string. crs_type: the type of crs/projection string, in [\"wkt\", \"proj4\", \"epsg\", \"string\"]. Returns: crs: the coordinate reference system \"\"\" crs_type = parse_crs_string ( string ) if crs_type == \"epsg\" : auth , code = string . split ( \":\" ) crs = rio . crs . CRS . from_epsg ( int ( code )) elif crs_type == \"proj4\" : crs = rio . crs . CRS . from_proj4 ( string ) elif crs_type == \"wkt\" : crs = rio . crs . CRS . from_wkt ( string ) else : crs = rio . crs . CRS . from_string ( string ) return crs validate_gpd ( geo ) \u00b6 Validates whether an input is a GeoDataFrame or a GeoSeries. Parameters: Name Type Description Default geo Union[geopandas.geoseries.GeoSeries, geopandas.geodataframe.GeoDataFrame] an input variable that should be in GeoPandas format required Source code in elapid/geo.py def validate_gpd ( geo : Union [ gpd . GeoSeries , gpd . GeoDataFrame ]) -> None : \"\"\"Validates whether an input is a GeoDataFrame or a GeoSeries. Args: geo: an input variable that should be in GeoPandas format Raises: TypeError if geo is not in GeoPandas format \"\"\" if not ( isinstance ( geo , gpd . GeoDataFrame ) or isinstance ( geo , gpd . GeoSeries )): raise TypeError ( \"Input must be a GeoDataFrame or GeoSeries\" ) validate_polygons ( geometry ) \u00b6 Iterate over a geoseries to find rows with invalid geometry types. Parameters: Name Type Description Default geometry Union[geopandas.geoseries.GeoSeries, geopandas.geodataframe.GeoDataFrame] a GeoSeries or GeoDataFrame with polygon geometries required Returns: Type Description Index an index of rows with valid polygon types Source code in elapid/geo.py def validate_polygons ( geometry : Union [ gpd . GeoSeries , gpd . GeoDataFrame ]) -> pd . Index : \"\"\"Iterate over a geoseries to find rows with invalid geometry types. Args: geometry: a GeoSeries or GeoDataFrame with polygon geometries Returns: an index of rows with valid polygon types \"\"\" if isinstance ( geometry , gpd . GeoDataFrame ): geometry = geometry . geometry index = [] for idx , geom in enumerate ( geometry ): if not ( isinstance ( geom , Polygon ) or isinstance ( geom , MultiPolygon )): index . append ( idx ) if len ( index ) > 0 : warnings . warn ( f \"Input geometry had { len ( index ) } invalid geometries. \" \"These will be dropped, but with the original index preserved.\" ) geometry . drop ( index = index , inplace = True ) return geometry . index xy_to_geoseries ( x , y , crs = 'epsg:4236' ) \u00b6 Converts x/y data into a geopandas geoseries. Parameters: Name Type Description Default x Union[float, list, numpy.ndarray] 1-D array-like of x location values required y Union[float, list, numpy.ndarray] 1-D array-like of y location values required crs Union[pyproj.crs.crs.CRS, str] coordinate reference system. accepts pyproj.CRS / rio.crs.CRS objects or anything allowed by pyproj.CRS.from_user_input() 'epsg:4236' Returns: Type Description gs Point geometry geoseries Source code in elapid/geo.py def xy_to_geoseries ( x : Union [ float , list , np . ndarray ], y : Union [ float , list , np . ndarray ], crs : CRSType = \"epsg:4236\" ) -> gpd . GeoSeries : \"\"\"Converts x/y data into a geopandas geoseries. Args: x: 1-D array-like of x location values y: 1-D array-like of y location values crs: coordinate reference system. accepts pyproj.CRS / rio.crs.CRS objects or anything allowed by pyproj.CRS.from_user_input() Returns: gs: Point geometry geoseries \"\"\" # handle single x/y location values x = to_iterable ( x ) y = to_iterable ( y ) points = [ Point ( x , y ) for x , y in zip ( x , y )] gs = gpd . GeoSeries ( points , crs = crs ) return gs zonal_stats ( polygons , raster_paths , labels = None , all_touched = True , mean = True , stdv = True , min = False , max = False , count = False , sum = False , skew = False , kurtosis = False , mode = False , all = False , percentiles = []) \u00b6 Compute raster summary stats for each polygon in a GeoSeries or GeoDataFrame. Parameters: Name Type Description Default polygons Union[geopandas.geoseries.GeoSeries, geopandas.geodataframe.GeoDataFrame] GeoSeries or GeoDataFrame with polygon geometries. required raster_paths list list of paths to rasters to summarize required labels list band labels. must match the total number of bands for all raster_paths. None all_touched bool include all pixels that touch a polygon. set to False to only include pixels whose centers intersect the polygon True mean, min, max, count, sum, stdv, skew, kurtosis, mode set to True to compute these stats required all bool compute all of the above stats False percentiles list list of 0-100 percentile ranges to compute [] Returns: Type Description GeoDataFrame GeoDataFrame with zonal stats for each raster band in new columns. If polygons is a GeoDataFrame, the zonal stats columns are appended to the original input. Source code in elapid/geo.py def zonal_stats ( polygons : Union [ gpd . GeoSeries , gpd . GeoDataFrame ], raster_paths : list , labels : list = None , all_touched : bool = True , mean : bool = True , stdv : bool = True , min : bool = False , max : bool = False , count : bool = False , sum : bool = False , skew : bool = False , kurtosis : bool = False , mode : bool = False , all : bool = False , percentiles : list = [], ) -> gpd . GeoDataFrame : \"\"\"Compute raster summary stats for each polygon in a GeoSeries or GeoDataFrame. Args: polygons: GeoSeries or GeoDataFrame with polygon geometries. raster_paths: list of paths to rasters to summarize labels: band labels. must match the total number of bands for all raster_paths. all_touched: include all pixels that touch a polygon. set to False to only include pixels whose centers intersect the polygon mean, min, max, count, sum, stdv, skew, kurtosis, mode: set to True to compute these stats all: compute all of the above stats percentiles: list of 0-100 percentile ranges to compute Returns: GeoDataFrame with zonal stats for each raster band in new columns. If `polygons` is a GeoDataFrame, the zonal stats columns are appended to the original input. \"\"\" # format the input geometries validate_gpd ( polygons ) valid_idx = validate_polygons ( polygons ) polygons = polygons . iloc [ valid_idx ] is_df = isinstance ( polygons , gpd . GeoDataFrame ) polys = polygons . geometry if is_df else polygons # format the input labels raster_paths = to_iterable ( raster_paths ) labels = format_band_labels ( raster_paths , labels ) # get the bands and indexes for each covariate raster nbands , band_idx = get_raster_band_indexes ( raster_paths ) # get the stats methods to compute for each feature stats_methods = get_raster_stats_methods ( mean = mean , min = min , max = max , count = count , sum = sum , stdv = stdv , skew = skew , kurtosis = kurtosis , mode = mode , percentiles = percentiles , all = all , ) # create dataframes for each raster and concatenate at the end raster_dfs = [] # run zonal stats raster-by-raster (instead of iterating first over geometries) for r , raster in tqdm ( enumerate ( raster_paths ), total = len ( raster_paths ), desc = \"Raster\" , ** tqdm_opts ): # format the band labels band_labels = labels [ band_idx [ r ] : band_idx [ r + 1 ]] n_raster_bands = band_idx [ r + 1 ] - band_idx [ r ] stats_labels = [] for method in stats_methods : stats_labels . append ([ f \" { band } _ { method . name } \" for band in band_labels ]) # open the raster for reading with rio . open ( raster , \"r\" ) as src : # reproject the polygon data as necessary if not crs_match ( polys . crs , src . crs ): polys = polys . to_crs ( src . crs ) # create output arrays to store each stat's output stats_arrays = [] for method in stats_methods : dtype = method . dtype or src . dtypes [ 0 ] stats_arrays . append ( np . zeros (( len ( polys ), n_raster_bands ), dtype = dtype )) # iterate over each geometry to read data and compute stats for p , poly in tqdm ( enumerate ( polys ), total = len ( polys ), desc = \"Polygon\" , leave = False , ** tqdm_opts ): data = read_raster_from_polygon ( src , poly ) for method , array in zip ( stats_methods , stats_arrays ): array [ p , :] = method . reduce ( data ) # convert each stat's array into dataframes and merge them together stats_dfs = [ pd . DataFrame ( array , columns = labels ) for array , labels in zip ( stats_arrays , stats_labels )] raster_dfs . append ( pd . concat ( stats_dfs , axis = 1 )) # merge the outputs from each raster if is_df : merged = gpd . GeoDataFrame ( pd . concat ([ polygons ] + raster_dfs , axis = 1 ), crs = polygons . crs ) else : merged = gpd . GeoDataFrame ( pd . concat ( raster_dfs , axis = 1 ), geometry = polygons , crs = polygons . crs ) return merged","title":"elapid.geo"},{"location":"module/geo/#elapidgeo","text":"Geospatial data operations like reading/writing/indexing raster and vector data.","title":"elapid.geo"},{"location":"module/geo/#elapid.geo.annotate","text":"Read raster values for each point in a vector and append as new columns. Parameters: Name Type Description Default points Union[str, geopandas.geoseries.GeoSeries, geopandas.geodataframe.GeoDataFrame] path to a point-format vector, OR GeoDataFrame with point locations, OR GeoSeries (e.g., gdf['geometry']) with point locations required raster_paths Union[str, list] raster paths to extract pixel values from. required labels list band name labels. number of labels should match the total number of bands across all raster_paths. None drop_na bool drop all records with no-data values. True Returns: Type Description gdf GeoDataFrame annotated with the pixel values from each raster Source code in elapid/geo.py def annotate ( points : Union [ str , gpd . GeoSeries , gpd . GeoDataFrame ], raster_paths : Union [ str , list ], labels : list = None , drop_na : bool = True , ): \"\"\"Read raster values for each point in a vector and append as new columns. Args: points: path to a point-format vector, OR GeoDataFrame with point locations, OR GeoSeries (e.g., gdf['geometry']) with point locations raster_paths: raster paths to extract pixel values from. labels: band name labels. number of labels should match the total number of bands across all raster_paths. drop_na: drop all records with no-data values. Returns: gdf: GeoDataFrame annotated with the pixel values from each raster \"\"\" # format the inputs raster_paths = to_iterable ( raster_paths ) labels = format_band_labels ( raster_paths , labels ) # read raster values based on the points dtype if isinstance ( points , gpd . GeoSeries ): gdf = annotate_geoseries ( points , raster_paths , labels = labels , drop_na = drop_na , ) elif isinstance ( points , gpd . GeoDataFrame ) or isinstance ( points , pd . DataFrame ): gdf = annotate_geoseries ( points . geometry , raster_paths , labels = labels , drop_na = drop_na , ) # append annotations to the input dataframe gdf = pd . concat ([ points , gdf . drop ([ \"geometry\" ], axis = 1 , errors = \"ignore\" )], axis = 1 ) elif os . path . isfile ( points ): gdf = annotate_vector ( points , raster_paths , labels = labels , drop_na = drop_na ) else : raise TypeError ( \"points arg must be a valid path, GeoDataFrame, or GeoSeries\" ) return gdf","title":"annotate()"},{"location":"module/geo/#elapid.geo.annotate_geoseries","text":"Reads and stores pixel values from rasters using point locations. Parameters: Name Type Description Default points GeoSeries GeoSeries with point locations. required raster_paths list rasters to extract pixel values from. required labels list band labels. must match the total number of bands for all raster_paths. None drop_na bool drop records with no-data values. True dtype str output column data type. uses the first raster's dtype by default. None Returns: Type Description gdf GeoDataFrame annotated with the pixel values from each raster Source code in elapid/geo.py def annotate_geoseries ( points : gpd . GeoSeries , raster_paths : list , labels : list = None , drop_na : bool = True , dtype : str = None ) -> gpd . GeoDataFrame : \"\"\"Reads and stores pixel values from rasters using point locations. Args: points: GeoSeries with point locations. raster_paths: rasters to extract pixel values from. labels: band labels. must match the total number of bands for all raster_paths. drop_na: drop records with no-data values. dtype: output column data type. uses the first raster's dtype by default. Returns: gdf: GeoDataFrame annotated with the pixel values from each raster \"\"\" # format the inputs raster_paths = to_iterable ( raster_paths ) labels = format_band_labels ( raster_paths , labels ) # get the dataset dimensions n_rasters = len ( raster_paths ) n_points = len ( points ) # create arrays and flags for updating raster_values = [] valid_idxs = [] nodata_flag = False # annotate each point with the pixel values for each raster for raster_idx , raster_path in tqdm ( enumerate ( raster_paths ), desc = \"Raster\" , total = n_rasters , ** tqdm_opts ): with rio . open ( raster_path , \"r\" ) as src : # reproject points to match raster and convert to a dataframe if not crs_match ( points . crs , src . crs ): points = points . to_crs ( src . crs ) # use the first rasters dtype for the output array if not set if raster_idx == 0 and dtype is None : dtype = src . dtypes [ 0 ] # get the raster row/col indices for each point and the respective read windows xys = [( point . x , point . y ) for point in points ] # read each pixel value samples = src . sample ( xys , masked = False ) # assign to an output array outarr = np . zeros (( n_points , src . count ), dtype = dtype ) for idx , sample in enumerate ( samples ): outarr [ idx ] = sample # identify nodata points to remove later if drop_na and src . nodata is not None : nodata_flag = True valid_idxs . append ( outarr [:, 0 ] != src . nodata ) raster_values . append ( outarr ) # merge the arrays from each raster values = np . concatenate ( raster_values , axis = 1 , dtype = dtype ) if nodata_flag : valid = np . max ( valid_idxs , axis = 0 ) values = values [ valid , :] points = points . iloc [ valid ] points . index = range ( valid . sum ()) # convert to a geodataframe gdf = gpd . GeoDataFrame ( values , geometry = points . geometry , columns = labels ) return gdf","title":"annotate_geoseries()"},{"location":"module/geo/#elapid.geo.annotate_vector","text":"Reads and stores pixel values from rasters using a point-format vector file. Parameters: Name Type Description Default vector_path str path to a vector file (shp, geojson, etc) required raster_paths list raster paths to extract pixel values from required labels list band name labels. should match the total number of bands across all raster_paths None drop_na bool drop all records with no-data values True Returns: Type Description gdf GeoDataFrame annotated with the pixel values from each raster Source code in elapid/geo.py def annotate_vector ( vector_path : str , raster_paths : list , labels : list = None , drop_na : bool = True , ) -> gpd . GeoDataFrame : \"\"\"Reads and stores pixel values from rasters using a point-format vector file. Args: vector_path: path to a vector file (shp, geojson, etc) raster_paths: raster paths to extract pixel values from labels: band name labels. should match the total number of bands across all raster_paths drop_na: drop all records with no-data values Returns: gdf: GeoDataFrame annotated with the pixel values from each raster \"\"\" # format the inputs raster_paths = to_iterable ( raster_paths ) labels = format_band_labels ( raster_paths , labels ) gdf = gpd . read_file ( vector_path ) raster_df = annotate_geoseries ( gdf . geometry , raster_paths , labels , drop_na ) gdf = pd . concat ([ gdf , raster_df . drop ([ \"geometry\" ], axis = 1 , errors = \"ignore\" )], axis = 1 ) return gdf","title":"annotate_vector()"},{"location":"module/geo/#elapid.geo.apply_model_to_array","text":"Applies a model to an array of covariates. Covariate array should be of shape (nbands, nrows, ncols). Parameters: Name Type Description Default model BaseEstimator object with a model.predict() function required array ndarray array of shape (nbands, nrows, ncols) with pixel values required nodata float numeric nodata value to apply to the output array required nodata_idx int array of bools with shape (nbands, nrows, ncols) containing nodata locations required count int number of bands in the prediction output 1 dtype str prediction array dtype 'float32' predict_proba bool use model.predict_proba() instead of model.predict() False **kwargs additonal keywords to pass to model.predict(). For MaxentModels, this would include transform=\"logistic\" {} Returns: Type Description ypred_window Array of shape (nrows, ncols) with model predictions Source code in elapid/geo.py def apply_model_to_array ( model : BaseEstimator , array : np . ndarray , nodata : float , nodata_idx : int , count : int = 1 , dtype : str = \"float32\" , predict_proba : bool = False , ** kwargs , ) -> np . ndarray : \"\"\"Applies a model to an array of covariates. Covariate array should be of shape (nbands, nrows, ncols). Args: model: object with a `model.predict()` function array: array of shape (nbands, nrows, ncols) with pixel values nodata: numeric nodata value to apply to the output array nodata_idx: array of bools with shape (nbands, nrows, ncols) containing nodata locations count: number of bands in the prediction output dtype: prediction array dtype predict_proba: use model.predict_proba() instead of model.predict() **kwargs: additonal keywords to pass to model.predict(). For MaxentModels, this would include transform=\"logistic\" Returns: ypred_window: Array of shape (nrows, ncols) with model predictions \"\"\" # only apply to valid pixels valid = ~ nodata_idx . any ( axis = 0 ) covariates = array [:, valid ] . transpose () ypred = model . predict ( covariates , ** kwargs ) if not predict_proba else model . predict_proba ( covariates , ** kwargs ) # reshape to the original window size rows , cols = valid . shape ypred_window = np . zeros (( count , rows , cols ), dtype = dtype ) + nodata ypred_window [:, valid ] = ypred . transpose () return ypred_window","title":"apply_model_to_array()"},{"location":"module/geo/#elapid.geo.apply_model_to_rasters","text":"Applies a trained model to a list of raster datasets. The list and band order of the rasters must match the order of the covariates used to train the model. It reads each dataset block-by-block, applies the model, and writes gridded predictions. If the raster datasets are not consistent (different extents, resolutions, etc.), it wll re-project the data on the fly, with the grid size, extent and projection based on a 'template' raster. Parameters: Name Type Description Default model BaseEstimator object with a model.predict() function required raster_paths list raster paths of covariates to apply the model to required output_path str path to the output file to create required resampling Enum resampling algorithm to apply to on-the-fly reprojection from rasterio.enums.Resampling <Resampling.average: 5> count int number of bands in the prediction output 1 dtype str the output raster data type 'float32' nodata float output nodata value -9999 driver str output raster format from rasterio.drivers.raster_driver_extensions() 'GTiff' compress str compression to apply to the output file 'deflate' bigtiff bool specify the output file as a bigtiff (for rasters > 2GB) True template_idx int index of the raster file to use as a template. template_idx=0 sets the first raster as template 0 windowed bool apply the model using windowed read/write slower, but more memory efficient True predict_proba bool use model.predict_proba() instead of model.predict() False ignore_sklearn bool silence sklearn warning messages True **kwargs additonal keywords to pass to model.predict() For MaxentModels, this would include transform=\"logistic\" {} Returns: Type Description None saves model predictions to disk. Source code in elapid/geo.py def apply_model_to_rasters ( model : BaseEstimator , raster_paths : list , output_path : str , resampling : rio . enums . Enum = rio . enums . Resampling . average , count : int = 1 , dtype : str = \"float32\" , nodata : float = - 9999 , driver : str = \"GTiff\" , compress : str = \"deflate\" , bigtiff : bool = True , template_idx : int = 0 , windowed : bool = True , predict_proba : bool = False , ignore_sklearn : bool = True , ** kwargs , ) -> None : \"\"\"Applies a trained model to a list of raster datasets. The list and band order of the rasters must match the order of the covariates used to train the model. It reads each dataset block-by-block, applies the model, and writes gridded predictions. If the raster datasets are not consistent (different extents, resolutions, etc.), it wll re-project the data on the fly, with the grid size, extent and projection based on a 'template' raster. Args: model: object with a model.predict() function raster_paths: raster paths of covariates to apply the model to output_path: path to the output file to create resampling: resampling algorithm to apply to on-the-fly reprojection from rasterio.enums.Resampling count: number of bands in the prediction output dtype: the output raster data type nodata: output nodata value driver: output raster format from rasterio.drivers.raster_driver_extensions() compress: compression to apply to the output file bigtiff: specify the output file as a bigtiff (for rasters > 2GB) template_idx: index of the raster file to use as a template. template_idx=0 sets the first raster as template windowed: apply the model using windowed read/write slower, but more memory efficient predict_proba: use model.predict_proba() instead of model.predict() ignore_sklearn: silence sklearn warning messages **kwargs: additonal keywords to pass to model.predict() For MaxentModels, this would include transform=\"logistic\" Returns: None: saves model predictions to disk. \"\"\" # make sure the raster_paths are iterable raster_paths = to_iterable ( raster_paths ) # get and set template parameters windows , dst_profile = create_output_raster_profile ( raster_paths , template_idx , count = count , windowed = windowed , nodata = nodata , compress = compress , driver = driver , bigtiff = bigtiff , ) # get the bands and indexes for each covariate raster nbands , band_idx = get_raster_band_indexes ( raster_paths ) # check whether the raster paths are aligned to determine how the data are read aligned = check_raster_alignment ( raster_paths ) # set a dummy nodata variable if none is set # (acutal nodata reads handled by rasterios src.read(masked=True) method) nodata = nodata or 0 # turn off sklearn warnings if ignore_sklearn : warnings . filterwarnings ( \"ignore\" , category = UserWarning ) # open all rasters to read from later srcs = [ rio . open ( raster_path ) for raster_path in raster_paths ] # use warped VRT reads to align all rasters pixel-pixel if not aligned if not aligned : vrt_options = { \"resampling\" : resampling , \"transform\" : dst_profile [ \"transform\" ], \"crs\" : dst_profile [ \"crs\" ], \"height\" : dst_profile [ \"height\" ], \"width\" : dst_profile [ \"width\" ], } srcs = [ rio . vrt . WarpedVRT ( src , ** vrt_options ) for src in srcs ] # read and reproject blocks from each data source and write predictions to disk with rio . open ( output_path , \"w\" , ** dst_profile ) as dst : for window in tqdm ( windows , desc = \"Window\" , ** tqdm_opts ): # create stacked arrays to handle multi-raster, multi-band inputs # that may have different nodata locations covariates = np . zeros (( nbands , window . height , window . width ), dtype = np . float32 ) nodata_idx = np . ones_like ( covariates , dtype = bool ) try : for i , src in enumerate ( srcs ): data = src . read ( window = window , masked = True ) covariates [ band_idx [ i ] : band_idx [ i + 1 ]] = data nodata_idx [ band_idx [ i ] : band_idx [ i + 1 ]] = data . mask # skip blocks full of no-data if data . mask . all (): raise NoDataException () predictions = apply_model_to_array ( model , covariates , nodata , nodata_idx , count = count , dtype = dtype , predict_proba = predict_proba , ** kwargs , ) dst . write ( predictions , window = window ) except NoDataException : continue","title":"apply_model_to_rasters()"},{"location":"module/geo/#elapid.geo.crs_match","text":"Evaluates whether two coordinate reference systems are the same. Parameters: Name Type Description Default crs1 Union[pyproj.crs.crs.CRS, str] the first CRS, from a rasterio dataset, a GeoDataFrame, or a string with projection parameters. required crs2 Union[pyproj.crs.crs.CRS, str] the second CRS, from the same sources above. required Returns: Type Description matches Boolean for whether the CRS match. Source code in elapid/geo.py def crs_match ( crs1 : CRSType , crs2 : CRSType ) -> bool : \"\"\"Evaluates whether two coordinate reference systems are the same. Args: crs1: the first CRS, from a rasterio dataset, a GeoDataFrame, or a string with projection parameters. crs2: the second CRS, from the same sources above. Returns: matches: Boolean for whether the CRS match. \"\"\" # normalize string inputs via rasterio if type ( crs1 ) is str : crs1 = string_to_crs ( crs1 ) if type ( crs2 ) is str : crs2 = string_to_crs ( crs2 ) matches = crs1 == crs2 return matches","title":"crs_match()"},{"location":"module/geo/#elapid.geo.parse_crs_string","text":"Parses a string to determine the CRS/spatial projection format. Parameters: Name Type Description Default string str a string with CRS/projection data. required Returns: Type Description crs_type Str in [\"wkt\", \"proj4\", \"epsg\", \"string\"]. Source code in elapid/geo.py def parse_crs_string ( string : str ) -> str : \"\"\"Parses a string to determine the CRS/spatial projection format. Args: string: a string with CRS/projection data. Returns: crs_type: Str in [\"wkt\", \"proj4\", \"epsg\", \"string\"]. \"\"\" if \"epsg:\" in string . lower (): return \"epsg\" elif \"+proj\" in string : return \"proj4\" elif \"SPHEROID\" in string : return \"wkt\" else : return \"string\"","title":"parse_crs_string()"},{"location":"module/geo/#elapid.geo.read_raster_from_polygon","text":"Read valid pixel values from all locations inside a polygon Uses the polygon as a mask in addition to the existing raster mask Parameters: Name Type Description Default src DatasetReader an open rasterio dataset to read from required poly Union[shapely.geometry.polygon.Polygon, shapely.geometry.multipolygon.MultiPolygon] a shapely Polygon or MultiPolygon required Returns: Type Description MaskedArray masked array of shape (nbands, nrows, ncols) Source code in elapid/geo.py def read_raster_from_polygon ( src : rio . DatasetReader , poly : Union [ Polygon , MultiPolygon ]) -> np . ma . MaskedArray : \"\"\"Read valid pixel values from all locations inside a polygon Uses the polygon as a mask in addition to the existing raster mask Args: src: an open rasterio dataset to read from poly: a shapely Polygon or MultiPolygon Returns: masked array of shape (nbands, nrows, ncols) \"\"\" # get the read parameters window = rio . windows . from_bounds ( * poly . bounds , src . transform ) transform = rio . windows . transform ( window , src . transform ) # get the data data = src . read ( window = window , masked = True , boundless = True ) bands , rows , cols = data . shape poly_mask = geometry_mask ([ poly ], transform = transform , out_shape = ( rows , cols )) # update the mask data [:, poly_mask ] = np . ma . masked return data","title":"read_raster_from_polygon()"},{"location":"module/geo/#elapid.geo.sample_bias_file","text":"Creates a semi-random geographic sampling of points weighted towards biased areas. Parameters: Name Type Description Default raster_path str raster bias file path to sample from. pixel values can be in arbitrary range, but must be odered low -> high probability required count int total number of samples to generate required ignore_mask bool sample from the full extent of the raster instead of unmasked areas only False Returns: Type Description points Point geometry geoseries Source code in elapid/geo.py def sample_bias_file ( raster_path : str , count : int , ignore_mask : bool = False ) -> gpd . GeoSeries : \"\"\"Creates a semi-random geographic sampling of points weighted towards biased areas. Args: raster_path: raster bias file path to sample from. pixel values can be in arbitrary range, but must be odered low -> high probability count: total number of samples to generate ignore_mask: sample from the full extent of the raster instead of unmasked areas only Returns: points: Point geometry geoseries \"\"\" with rio . open ( raster_path ) as src : if src . nodata is None or ignore_mask : data = src . read ( 1 ) rows , cols = np . where ( data ) values = data . flatten () probabilities = ( values - values . min ()) / ( values - values . min ()) . sum () samples = np . random . choice ( len ( rows ), size = count , p = probabilities ) else : data = src . read ( 1 , masked = True ) rows , cols = np . where ( ~ data . mask ) values = data [ rows , cols ] probabilities = ( values - values . min ()) / ( values - values . min ()) . sum () samples = np . random . choice ( len ( rows ), size = count , p = probabilities ) xy = np . zeros (( count , 2 )) for i , sample in enumerate ( samples ): xy [ i ] = src . xy ( rows [ sample ], cols [ sample ]) points = xy_to_geoseries ( xy [:, 0 ], xy [:, 1 ], crs = src . crs ) return points","title":"sample_bias_file()"},{"location":"module/geo/#elapid.geo.sample_geoseries","text":"Creates random geographic point samples inside a polygon/multipolygon Parameters: Name Type Description Default geoseries GeoSeries geometry dataset (e.g. gdf['geometry'] ) with polygons/multipolygons required count int number of samples to generate required overestimate float scaler to generate extra samples to toss points outside of the polygon/inside it's bounds 2 Returns: Type Description points Point geometry geoseries Source code in elapid/geo.py def sample_geoseries ( geoseries : gpd . GeoSeries , count : int , overestimate : float = 2 ) -> gpd . GeoSeries : \"\"\"Creates random geographic point samples inside a polygon/multipolygon Args: geoseries: geometry dataset (e.g. `gdf['geometry']`) with polygons/multipolygons count: number of samples to generate overestimate: scaler to generate extra samples to toss points outside of the polygon/inside it's bounds Returns: points: Point geometry geoseries \"\"\" if type ( geoseries ) is gpd . GeoDataFrame : geoseries = geoseries . geometry polygon = geoseries . unary_union xmin , ymin , xmax , ymax = polygon . bounds ratio = polygon . area / polygon . envelope . area samples = np . random . uniform (( xmin , ymin ), ( xmax , ymax ), ( int ( count / ratio * overestimate ), 2 )) multipoint = MultiPoint ( samples ) multipoint = multipoint . intersection ( polygon ) sample_array = np . zeros (( len ( multipoint . geoms ), 2 )) for idx , point in enumerate ( multipoint . geoms ): sample_array [ idx ] = ( point . x , point . y ) xy = sample_array [ np . random . choice ( len ( sample_array ), count )] points = xy_to_geoseries ( xy [:, 0 ], xy [:, 1 ], crs = geoseries . crs ) return points","title":"sample_geoseries()"},{"location":"module/geo/#elapid.geo.sample_raster","text":"Creates a random geographic sampling of points based on a raster's extent. Selects from unmasked locations if the rasters nodata value is set. Parameters: Name Type Description Default raster_path str raster file path to sample locations from required count int number of samples to generate required ignore_mask bool sample from the full extent of the raster instead of unmasked areas only False Returns: Type Description points Point geometry geoseries Source code in elapid/geo.py def sample_raster ( raster_path : str , count : int , ignore_mask : bool = False ) -> gpd . GeoSeries : \"\"\"Creates a random geographic sampling of points based on a raster's extent. Selects from unmasked locations if the rasters nodata value is set. Args: raster_path: raster file path to sample locations from count: number of samples to generate ignore_mask: sample from the full extent of the raster instead of unmasked areas only Returns: points: Point geometry geoseries \"\"\" # handle masked vs unmasked data differently with rio . open ( raster_path ) as src : if src . nodata is None or ignore_mask : xmin , ymin , xmax , ymax = src . bounds xy = np . random . uniform (( xmin , ymin ), ( xmax , ymax ), ( count , 2 )) else : masked = src . read_masks ( 1 ) rows , cols = np . where ( masked == 255 ) samples = np . random . randint ( 0 , len ( rows ), count ) xy = np . zeros (( count , 2 )) for i , sample in enumerate ( samples ): xy [ i ] = src . xy ( rows [ sample ], cols [ sample ]) points = xy_to_geoseries ( xy [:, 0 ], xy [:, 1 ], crs = src . crs ) return points","title":"sample_raster()"},{"location":"module/geo/#elapid.geo.sample_vector","text":"Creates a random geographic sampling of points inside of a polygon/multipolygon type vector file. Parameters: Name Type Description Default vector_path str path to a vector file (shp, geojson, etc) required count int number of samples to generate required overestimate float scaler to generate extra samples to toss points outside of the polygon/inside it's bounds 2 Returns: Type Description points Point geometry geoseries Source code in elapid/geo.py def sample_vector ( vector_path : str , count : int , overestimate : float = 2 ) -> gpd . GeoSeries : \"\"\"Creates a random geographic sampling of points inside of a polygon/multipolygon type vector file. Args: vector_path: path to a vector file (shp, geojson, etc) count: number of samples to generate overestimate: scaler to generate extra samples to toss points outside of the polygon/inside it's bounds Returns: points: Point geometry geoseries \"\"\" gdf = gpd . read_file ( vector_path ) return sample_geoseries ( gdf . geometry , count , overestimate = overestimate )","title":"sample_vector()"},{"location":"module/geo/#elapid.geo.string_to_crs","text":"Converts a crs/projection string to a pyproj-readable CRS object Parameters: Name Type Description Default string str a crs/projection string. required crs_type the type of crs/projection string, in [\"wkt\", \"proj4\", \"epsg\", \"string\"]. required Returns: Type Description crs the coordinate reference system Source code in elapid/geo.py def string_to_crs ( string : str ) -> rio . crs . CRS : \"\"\"Converts a crs/projection string to a pyproj-readable CRS object Args: string: a crs/projection string. crs_type: the type of crs/projection string, in [\"wkt\", \"proj4\", \"epsg\", \"string\"]. Returns: crs: the coordinate reference system \"\"\" crs_type = parse_crs_string ( string ) if crs_type == \"epsg\" : auth , code = string . split ( \":\" ) crs = rio . crs . CRS . from_epsg ( int ( code )) elif crs_type == \"proj4\" : crs = rio . crs . CRS . from_proj4 ( string ) elif crs_type == \"wkt\" : crs = rio . crs . CRS . from_wkt ( string ) else : crs = rio . crs . CRS . from_string ( string ) return crs","title":"string_to_crs()"},{"location":"module/geo/#elapid.geo.validate_gpd","text":"Validates whether an input is a GeoDataFrame or a GeoSeries. Parameters: Name Type Description Default geo Union[geopandas.geoseries.GeoSeries, geopandas.geodataframe.GeoDataFrame] an input variable that should be in GeoPandas format required Source code in elapid/geo.py def validate_gpd ( geo : Union [ gpd . GeoSeries , gpd . GeoDataFrame ]) -> None : \"\"\"Validates whether an input is a GeoDataFrame or a GeoSeries. Args: geo: an input variable that should be in GeoPandas format Raises: TypeError if geo is not in GeoPandas format \"\"\" if not ( isinstance ( geo , gpd . GeoDataFrame ) or isinstance ( geo , gpd . GeoSeries )): raise TypeError ( \"Input must be a GeoDataFrame or GeoSeries\" )","title":"validate_gpd()"},{"location":"module/geo/#elapid.geo.validate_polygons","text":"Iterate over a geoseries to find rows with invalid geometry types. Parameters: Name Type Description Default geometry Union[geopandas.geoseries.GeoSeries, geopandas.geodataframe.GeoDataFrame] a GeoSeries or GeoDataFrame with polygon geometries required Returns: Type Description Index an index of rows with valid polygon types Source code in elapid/geo.py def validate_polygons ( geometry : Union [ gpd . GeoSeries , gpd . GeoDataFrame ]) -> pd . Index : \"\"\"Iterate over a geoseries to find rows with invalid geometry types. Args: geometry: a GeoSeries or GeoDataFrame with polygon geometries Returns: an index of rows with valid polygon types \"\"\" if isinstance ( geometry , gpd . GeoDataFrame ): geometry = geometry . geometry index = [] for idx , geom in enumerate ( geometry ): if not ( isinstance ( geom , Polygon ) or isinstance ( geom , MultiPolygon )): index . append ( idx ) if len ( index ) > 0 : warnings . warn ( f \"Input geometry had { len ( index ) } invalid geometries. \" \"These will be dropped, but with the original index preserved.\" ) geometry . drop ( index = index , inplace = True ) return geometry . index","title":"validate_polygons()"},{"location":"module/geo/#elapid.geo.xy_to_geoseries","text":"Converts x/y data into a geopandas geoseries. Parameters: Name Type Description Default x Union[float, list, numpy.ndarray] 1-D array-like of x location values required y Union[float, list, numpy.ndarray] 1-D array-like of y location values required crs Union[pyproj.crs.crs.CRS, str] coordinate reference system. accepts pyproj.CRS / rio.crs.CRS objects or anything allowed by pyproj.CRS.from_user_input() 'epsg:4236' Returns: Type Description gs Point geometry geoseries Source code in elapid/geo.py def xy_to_geoseries ( x : Union [ float , list , np . ndarray ], y : Union [ float , list , np . ndarray ], crs : CRSType = \"epsg:4236\" ) -> gpd . GeoSeries : \"\"\"Converts x/y data into a geopandas geoseries. Args: x: 1-D array-like of x location values y: 1-D array-like of y location values crs: coordinate reference system. accepts pyproj.CRS / rio.crs.CRS objects or anything allowed by pyproj.CRS.from_user_input() Returns: gs: Point geometry geoseries \"\"\" # handle single x/y location values x = to_iterable ( x ) y = to_iterable ( y ) points = [ Point ( x , y ) for x , y in zip ( x , y )] gs = gpd . GeoSeries ( points , crs = crs ) return gs","title":"xy_to_geoseries()"},{"location":"module/geo/#elapid.geo.zonal_stats","text":"Compute raster summary stats for each polygon in a GeoSeries or GeoDataFrame. Parameters: Name Type Description Default polygons Union[geopandas.geoseries.GeoSeries, geopandas.geodataframe.GeoDataFrame] GeoSeries or GeoDataFrame with polygon geometries. required raster_paths list list of paths to rasters to summarize required labels list band labels. must match the total number of bands for all raster_paths. None all_touched bool include all pixels that touch a polygon. set to False to only include pixels whose centers intersect the polygon True mean, min, max, count, sum, stdv, skew, kurtosis, mode set to True to compute these stats required all bool compute all of the above stats False percentiles list list of 0-100 percentile ranges to compute [] Returns: Type Description GeoDataFrame GeoDataFrame with zonal stats for each raster band in new columns. If polygons is a GeoDataFrame, the zonal stats columns are appended to the original input. Source code in elapid/geo.py def zonal_stats ( polygons : Union [ gpd . GeoSeries , gpd . GeoDataFrame ], raster_paths : list , labels : list = None , all_touched : bool = True , mean : bool = True , stdv : bool = True , min : bool = False , max : bool = False , count : bool = False , sum : bool = False , skew : bool = False , kurtosis : bool = False , mode : bool = False , all : bool = False , percentiles : list = [], ) -> gpd . GeoDataFrame : \"\"\"Compute raster summary stats for each polygon in a GeoSeries or GeoDataFrame. Args: polygons: GeoSeries or GeoDataFrame with polygon geometries. raster_paths: list of paths to rasters to summarize labels: band labels. must match the total number of bands for all raster_paths. all_touched: include all pixels that touch a polygon. set to False to only include pixels whose centers intersect the polygon mean, min, max, count, sum, stdv, skew, kurtosis, mode: set to True to compute these stats all: compute all of the above stats percentiles: list of 0-100 percentile ranges to compute Returns: GeoDataFrame with zonal stats for each raster band in new columns. If `polygons` is a GeoDataFrame, the zonal stats columns are appended to the original input. \"\"\" # format the input geometries validate_gpd ( polygons ) valid_idx = validate_polygons ( polygons ) polygons = polygons . iloc [ valid_idx ] is_df = isinstance ( polygons , gpd . GeoDataFrame ) polys = polygons . geometry if is_df else polygons # format the input labels raster_paths = to_iterable ( raster_paths ) labels = format_band_labels ( raster_paths , labels ) # get the bands and indexes for each covariate raster nbands , band_idx = get_raster_band_indexes ( raster_paths ) # get the stats methods to compute for each feature stats_methods = get_raster_stats_methods ( mean = mean , min = min , max = max , count = count , sum = sum , stdv = stdv , skew = skew , kurtosis = kurtosis , mode = mode , percentiles = percentiles , all = all , ) # create dataframes for each raster and concatenate at the end raster_dfs = [] # run zonal stats raster-by-raster (instead of iterating first over geometries) for r , raster in tqdm ( enumerate ( raster_paths ), total = len ( raster_paths ), desc = \"Raster\" , ** tqdm_opts ): # format the band labels band_labels = labels [ band_idx [ r ] : band_idx [ r + 1 ]] n_raster_bands = band_idx [ r + 1 ] - band_idx [ r ] stats_labels = [] for method in stats_methods : stats_labels . append ([ f \" { band } _ { method . name } \" for band in band_labels ]) # open the raster for reading with rio . open ( raster , \"r\" ) as src : # reproject the polygon data as necessary if not crs_match ( polys . crs , src . crs ): polys = polys . to_crs ( src . crs ) # create output arrays to store each stat's output stats_arrays = [] for method in stats_methods : dtype = method . dtype or src . dtypes [ 0 ] stats_arrays . append ( np . zeros (( len ( polys ), n_raster_bands ), dtype = dtype )) # iterate over each geometry to read data and compute stats for p , poly in tqdm ( enumerate ( polys ), total = len ( polys ), desc = \"Polygon\" , leave = False , ** tqdm_opts ): data = read_raster_from_polygon ( src , poly ) for method , array in zip ( stats_methods , stats_arrays ): array [ p , :] = method . reduce ( data ) # convert each stat's array into dataframes and merge them together stats_dfs = [ pd . DataFrame ( array , columns = labels ) for array , labels in zip ( stats_arrays , stats_labels )] raster_dfs . append ( pd . concat ( stats_dfs , axis = 1 )) # merge the outputs from each raster if is_df : merged = gpd . GeoDataFrame ( pd . concat ([ polygons ] + raster_dfs , axis = 1 ), crs = polygons . crs ) else : merged = gpd . GeoDataFrame ( pd . concat ( raster_dfs , axis = 1 ), geometry = polygons , crs = polygons . crs ) return merged","title":"zonal_stats()"},{"location":"module/models/","text":"elapid.models \u00b6 Classes for training species distribution models. MaxentModel ( BaseEstimator ) \u00b6 Model estimator for Maxent-style species distribution models. Source code in elapid/models.py class MaxentModel ( BaseEstimator ): \"\"\"Model estimator for Maxent-style species distribution models.\"\"\" feature_types_ : list = MaxentConfig . feature_types tau_ : float = MaxentConfig . tau clamp_ : bool = MaxentConfig . clamp convergence_tolerance_ : float = MaxentConfig . tolerance beta_multiplier_ : float = MaxentConfig . beta_multiplier beta_hinge_ : float = MaxentConfig . beta_hinge beta_lqp_ : float = MaxentConfig . beta_lqp beta_threshold_ : float = MaxentConfig . beta_threshold beta_categorical_ : float = MaxentConfig . beta_categorical n_hinge_features_ : int = MaxentConfig . n_hinge_features n_threshold_features_ : int = MaxentConfig . n_threshold_features scorer_ : str = MaxentConfig . scorer use_lambdas_ : str = MaxentConfig . use_lambdas n_lambdas_ : str = MaxentConfig . n_lambdas initialized_ : bool = False beta_scores_ : np . array = None entropy_ : float = 0.0 alpha_ : float = 0.0 estimator : BaseEstimator = None transformer : _features . MaxentFeatureTransformer = None weights_ : np . ndarray = None regularization_ : np . ndarray = None lambdas_ : np . ndarray = None weight_strategy_ : Union [ str , float ] = None n_cpus_ = n_cpus def __init__ ( self , feature_types : Union [ list , str ] = MaxentConfig . feature_types , tau : float = MaxentConfig . tau , clamp : bool = MaxentConfig . clamp , scorer : str = MaxentConfig . scorer , beta_multiplier : float = MaxentConfig . beta_multiplier , beta_lqp : float = MaxentConfig . beta_lqp , beta_hinge : float = MaxentConfig . beta_hinge , beta_threshold : float = MaxentConfig . beta_lqp , beta_categorical : float = MaxentConfig . beta_categorical , n_hinge_features : int = MaxentConfig . n_hinge_features , n_threshold_features : int = MaxentConfig . n_threshold_features , convergence_tolerance : float = MaxentConfig . tolerance , use_lambdas : str = MaxentConfig . use_lambdas , n_lambdas : int = MaxentConfig . n_lambdas , weights : Union [ str , float ] = MaxentConfig . weights , n_cpus : int = n_cpus , ): \"\"\"Create a maxent model object. Args: feature_types: maxent feature types to fit. must be in string \"lqphta\" or list [\"linear\", \"quadratic\", \"product\", \"hinge\", \"threshold\", \"auto\"] tau: maxent prevalence value for scaling logistic output clamp: set features to min/max range from training during prediction scorer: sklearn scoring function for model training beta_multiplier: scaler for all regularization parameters. higher values drop more coeffiecients beta_lqp: linear, quadratic and product feature regularization scaler beta_hinge: hinge feature regularization scaler beta_threshold: threshold feature regularization scaler beta_categorical: categorical feature regularization scaler convergence_tolerance: model convergence tolerance level use_lambdas: guide for which model lambdas to select (either \"best\" or \"last\") n_lambdas: number of lamba values to fit models with weights: strategy for weighting presence samples. pass \"balance\" to compute the ratio based on sample frequency or pass a float for the presence:background weight ratio n_cpus: threads to use during model training \"\"\" self . feature_types_ = validate_feature_types ( feature_types ) self . tau_ = tau self . clamp_ = clamp self . convergence_tolerance_ = convergence_tolerance self . beta_multiplier_ = beta_multiplier self . beta_hinge_ = beta_hinge self . beta_lqp_ = beta_lqp self . beta_threshold_ = beta_threshold self . beta_categorical_ = beta_categorical self . n_hinge_features_ = n_hinge_features self . n_threshold_features_ = n_threshold_features self . n_cpus_ = n_cpus self . scorer_ = scorer self . use_lambdas_ = use_lambdas self . n_lambdas_ = n_lambdas self . weight_strategy_ = weights def fit ( self , x : ArrayLike , y : ArrayLike , categorical : List [ int ] = None , labels : list = None , is_features : bool = False , feature_labels : list = None , ) -> None : \"\"\"Trains a maxent model using a set of covariates and presence/background points. Args: x: array-like of shape (n_samples, n_features) with covariate data y: array-like of shape (n_samples,) with binary presence/background (1/0) values categorical: indices for which columns are categorical labels: covariate labels. ignored if x is a pandas DataFrame is_features: specify that x data has been transformed from covariates to features feature_labels: list of length n_features, with labels identifying each column's feature type with options [\"linear\", \"quadratic\", \"product\", \"threshold\", \"hinge\", \"categorical\"] must be set if `is_features=True` \"\"\" # fir the feature transformer if is_features : features = x assert feature_labels is not None , \"feature_labels must be set if is_features=True\" else : self . transformer = _features . MaxentFeatureTransformer ( feature_types = self . feature_types_ , clamp = self . clamp_ , n_hinge_features = self . n_hinge_features_ , n_threshold_features = self . n_threshold_features_ , ) features = self . transformer . fit_transform ( x , categorical = categorical , labels = labels ) feature_labels = self . transformer . feature_names_ # compute sample weights if self . weight_strategy_ == \"balance\" : pbr = len ( y ) / y . sum () else : pbr = self . weight_strategy_ self . weights_ = _features . compute_weights ( y , pbr = pbr ) # set feature regularization parameters self . regularization_ = _features . compute_regularization ( y , features , feature_labels = feature_labels , beta_multiplier = self . beta_multiplier_ , beta_threshold = self . beta_threshold_ , beta_hinge = self . beta_hinge_ , beta_categorical = self . beta_categorical_ , ) # get model lambda scores to initialize the glm self . lambdas_ = _features . compute_lambdas ( y , self . weights_ , self . regularization_ ) # model fitting self . initialize_model ( lambdas = self . lambdas_ ) self . estimator . fit ( features , y , sample_weight = self . weights_ , relative_penalties = self . regularization_ , ) # get the beta values based on which lamba selection method to use if self . use_lambdas_ == \"last\" : self . beta_scores_ = self . estimator . coef_path_ [ 0 , :, - 1 ] elif self . use_lambdas_ == \"best\" : self . beta_scores_ = self . estimator . coef_path_ [ 0 , :, self . estimator . lambda_max_inx_ ] # apply maxent-specific transformations raw = self . predict ( features [ y == 0 ], transform = \"raw\" , is_features = True ) # alpha is a normalizing constant that ensures that f1(z) integrates (sums) to 1 self . alpha_ = maxent_alpha ( raw ) # the distance from f(z) is the relative entropy of f1(z) WRT f(z) self . entropy_ = maxent_entropy ( raw ) def predict ( self , x : ArrayLike , transform : str = \"cloglog\" , is_features : bool = False ) -> ArrayLike : \"\"\"Applies a model to a set of covariates or features. Requires that a model has been fit. Args: x: array-like of shape (n_samples, n_features) with covariate data transform: maxent model transformation type. select from [\"raw\", \"logistic\", \"cloglog\"]. is_features: flag that x data has already been transformed from covariates to features Returns: predictions: array-like of shape (n_samples,) with model predictions \"\"\" assert self . initialized_ , \"Model must be fit first\" # feature transformations features = x if is_features else self . transformer . transform ( x ) # apply the model engma = np . matmul ( features , self . beta_scores_ ) + self . alpha_ # scale based on the transform type if transform == \"raw\" : return np . exp ( engma ) elif transform == \"logistic\" : # below is R's maxnet (tau-free) logistic formulation # return 1 / (1 + np.exp(-self.entropy_ - raw)) # use the java formulation instead logratio = np . exp ( engma ) * np . exp ( self . entropy_ ) return ( self . tau_ * logratio ) / (( 1 - self . tau_ ) + ( self . tau_ * logratio )) elif transform == \"cloglog\" : # below is R's maxent cloglog formula # return 1 - np.exp(0 - np.exp(self.entropy_ - raw)) # use java again return 1 - np . exp ( - np . exp ( engma ) * np . exp ( self . entropy_ )) def fit_predict ( self , x : ArrayLike , y : ArrayLike , categorical : list = None , labels : list = None , transform : str = \"cloglog\" , is_features : bool = False , feature_labels : list = None , ) -> ArrayLike : \"\"\"Trains and applies a model to x/y data. Args: x: array-like of shape (n_samples, n_features) with covariate data y: array-like of shape (n_samples,) with binary presence/background (1/0) values categorical: column indices indicating which columns are categorical labels: Covariate labels. Ignored if x is a pandas DataFrame transform: maxent model transformation type. select from [\"raw\", \"logistic\", \"cloglog\"]. is_features: specify that x data has already been transformed from covariates to features feature_labels: list of length n_features, with labels identifying each column's feature type with options [\"linear\", \"quadratic\", \"product\", \"threshold\", \"hinge\", \"categorical\"] must be set if `is_features=True` Returns: predictions: Array-like of shape (n_samples,) with model predictions \"\"\" self . fit ( x , y , categorical = categorical , labels = labels , is_features = is_features , feature_labels = feature_labels ) predictions = self . predict ( x , transform = transform , is_features = is_features ) return predictions def initialize_model ( self , lambdas : np . array , alpha : float = 1 , standardize : bool = False , fit_intercept : bool = True , ) -> None : \"\"\"Creates the Logistic Regression with elastic net penalty model object. Args: lambdas: array of model lambda values. get from elapid.features.compute_lambdas() alpha: elasticnet mixing parameter. alpha=1 for lasso, alpha=0 for ridge standardize: specify coefficient normalization fit_intercept: include an intercept parameter Returns: None. updates the self.estimator with an sklearn-style model estimator \"\"\" self . estimator = LogitNet ( alpha = alpha , lambda_path = lambdas , standardize = standardize , fit_intercept = fit_intercept , scoring = self . scorer_ , n_jobs = self . n_cpus_ , tol = self . convergence_tolerance_ , ) self . initialized_ = True __init__ ( self , feature_types = [ 'linear' , 'hinge' , 'product' ], tau = 0.5 , clamp = True , scorer = 'roc_auc' , beta_multiplier = 1.0 , beta_lqp = 1.0 , beta_hinge = 1.0 , beta_threshold = 1.0 , beta_categorical = 1.0 , n_hinge_features = 10 , n_threshold_features = 10 , convergence_tolerance = 1e-07 , use_lambdas = 'best' , n_lambdas = 100 , weights = 'balance' , n_cpus = 4 ) special \u00b6 Create a maxent model object. Parameters: Name Type Description Default feature_types Union[list, str] maxent feature types to fit. must be in string \"lqphta\" or list [\"linear\", \"quadratic\", \"product\", \"hinge\", \"threshold\", \"auto\"] ['linear', 'hinge', 'product'] tau float maxent prevalence value for scaling logistic output 0.5 clamp bool set features to min/max range from training during prediction True scorer str sklearn scoring function for model training 'roc_auc' beta_multiplier float scaler for all regularization parameters. higher values drop more coeffiecients 1.0 beta_lqp float linear, quadratic and product feature regularization scaler 1.0 beta_hinge float hinge feature regularization scaler 1.0 beta_threshold float threshold feature regularization scaler 1.0 beta_categorical float categorical feature regularization scaler 1.0 convergence_tolerance float model convergence tolerance level 1e-07 use_lambdas str guide for which model lambdas to select (either \"best\" or \"last\") 'best' n_lambdas int number of lamba values to fit models with 100 weights Union[str, float] strategy for weighting presence samples. pass \"balance\" to compute the ratio based on sample frequency or pass a float for the presence:background weight ratio 'balance' n_cpus int threads to use during model training 4 Source code in elapid/models.py def __init__ ( self , feature_types : Union [ list , str ] = MaxentConfig . feature_types , tau : float = MaxentConfig . tau , clamp : bool = MaxentConfig . clamp , scorer : str = MaxentConfig . scorer , beta_multiplier : float = MaxentConfig . beta_multiplier , beta_lqp : float = MaxentConfig . beta_lqp , beta_hinge : float = MaxentConfig . beta_hinge , beta_threshold : float = MaxentConfig . beta_lqp , beta_categorical : float = MaxentConfig . beta_categorical , n_hinge_features : int = MaxentConfig . n_hinge_features , n_threshold_features : int = MaxentConfig . n_threshold_features , convergence_tolerance : float = MaxentConfig . tolerance , use_lambdas : str = MaxentConfig . use_lambdas , n_lambdas : int = MaxentConfig . n_lambdas , weights : Union [ str , float ] = MaxentConfig . weights , n_cpus : int = n_cpus , ): \"\"\"Create a maxent model object. Args: feature_types: maxent feature types to fit. must be in string \"lqphta\" or list [\"linear\", \"quadratic\", \"product\", \"hinge\", \"threshold\", \"auto\"] tau: maxent prevalence value for scaling logistic output clamp: set features to min/max range from training during prediction scorer: sklearn scoring function for model training beta_multiplier: scaler for all regularization parameters. higher values drop more coeffiecients beta_lqp: linear, quadratic and product feature regularization scaler beta_hinge: hinge feature regularization scaler beta_threshold: threshold feature regularization scaler beta_categorical: categorical feature regularization scaler convergence_tolerance: model convergence tolerance level use_lambdas: guide for which model lambdas to select (either \"best\" or \"last\") n_lambdas: number of lamba values to fit models with weights: strategy for weighting presence samples. pass \"balance\" to compute the ratio based on sample frequency or pass a float for the presence:background weight ratio n_cpus: threads to use during model training \"\"\" self . feature_types_ = validate_feature_types ( feature_types ) self . tau_ = tau self . clamp_ = clamp self . convergence_tolerance_ = convergence_tolerance self . beta_multiplier_ = beta_multiplier self . beta_hinge_ = beta_hinge self . beta_lqp_ = beta_lqp self . beta_threshold_ = beta_threshold self . beta_categorical_ = beta_categorical self . n_hinge_features_ = n_hinge_features self . n_threshold_features_ = n_threshold_features self . n_cpus_ = n_cpus self . scorer_ = scorer self . use_lambdas_ = use_lambdas self . n_lambdas_ = n_lambdas self . weight_strategy_ = weights fit ( self , x , y , categorical = None , labels = None , is_features = False , feature_labels = None ) \u00b6 Trains a maxent model using a set of covariates and presence/background points. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) with covariate data required y Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples,) with binary presence/background (1/0) values required categorical List[int] indices for which columns are categorical None labels list covariate labels. ignored if x is a pandas DataFrame None is_features bool specify that x data has been transformed from covariates to features False feature_labels list list of length n_features, with labels identifying each column's feature type with options [\"linear\", \"quadratic\", \"product\", \"threshold\", \"hinge\", \"categorical\"] must be set if is_features=True None Source code in elapid/models.py def fit ( self , x : ArrayLike , y : ArrayLike , categorical : List [ int ] = None , labels : list = None , is_features : bool = False , feature_labels : list = None , ) -> None : \"\"\"Trains a maxent model using a set of covariates and presence/background points. Args: x: array-like of shape (n_samples, n_features) with covariate data y: array-like of shape (n_samples,) with binary presence/background (1/0) values categorical: indices for which columns are categorical labels: covariate labels. ignored if x is a pandas DataFrame is_features: specify that x data has been transformed from covariates to features feature_labels: list of length n_features, with labels identifying each column's feature type with options [\"linear\", \"quadratic\", \"product\", \"threshold\", \"hinge\", \"categorical\"] must be set if `is_features=True` \"\"\" # fir the feature transformer if is_features : features = x assert feature_labels is not None , \"feature_labels must be set if is_features=True\" else : self . transformer = _features . MaxentFeatureTransformer ( feature_types = self . feature_types_ , clamp = self . clamp_ , n_hinge_features = self . n_hinge_features_ , n_threshold_features = self . n_threshold_features_ , ) features = self . transformer . fit_transform ( x , categorical = categorical , labels = labels ) feature_labels = self . transformer . feature_names_ # compute sample weights if self . weight_strategy_ == \"balance\" : pbr = len ( y ) / y . sum () else : pbr = self . weight_strategy_ self . weights_ = _features . compute_weights ( y , pbr = pbr ) # set feature regularization parameters self . regularization_ = _features . compute_regularization ( y , features , feature_labels = feature_labels , beta_multiplier = self . beta_multiplier_ , beta_threshold = self . beta_threshold_ , beta_hinge = self . beta_hinge_ , beta_categorical = self . beta_categorical_ , ) # get model lambda scores to initialize the glm self . lambdas_ = _features . compute_lambdas ( y , self . weights_ , self . regularization_ ) # model fitting self . initialize_model ( lambdas = self . lambdas_ ) self . estimator . fit ( features , y , sample_weight = self . weights_ , relative_penalties = self . regularization_ , ) # get the beta values based on which lamba selection method to use if self . use_lambdas_ == \"last\" : self . beta_scores_ = self . estimator . coef_path_ [ 0 , :, - 1 ] elif self . use_lambdas_ == \"best\" : self . beta_scores_ = self . estimator . coef_path_ [ 0 , :, self . estimator . lambda_max_inx_ ] # apply maxent-specific transformations raw = self . predict ( features [ y == 0 ], transform = \"raw\" , is_features = True ) # alpha is a normalizing constant that ensures that f1(z) integrates (sums) to 1 self . alpha_ = maxent_alpha ( raw ) # the distance from f(z) is the relative entropy of f1(z) WRT f(z) self . entropy_ = maxent_entropy ( raw ) fit_predict ( self , x , y , categorical = None , labels = None , transform = 'cloglog' , is_features = False , feature_labels = None ) \u00b6 Trains and applies a model to x/y data. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) with covariate data required y Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples,) with binary presence/background (1/0) values required categorical list column indices indicating which columns are categorical None labels list Covariate labels. Ignored if x is a pandas DataFrame None transform str maxent model transformation type. select from [\"raw\", \"logistic\", \"cloglog\"]. 'cloglog' is_features bool specify that x data has already been transformed from covariates to features False feature_labels list list of length n_features, with labels identifying each column's feature type with options [\"linear\", \"quadratic\", \"product\", \"threshold\", \"hinge\", \"categorical\"] must be set if is_features=True None Returns: Type Description predictions Array-like of shape (n_samples,) with model predictions Source code in elapid/models.py def fit_predict ( self , x : ArrayLike , y : ArrayLike , categorical : list = None , labels : list = None , transform : str = \"cloglog\" , is_features : bool = False , feature_labels : list = None , ) -> ArrayLike : \"\"\"Trains and applies a model to x/y data. Args: x: array-like of shape (n_samples, n_features) with covariate data y: array-like of shape (n_samples,) with binary presence/background (1/0) values categorical: column indices indicating which columns are categorical labels: Covariate labels. Ignored if x is a pandas DataFrame transform: maxent model transformation type. select from [\"raw\", \"logistic\", \"cloglog\"]. is_features: specify that x data has already been transformed from covariates to features feature_labels: list of length n_features, with labels identifying each column's feature type with options [\"linear\", \"quadratic\", \"product\", \"threshold\", \"hinge\", \"categorical\"] must be set if `is_features=True` Returns: predictions: Array-like of shape (n_samples,) with model predictions \"\"\" self . fit ( x , y , categorical = categorical , labels = labels , is_features = is_features , feature_labels = feature_labels ) predictions = self . predict ( x , transform = transform , is_features = is_features ) return predictions initialize_model ( self , lambdas , alpha = 1 , standardize = False , fit_intercept = True ) \u00b6 Creates the Logistic Regression with elastic net penalty model object. Parameters: Name Type Description Default lambdas <built-in function array> array of model lambda values. get from elapid.features.compute_lambdas() required alpha float elasticnet mixing parameter. alpha=1 for lasso, alpha=0 for ridge 1 standardize bool specify coefficient normalization False fit_intercept bool include an intercept parameter True Returns: Type Description None None. updates the self.estimator with an sklearn-style model estimator Source code in elapid/models.py def initialize_model ( self , lambdas : np . array , alpha : float = 1 , standardize : bool = False , fit_intercept : bool = True , ) -> None : \"\"\"Creates the Logistic Regression with elastic net penalty model object. Args: lambdas: array of model lambda values. get from elapid.features.compute_lambdas() alpha: elasticnet mixing parameter. alpha=1 for lasso, alpha=0 for ridge standardize: specify coefficient normalization fit_intercept: include an intercept parameter Returns: None. updates the self.estimator with an sklearn-style model estimator \"\"\" self . estimator = LogitNet ( alpha = alpha , lambda_path = lambdas , standardize = standardize , fit_intercept = fit_intercept , scoring = self . scorer_ , n_jobs = self . n_cpus_ , tol = self . convergence_tolerance_ , ) self . initialized_ = True predict ( self , x , transform = 'cloglog' , is_features = False ) \u00b6 Applies a model to a set of covariates or features. Requires that a model has been fit. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) with covariate data required transform str maxent model transformation type. select from [\"raw\", \"logistic\", \"cloglog\"]. 'cloglog' is_features bool flag that x data has already been transformed from covariates to features False Returns: Type Description predictions array-like of shape (n_samples,) with model predictions Source code in elapid/models.py def predict ( self , x : ArrayLike , transform : str = \"cloglog\" , is_features : bool = False ) -> ArrayLike : \"\"\"Applies a model to a set of covariates or features. Requires that a model has been fit. Args: x: array-like of shape (n_samples, n_features) with covariate data transform: maxent model transformation type. select from [\"raw\", \"logistic\", \"cloglog\"]. is_features: flag that x data has already been transformed from covariates to features Returns: predictions: array-like of shape (n_samples,) with model predictions \"\"\" assert self . initialized_ , \"Model must be fit first\" # feature transformations features = x if is_features else self . transformer . transform ( x ) # apply the model engma = np . matmul ( features , self . beta_scores_ ) + self . alpha_ # scale based on the transform type if transform == \"raw\" : return np . exp ( engma ) elif transform == \"logistic\" : # below is R's maxnet (tau-free) logistic formulation # return 1 / (1 + np.exp(-self.entropy_ - raw)) # use the java formulation instead logratio = np . exp ( engma ) * np . exp ( self . entropy_ ) return ( self . tau_ * logratio ) / (( 1 - self . tau_ ) + ( self . tau_ * logratio )) elif transform == \"cloglog\" : # below is R's maxent cloglog formula # return 1 - np.exp(0 - np.exp(self.entropy_ - raw)) # use java again return 1 - np . exp ( - np . exp ( engma ) * np . exp ( self . entropy_ )) NicheEnvelopeModel ( BaseEstimator ) \u00b6 Model estimator for niche envelope-style models. Source code in elapid/models.py class NicheEnvelopeModel ( BaseEstimator ): \"\"\"Model estimator for niche envelope-style models.\"\"\" percentile_range_ : Tuple [ float , float ] = None feature_mins_ : np . ndarray = None feature_maxs_ : np . ndarray = None categorical_estimator : BaseEstimator = None categorical_ : list = None continuous_ : list = None categorical_pd_ : list = None continuous_pd_ : list = None in_categorical_ : np . ndarray = None def __init__ ( self , percentile_range : Tuple [ float , float ] = NicheEnvelopeConfig . percentile_range ): \"\"\"Create a niche envelope model estimator. Args: percentile_range: covariate values within this range are flagged as suitable habitat using a narrow range like [10, 90] drops more areas from suitability maps while [0, 100] creates an envelope around the full range of observed covariates at all y==1 locations. \"\"\" self . percentile_range_ = percentile_range self . categorical_estimator = _features . CategoricalTransformer () def _format_covariate_data ( self , x : ArrayLike ) -> Tuple [ np . array , np . array ]: \"\"\"Reads input x data and formats it to consistent array dtypes. Args: x: array-like of shape (n_samples, n_features) Returns: (continuous, categorical) tuple of ndarrays with continuous and categorical covariate data. \"\"\" if isinstance ( x , np . ndarray ): if self . categorical_ is None : con = x cat = None else : con = x [:, self . continuous_ ] cat = x [:, self . categorical_ ] elif isinstance ( x , pd . DataFrame ): con = x [ self . continuous_pd_ ] . to_numpy () if len ( self . categorical_pd_ ) > 0 : cat = x [ self . categorical_pd_ ] . to_numpy () else : cat = None else : raise TypeError ( f \"Unsupported x dtype: { type ( x ) } . Must be pd.DataFrame or np.array\" ) return con , cat def _format_labels_and_dtypes ( self , x : ArrayLike , categorical : list = None , labels : list = None ) -> None : \"\"\"Read input x data and lists of categorical data indices and band labels to format and store this info for later indexing. Args: s: array-like of shape (n_samples, n_features) categorical: indices indicating which x columns are categorical labels: covariate column labels. ignored if x is a pandas DataFrame \"\"\" if isinstance ( x , np . ndarray ): nrows , ncols = x . shape if categorical is None : continuous = list ( range ( ncols )) else : continuous = list ( set ( range ( ncols )) . difference ( set ( categorical ))) self . labels_ = labels or make_band_labels ( ncols ) self . categorical_ = categorical self . continuous_ = continuous elif isinstance ( x , pd . DataFrame ): x . drop ([ \"geometry\" ], axis = 1 , errors = \"ignore\" , inplace = True ) self . labels_ = labels or list ( x . columns ) # store both pandas and numpy indexing of these values self . continuous_pd_ = list ( x . select_dtypes ( exclude = \"category\" ) . columns ) self . categorical_pd_ = list ( x . select_dtypes ( include = \"category\" ) . columns ) all_columns = list ( x . columns ) self . continuous_ = [ all_columns . index ( item ) for item in self . continuous_pd_ if item in all_columns ] if len ( self . categorical_pd_ ) != 0 : self . categorical_ = [ all_columns . index ( item ) for item in self . categorical_pd_ if item in all_columns ] else : self . categorical_ = None def fit ( self , x : ArrayLike , y : ArrayLike , categorical : list = None , labels : list = None ) -> None : \"\"\"Fits a niche envelope model using a set of covariates and presence/background points. Args: x: array-like of shape (n_samples, n_features) with covariate data y: array-like of shape (n_samples,) with binary presence/background (1/0) values categorical: indices for which columns are categorical labels: covariate labels. ignored if x is a pandas DataFrame \"\"\" # format the input x data self . _format_labels_and_dtypes ( x , categorical = categorical , labels = labels ) con , cat = self . _format_covariate_data ( x ) # estimate the feature range of the continuous data self . feature_mins_ = np . percentile ( con [ y == 1 ], self . percentile_range_ [ 0 ], axis = 0 ) self . feature_maxs_ = np . percentile ( con [ y == 1 ], self . percentile_range_ [ 1 ], axis = 0 ) # one-hot encode the categorical data and label the classes with if cat is not None : ohe = self . categorical_estimator . fit_transform ( cat ) self . in_categorical_ = np . any ( ohe [ y == 1 ], axis = 0 ) def predict ( self , x : ArrayLike , overlay : str = \"average\" ) -> np . ndarray : \"\"\"Applies a model to a set of covariates or features. Requires that a model has been fit. Args: x: array-like of shape (n_samples, n_features) with covariate data overlay: niche envelope overlap type. select from [\"average\", \"intersection\", \"union\"] Returns: array-like of shape (n_samples,) with model predictions \"\"\" overlay = overlay . lower () overlay_options = [ \"average\" , \"intersection\" , \"union\" ] assert overlay in overlay_options , f \"overlay must be one of { ', ' . join ( overlay_options ) } \" # format the input data con , cat = self . _format_covariate_data ( x ) nrows , ncols = x . shape # any value within the transformed range is considered within the envelope in_range = ( con >= self . feature_mins_ ) * ( con <= self . feature_maxs_ ) # map the class locations where the species has been observed if cat is not None : ohe = self . categorical_estimator . transform ( cat ) should_be_here = ohe [:, self . in_categorical_ ] . any ( axis = 1 ) . reshape (( nrows , 1 )) shouldnt_be_here = ( ~ ohe [:, ~ self . in_categorical_ ] . any ( axis = 1 )) . reshape (( nrows , 1 )) in_range = np . concatenate (( in_range , should_be_here , shouldnt_be_here ), axis = 1 ) # comput envelope based on the overlay method if overlay == \"average\" : ypred = np . mean ( in_range , axis = 1 , dtype = \"float32\" ) elif overlay == \"intersection\" : ypred = np . all ( in_range , axis = 1 ) . astype ( \"uint8\" ) elif overlay == \"union\" : ypred = np . any ( in_range , axis = 1 ) . astype ( \"uint8\" ) return ypred def fit_predict ( self , x : ArrayLike , y : ArrayLike , categorical : list = None , labels : list = None , overlay : str = \"average\" ) -> np . ndarray : \"\"\"Trains and applies a model to x/y data. Args: x: array-like of shape (n_samples, n_features) with covariate data y: array-like of shape (n_samples,) with binary presence/background (1/0) values categorical: column indices indicating which columns are categorical labels: Covariate labels. Ignored if x is a pandas DataFrame overlay: maxent model transformation type. select from [\"average\", \"intersection\", \"union\"] Returns: array-like of shape (n_samples,) with model predictions \"\"\" self . fit ( x , y , categorical = categorical , labels = labels ) return self . predict ( x , overlay = overlay ) __init__ ( self , percentile_range = [ 2.5 , 97.5 ]) special \u00b6 Create a niche envelope model estimator. Parameters: Name Type Description Default percentile_range Tuple[float, float] covariate values within this range are flagged as suitable habitat using a narrow range like [10, 90] drops more areas from suitability maps while [0, 100] creates an envelope around the full range of observed covariates at all y==1 locations. [2.5, 97.5] Source code in elapid/models.py def __init__ ( self , percentile_range : Tuple [ float , float ] = NicheEnvelopeConfig . percentile_range ): \"\"\"Create a niche envelope model estimator. Args: percentile_range: covariate values within this range are flagged as suitable habitat using a narrow range like [10, 90] drops more areas from suitability maps while [0, 100] creates an envelope around the full range of observed covariates at all y==1 locations. \"\"\" self . percentile_range_ = percentile_range self . categorical_estimator = _features . CategoricalTransformer () fit ( self , x , y , categorical = None , labels = None ) \u00b6 Fits a niche envelope model using a set of covariates and presence/background points. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) with covariate data required y Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples,) with binary presence/background (1/0) values required categorical list indices for which columns are categorical None labels list covariate labels. ignored if x is a pandas DataFrame None Source code in elapid/models.py def fit ( self , x : ArrayLike , y : ArrayLike , categorical : list = None , labels : list = None ) -> None : \"\"\"Fits a niche envelope model using a set of covariates and presence/background points. Args: x: array-like of shape (n_samples, n_features) with covariate data y: array-like of shape (n_samples,) with binary presence/background (1/0) values categorical: indices for which columns are categorical labels: covariate labels. ignored if x is a pandas DataFrame \"\"\" # format the input x data self . _format_labels_and_dtypes ( x , categorical = categorical , labels = labels ) con , cat = self . _format_covariate_data ( x ) # estimate the feature range of the continuous data self . feature_mins_ = np . percentile ( con [ y == 1 ], self . percentile_range_ [ 0 ], axis = 0 ) self . feature_maxs_ = np . percentile ( con [ y == 1 ], self . percentile_range_ [ 1 ], axis = 0 ) # one-hot encode the categorical data and label the classes with if cat is not None : ohe = self . categorical_estimator . fit_transform ( cat ) self . in_categorical_ = np . any ( ohe [ y == 1 ], axis = 0 ) fit_predict ( self , x , y , categorical = None , labels = None , overlay = 'average' ) \u00b6 Trains and applies a model to x/y data. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) with covariate data required y Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples,) with binary presence/background (1/0) values required categorical list column indices indicating which columns are categorical None labels list Covariate labels. Ignored if x is a pandas DataFrame None overlay str maxent model transformation type. select from [\"average\", \"intersection\", \"union\"] 'average' Returns: Type Description ndarray array-like of shape (n_samples,) with model predictions Source code in elapid/models.py def fit_predict ( self , x : ArrayLike , y : ArrayLike , categorical : list = None , labels : list = None , overlay : str = \"average\" ) -> np . ndarray : \"\"\"Trains and applies a model to x/y data. Args: x: array-like of shape (n_samples, n_features) with covariate data y: array-like of shape (n_samples,) with binary presence/background (1/0) values categorical: column indices indicating which columns are categorical labels: Covariate labels. Ignored if x is a pandas DataFrame overlay: maxent model transformation type. select from [\"average\", \"intersection\", \"union\"] Returns: array-like of shape (n_samples,) with model predictions \"\"\" self . fit ( x , y , categorical = categorical , labels = labels ) return self . predict ( x , overlay = overlay ) predict ( self , x , overlay = 'average' ) \u00b6 Applies a model to a set of covariates or features. Requires that a model has been fit. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) with covariate data required overlay str niche envelope overlap type. select from [\"average\", \"intersection\", \"union\"] 'average' Returns: Type Description ndarray array-like of shape (n_samples,) with model predictions Source code in elapid/models.py def predict ( self , x : ArrayLike , overlay : str = \"average\" ) -> np . ndarray : \"\"\"Applies a model to a set of covariates or features. Requires that a model has been fit. Args: x: array-like of shape (n_samples, n_features) with covariate data overlay: niche envelope overlap type. select from [\"average\", \"intersection\", \"union\"] Returns: array-like of shape (n_samples,) with model predictions \"\"\" overlay = overlay . lower () overlay_options = [ \"average\" , \"intersection\" , \"union\" ] assert overlay in overlay_options , f \"overlay must be one of { ', ' . join ( overlay_options ) } \" # format the input data con , cat = self . _format_covariate_data ( x ) nrows , ncols = x . shape # any value within the transformed range is considered within the envelope in_range = ( con >= self . feature_mins_ ) * ( con <= self . feature_maxs_ ) # map the class locations where the species has been observed if cat is not None : ohe = self . categorical_estimator . transform ( cat ) should_be_here = ohe [:, self . in_categorical_ ] . any ( axis = 1 ) . reshape (( nrows , 1 )) shouldnt_be_here = ( ~ ohe [:, ~ self . in_categorical_ ] . any ( axis = 1 )) . reshape (( nrows , 1 )) in_range = np . concatenate (( in_range , should_be_here , shouldnt_be_here ), axis = 1 ) # comput envelope based on the overlay method if overlay == \"average\" : ypred = np . mean ( in_range , axis = 1 , dtype = \"float32\" ) elif overlay == \"intersection\" : ypred = np . all ( in_range , axis = 1 ) . astype ( \"uint8\" ) elif overlay == \"union\" : ypred = np . any ( in_range , axis = 1 ) . astype ( \"uint8\" ) return ypred maxent_alpha ( raw ) \u00b6 Compute the sum-to-one alpha maxent model parameter. Parameters: Name Type Description Default raw ndarray uncalibrated maxent raw (exponential) model output required Returns: Type Description alpha the output sum-to-one scaling factor Source code in elapid/models.py def maxent_alpha ( raw : np . ndarray ) -> float : \"\"\"Compute the sum-to-one alpha maxent model parameter. Args: raw: uncalibrated maxent raw (exponential) model output Returns: alpha: the output sum-to-one scaling factor \"\"\" return - np . log ( np . sum ( raw )) maxent_entropy ( raw ) \u00b6 Compute the maxent model entropy score for scaling the logistic output Parameters: Name Type Description Default raw ndarray uncalibrated maxent raw (exponential) model output required Returns: Type Description entropy background distribution entropy score Source code in elapid/models.py def maxent_entropy ( raw : np . ndarray ) -> float : \"\"\"Compute the maxent model entropy score for scaling the logistic output Args: raw: uncalibrated maxent raw (exponential) model output Returns: entropy: background distribution entropy score \"\"\" scaled = raw / np . sum ( raw ) return - np . sum ( scaled * np . log ( scaled ))","title":"elapid.models"},{"location":"module/models/#elapidmodels","text":"Classes for training species distribution models.","title":"elapid.models"},{"location":"module/models/#elapid.models.MaxentModel","text":"Model estimator for Maxent-style species distribution models. Source code in elapid/models.py class MaxentModel ( BaseEstimator ): \"\"\"Model estimator for Maxent-style species distribution models.\"\"\" feature_types_ : list = MaxentConfig . feature_types tau_ : float = MaxentConfig . tau clamp_ : bool = MaxentConfig . clamp convergence_tolerance_ : float = MaxentConfig . tolerance beta_multiplier_ : float = MaxentConfig . beta_multiplier beta_hinge_ : float = MaxentConfig . beta_hinge beta_lqp_ : float = MaxentConfig . beta_lqp beta_threshold_ : float = MaxentConfig . beta_threshold beta_categorical_ : float = MaxentConfig . beta_categorical n_hinge_features_ : int = MaxentConfig . n_hinge_features n_threshold_features_ : int = MaxentConfig . n_threshold_features scorer_ : str = MaxentConfig . scorer use_lambdas_ : str = MaxentConfig . use_lambdas n_lambdas_ : str = MaxentConfig . n_lambdas initialized_ : bool = False beta_scores_ : np . array = None entropy_ : float = 0.0 alpha_ : float = 0.0 estimator : BaseEstimator = None transformer : _features . MaxentFeatureTransformer = None weights_ : np . ndarray = None regularization_ : np . ndarray = None lambdas_ : np . ndarray = None weight_strategy_ : Union [ str , float ] = None n_cpus_ = n_cpus def __init__ ( self , feature_types : Union [ list , str ] = MaxentConfig . feature_types , tau : float = MaxentConfig . tau , clamp : bool = MaxentConfig . clamp , scorer : str = MaxentConfig . scorer , beta_multiplier : float = MaxentConfig . beta_multiplier , beta_lqp : float = MaxentConfig . beta_lqp , beta_hinge : float = MaxentConfig . beta_hinge , beta_threshold : float = MaxentConfig . beta_lqp , beta_categorical : float = MaxentConfig . beta_categorical , n_hinge_features : int = MaxentConfig . n_hinge_features , n_threshold_features : int = MaxentConfig . n_threshold_features , convergence_tolerance : float = MaxentConfig . tolerance , use_lambdas : str = MaxentConfig . use_lambdas , n_lambdas : int = MaxentConfig . n_lambdas , weights : Union [ str , float ] = MaxentConfig . weights , n_cpus : int = n_cpus , ): \"\"\"Create a maxent model object. Args: feature_types: maxent feature types to fit. must be in string \"lqphta\" or list [\"linear\", \"quadratic\", \"product\", \"hinge\", \"threshold\", \"auto\"] tau: maxent prevalence value for scaling logistic output clamp: set features to min/max range from training during prediction scorer: sklearn scoring function for model training beta_multiplier: scaler for all regularization parameters. higher values drop more coeffiecients beta_lqp: linear, quadratic and product feature regularization scaler beta_hinge: hinge feature regularization scaler beta_threshold: threshold feature regularization scaler beta_categorical: categorical feature regularization scaler convergence_tolerance: model convergence tolerance level use_lambdas: guide for which model lambdas to select (either \"best\" or \"last\") n_lambdas: number of lamba values to fit models with weights: strategy for weighting presence samples. pass \"balance\" to compute the ratio based on sample frequency or pass a float for the presence:background weight ratio n_cpus: threads to use during model training \"\"\" self . feature_types_ = validate_feature_types ( feature_types ) self . tau_ = tau self . clamp_ = clamp self . convergence_tolerance_ = convergence_tolerance self . beta_multiplier_ = beta_multiplier self . beta_hinge_ = beta_hinge self . beta_lqp_ = beta_lqp self . beta_threshold_ = beta_threshold self . beta_categorical_ = beta_categorical self . n_hinge_features_ = n_hinge_features self . n_threshold_features_ = n_threshold_features self . n_cpus_ = n_cpus self . scorer_ = scorer self . use_lambdas_ = use_lambdas self . n_lambdas_ = n_lambdas self . weight_strategy_ = weights def fit ( self , x : ArrayLike , y : ArrayLike , categorical : List [ int ] = None , labels : list = None , is_features : bool = False , feature_labels : list = None , ) -> None : \"\"\"Trains a maxent model using a set of covariates and presence/background points. Args: x: array-like of shape (n_samples, n_features) with covariate data y: array-like of shape (n_samples,) with binary presence/background (1/0) values categorical: indices for which columns are categorical labels: covariate labels. ignored if x is a pandas DataFrame is_features: specify that x data has been transformed from covariates to features feature_labels: list of length n_features, with labels identifying each column's feature type with options [\"linear\", \"quadratic\", \"product\", \"threshold\", \"hinge\", \"categorical\"] must be set if `is_features=True` \"\"\" # fir the feature transformer if is_features : features = x assert feature_labels is not None , \"feature_labels must be set if is_features=True\" else : self . transformer = _features . MaxentFeatureTransformer ( feature_types = self . feature_types_ , clamp = self . clamp_ , n_hinge_features = self . n_hinge_features_ , n_threshold_features = self . n_threshold_features_ , ) features = self . transformer . fit_transform ( x , categorical = categorical , labels = labels ) feature_labels = self . transformer . feature_names_ # compute sample weights if self . weight_strategy_ == \"balance\" : pbr = len ( y ) / y . sum () else : pbr = self . weight_strategy_ self . weights_ = _features . compute_weights ( y , pbr = pbr ) # set feature regularization parameters self . regularization_ = _features . compute_regularization ( y , features , feature_labels = feature_labels , beta_multiplier = self . beta_multiplier_ , beta_threshold = self . beta_threshold_ , beta_hinge = self . beta_hinge_ , beta_categorical = self . beta_categorical_ , ) # get model lambda scores to initialize the glm self . lambdas_ = _features . compute_lambdas ( y , self . weights_ , self . regularization_ ) # model fitting self . initialize_model ( lambdas = self . lambdas_ ) self . estimator . fit ( features , y , sample_weight = self . weights_ , relative_penalties = self . regularization_ , ) # get the beta values based on which lamba selection method to use if self . use_lambdas_ == \"last\" : self . beta_scores_ = self . estimator . coef_path_ [ 0 , :, - 1 ] elif self . use_lambdas_ == \"best\" : self . beta_scores_ = self . estimator . coef_path_ [ 0 , :, self . estimator . lambda_max_inx_ ] # apply maxent-specific transformations raw = self . predict ( features [ y == 0 ], transform = \"raw\" , is_features = True ) # alpha is a normalizing constant that ensures that f1(z) integrates (sums) to 1 self . alpha_ = maxent_alpha ( raw ) # the distance from f(z) is the relative entropy of f1(z) WRT f(z) self . entropy_ = maxent_entropy ( raw ) def predict ( self , x : ArrayLike , transform : str = \"cloglog\" , is_features : bool = False ) -> ArrayLike : \"\"\"Applies a model to a set of covariates or features. Requires that a model has been fit. Args: x: array-like of shape (n_samples, n_features) with covariate data transform: maxent model transformation type. select from [\"raw\", \"logistic\", \"cloglog\"]. is_features: flag that x data has already been transformed from covariates to features Returns: predictions: array-like of shape (n_samples,) with model predictions \"\"\" assert self . initialized_ , \"Model must be fit first\" # feature transformations features = x if is_features else self . transformer . transform ( x ) # apply the model engma = np . matmul ( features , self . beta_scores_ ) + self . alpha_ # scale based on the transform type if transform == \"raw\" : return np . exp ( engma ) elif transform == \"logistic\" : # below is R's maxnet (tau-free) logistic formulation # return 1 / (1 + np.exp(-self.entropy_ - raw)) # use the java formulation instead logratio = np . exp ( engma ) * np . exp ( self . entropy_ ) return ( self . tau_ * logratio ) / (( 1 - self . tau_ ) + ( self . tau_ * logratio )) elif transform == \"cloglog\" : # below is R's maxent cloglog formula # return 1 - np.exp(0 - np.exp(self.entropy_ - raw)) # use java again return 1 - np . exp ( - np . exp ( engma ) * np . exp ( self . entropy_ )) def fit_predict ( self , x : ArrayLike , y : ArrayLike , categorical : list = None , labels : list = None , transform : str = \"cloglog\" , is_features : bool = False , feature_labels : list = None , ) -> ArrayLike : \"\"\"Trains and applies a model to x/y data. Args: x: array-like of shape (n_samples, n_features) with covariate data y: array-like of shape (n_samples,) with binary presence/background (1/0) values categorical: column indices indicating which columns are categorical labels: Covariate labels. Ignored if x is a pandas DataFrame transform: maxent model transformation type. select from [\"raw\", \"logistic\", \"cloglog\"]. is_features: specify that x data has already been transformed from covariates to features feature_labels: list of length n_features, with labels identifying each column's feature type with options [\"linear\", \"quadratic\", \"product\", \"threshold\", \"hinge\", \"categorical\"] must be set if `is_features=True` Returns: predictions: Array-like of shape (n_samples,) with model predictions \"\"\" self . fit ( x , y , categorical = categorical , labels = labels , is_features = is_features , feature_labels = feature_labels ) predictions = self . predict ( x , transform = transform , is_features = is_features ) return predictions def initialize_model ( self , lambdas : np . array , alpha : float = 1 , standardize : bool = False , fit_intercept : bool = True , ) -> None : \"\"\"Creates the Logistic Regression with elastic net penalty model object. Args: lambdas: array of model lambda values. get from elapid.features.compute_lambdas() alpha: elasticnet mixing parameter. alpha=1 for lasso, alpha=0 for ridge standardize: specify coefficient normalization fit_intercept: include an intercept parameter Returns: None. updates the self.estimator with an sklearn-style model estimator \"\"\" self . estimator = LogitNet ( alpha = alpha , lambda_path = lambdas , standardize = standardize , fit_intercept = fit_intercept , scoring = self . scorer_ , n_jobs = self . n_cpus_ , tol = self . convergence_tolerance_ , ) self . initialized_ = True","title":"MaxentModel"},{"location":"module/models/#elapid.models.MaxentModel.__init__","text":"Create a maxent model object. Parameters: Name Type Description Default feature_types Union[list, str] maxent feature types to fit. must be in string \"lqphta\" or list [\"linear\", \"quadratic\", \"product\", \"hinge\", \"threshold\", \"auto\"] ['linear', 'hinge', 'product'] tau float maxent prevalence value for scaling logistic output 0.5 clamp bool set features to min/max range from training during prediction True scorer str sklearn scoring function for model training 'roc_auc' beta_multiplier float scaler for all regularization parameters. higher values drop more coeffiecients 1.0 beta_lqp float linear, quadratic and product feature regularization scaler 1.0 beta_hinge float hinge feature regularization scaler 1.0 beta_threshold float threshold feature regularization scaler 1.0 beta_categorical float categorical feature regularization scaler 1.0 convergence_tolerance float model convergence tolerance level 1e-07 use_lambdas str guide for which model lambdas to select (either \"best\" or \"last\") 'best' n_lambdas int number of lamba values to fit models with 100 weights Union[str, float] strategy for weighting presence samples. pass \"balance\" to compute the ratio based on sample frequency or pass a float for the presence:background weight ratio 'balance' n_cpus int threads to use during model training 4 Source code in elapid/models.py def __init__ ( self , feature_types : Union [ list , str ] = MaxentConfig . feature_types , tau : float = MaxentConfig . tau , clamp : bool = MaxentConfig . clamp , scorer : str = MaxentConfig . scorer , beta_multiplier : float = MaxentConfig . beta_multiplier , beta_lqp : float = MaxentConfig . beta_lqp , beta_hinge : float = MaxentConfig . beta_hinge , beta_threshold : float = MaxentConfig . beta_lqp , beta_categorical : float = MaxentConfig . beta_categorical , n_hinge_features : int = MaxentConfig . n_hinge_features , n_threshold_features : int = MaxentConfig . n_threshold_features , convergence_tolerance : float = MaxentConfig . tolerance , use_lambdas : str = MaxentConfig . use_lambdas , n_lambdas : int = MaxentConfig . n_lambdas , weights : Union [ str , float ] = MaxentConfig . weights , n_cpus : int = n_cpus , ): \"\"\"Create a maxent model object. Args: feature_types: maxent feature types to fit. must be in string \"lqphta\" or list [\"linear\", \"quadratic\", \"product\", \"hinge\", \"threshold\", \"auto\"] tau: maxent prevalence value for scaling logistic output clamp: set features to min/max range from training during prediction scorer: sklearn scoring function for model training beta_multiplier: scaler for all regularization parameters. higher values drop more coeffiecients beta_lqp: linear, quadratic and product feature regularization scaler beta_hinge: hinge feature regularization scaler beta_threshold: threshold feature regularization scaler beta_categorical: categorical feature regularization scaler convergence_tolerance: model convergence tolerance level use_lambdas: guide for which model lambdas to select (either \"best\" or \"last\") n_lambdas: number of lamba values to fit models with weights: strategy for weighting presence samples. pass \"balance\" to compute the ratio based on sample frequency or pass a float for the presence:background weight ratio n_cpus: threads to use during model training \"\"\" self . feature_types_ = validate_feature_types ( feature_types ) self . tau_ = tau self . clamp_ = clamp self . convergence_tolerance_ = convergence_tolerance self . beta_multiplier_ = beta_multiplier self . beta_hinge_ = beta_hinge self . beta_lqp_ = beta_lqp self . beta_threshold_ = beta_threshold self . beta_categorical_ = beta_categorical self . n_hinge_features_ = n_hinge_features self . n_threshold_features_ = n_threshold_features self . n_cpus_ = n_cpus self . scorer_ = scorer self . use_lambdas_ = use_lambdas self . n_lambdas_ = n_lambdas self . weight_strategy_ = weights","title":"__init__()"},{"location":"module/models/#elapid.models.MaxentModel.fit","text":"Trains a maxent model using a set of covariates and presence/background points. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) with covariate data required y Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples,) with binary presence/background (1/0) values required categorical List[int] indices for which columns are categorical None labels list covariate labels. ignored if x is a pandas DataFrame None is_features bool specify that x data has been transformed from covariates to features False feature_labels list list of length n_features, with labels identifying each column's feature type with options [\"linear\", \"quadratic\", \"product\", \"threshold\", \"hinge\", \"categorical\"] must be set if is_features=True None Source code in elapid/models.py def fit ( self , x : ArrayLike , y : ArrayLike , categorical : List [ int ] = None , labels : list = None , is_features : bool = False , feature_labels : list = None , ) -> None : \"\"\"Trains a maxent model using a set of covariates and presence/background points. Args: x: array-like of shape (n_samples, n_features) with covariate data y: array-like of shape (n_samples,) with binary presence/background (1/0) values categorical: indices for which columns are categorical labels: covariate labels. ignored if x is a pandas DataFrame is_features: specify that x data has been transformed from covariates to features feature_labels: list of length n_features, with labels identifying each column's feature type with options [\"linear\", \"quadratic\", \"product\", \"threshold\", \"hinge\", \"categorical\"] must be set if `is_features=True` \"\"\" # fir the feature transformer if is_features : features = x assert feature_labels is not None , \"feature_labels must be set if is_features=True\" else : self . transformer = _features . MaxentFeatureTransformer ( feature_types = self . feature_types_ , clamp = self . clamp_ , n_hinge_features = self . n_hinge_features_ , n_threshold_features = self . n_threshold_features_ , ) features = self . transformer . fit_transform ( x , categorical = categorical , labels = labels ) feature_labels = self . transformer . feature_names_ # compute sample weights if self . weight_strategy_ == \"balance\" : pbr = len ( y ) / y . sum () else : pbr = self . weight_strategy_ self . weights_ = _features . compute_weights ( y , pbr = pbr ) # set feature regularization parameters self . regularization_ = _features . compute_regularization ( y , features , feature_labels = feature_labels , beta_multiplier = self . beta_multiplier_ , beta_threshold = self . beta_threshold_ , beta_hinge = self . beta_hinge_ , beta_categorical = self . beta_categorical_ , ) # get model lambda scores to initialize the glm self . lambdas_ = _features . compute_lambdas ( y , self . weights_ , self . regularization_ ) # model fitting self . initialize_model ( lambdas = self . lambdas_ ) self . estimator . fit ( features , y , sample_weight = self . weights_ , relative_penalties = self . regularization_ , ) # get the beta values based on which lamba selection method to use if self . use_lambdas_ == \"last\" : self . beta_scores_ = self . estimator . coef_path_ [ 0 , :, - 1 ] elif self . use_lambdas_ == \"best\" : self . beta_scores_ = self . estimator . coef_path_ [ 0 , :, self . estimator . lambda_max_inx_ ] # apply maxent-specific transformations raw = self . predict ( features [ y == 0 ], transform = \"raw\" , is_features = True ) # alpha is a normalizing constant that ensures that f1(z) integrates (sums) to 1 self . alpha_ = maxent_alpha ( raw ) # the distance from f(z) is the relative entropy of f1(z) WRT f(z) self . entropy_ = maxent_entropy ( raw )","title":"fit()"},{"location":"module/models/#elapid.models.MaxentModel.fit_predict","text":"Trains and applies a model to x/y data. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) with covariate data required y Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples,) with binary presence/background (1/0) values required categorical list column indices indicating which columns are categorical None labels list Covariate labels. Ignored if x is a pandas DataFrame None transform str maxent model transformation type. select from [\"raw\", \"logistic\", \"cloglog\"]. 'cloglog' is_features bool specify that x data has already been transformed from covariates to features False feature_labels list list of length n_features, with labels identifying each column's feature type with options [\"linear\", \"quadratic\", \"product\", \"threshold\", \"hinge\", \"categorical\"] must be set if is_features=True None Returns: Type Description predictions Array-like of shape (n_samples,) with model predictions Source code in elapid/models.py def fit_predict ( self , x : ArrayLike , y : ArrayLike , categorical : list = None , labels : list = None , transform : str = \"cloglog\" , is_features : bool = False , feature_labels : list = None , ) -> ArrayLike : \"\"\"Trains and applies a model to x/y data. Args: x: array-like of shape (n_samples, n_features) with covariate data y: array-like of shape (n_samples,) with binary presence/background (1/0) values categorical: column indices indicating which columns are categorical labels: Covariate labels. Ignored if x is a pandas DataFrame transform: maxent model transformation type. select from [\"raw\", \"logistic\", \"cloglog\"]. is_features: specify that x data has already been transformed from covariates to features feature_labels: list of length n_features, with labels identifying each column's feature type with options [\"linear\", \"quadratic\", \"product\", \"threshold\", \"hinge\", \"categorical\"] must be set if `is_features=True` Returns: predictions: Array-like of shape (n_samples,) with model predictions \"\"\" self . fit ( x , y , categorical = categorical , labels = labels , is_features = is_features , feature_labels = feature_labels ) predictions = self . predict ( x , transform = transform , is_features = is_features ) return predictions","title":"fit_predict()"},{"location":"module/models/#elapid.models.MaxentModel.initialize_model","text":"Creates the Logistic Regression with elastic net penalty model object. Parameters: Name Type Description Default lambdas <built-in function array> array of model lambda values. get from elapid.features.compute_lambdas() required alpha float elasticnet mixing parameter. alpha=1 for lasso, alpha=0 for ridge 1 standardize bool specify coefficient normalization False fit_intercept bool include an intercept parameter True Returns: Type Description None None. updates the self.estimator with an sklearn-style model estimator Source code in elapid/models.py def initialize_model ( self , lambdas : np . array , alpha : float = 1 , standardize : bool = False , fit_intercept : bool = True , ) -> None : \"\"\"Creates the Logistic Regression with elastic net penalty model object. Args: lambdas: array of model lambda values. get from elapid.features.compute_lambdas() alpha: elasticnet mixing parameter. alpha=1 for lasso, alpha=0 for ridge standardize: specify coefficient normalization fit_intercept: include an intercept parameter Returns: None. updates the self.estimator with an sklearn-style model estimator \"\"\" self . estimator = LogitNet ( alpha = alpha , lambda_path = lambdas , standardize = standardize , fit_intercept = fit_intercept , scoring = self . scorer_ , n_jobs = self . n_cpus_ , tol = self . convergence_tolerance_ , ) self . initialized_ = True","title":"initialize_model()"},{"location":"module/models/#elapid.models.MaxentModel.predict","text":"Applies a model to a set of covariates or features. Requires that a model has been fit. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) with covariate data required transform str maxent model transformation type. select from [\"raw\", \"logistic\", \"cloglog\"]. 'cloglog' is_features bool flag that x data has already been transformed from covariates to features False Returns: Type Description predictions array-like of shape (n_samples,) with model predictions Source code in elapid/models.py def predict ( self , x : ArrayLike , transform : str = \"cloglog\" , is_features : bool = False ) -> ArrayLike : \"\"\"Applies a model to a set of covariates or features. Requires that a model has been fit. Args: x: array-like of shape (n_samples, n_features) with covariate data transform: maxent model transformation type. select from [\"raw\", \"logistic\", \"cloglog\"]. is_features: flag that x data has already been transformed from covariates to features Returns: predictions: array-like of shape (n_samples,) with model predictions \"\"\" assert self . initialized_ , \"Model must be fit first\" # feature transformations features = x if is_features else self . transformer . transform ( x ) # apply the model engma = np . matmul ( features , self . beta_scores_ ) + self . alpha_ # scale based on the transform type if transform == \"raw\" : return np . exp ( engma ) elif transform == \"logistic\" : # below is R's maxnet (tau-free) logistic formulation # return 1 / (1 + np.exp(-self.entropy_ - raw)) # use the java formulation instead logratio = np . exp ( engma ) * np . exp ( self . entropy_ ) return ( self . tau_ * logratio ) / (( 1 - self . tau_ ) + ( self . tau_ * logratio )) elif transform == \"cloglog\" : # below is R's maxent cloglog formula # return 1 - np.exp(0 - np.exp(self.entropy_ - raw)) # use java again return 1 - np . exp ( - np . exp ( engma ) * np . exp ( self . entropy_ ))","title":"predict()"},{"location":"module/models/#elapid.models.NicheEnvelopeModel","text":"Model estimator for niche envelope-style models. Source code in elapid/models.py class NicheEnvelopeModel ( BaseEstimator ): \"\"\"Model estimator for niche envelope-style models.\"\"\" percentile_range_ : Tuple [ float , float ] = None feature_mins_ : np . ndarray = None feature_maxs_ : np . ndarray = None categorical_estimator : BaseEstimator = None categorical_ : list = None continuous_ : list = None categorical_pd_ : list = None continuous_pd_ : list = None in_categorical_ : np . ndarray = None def __init__ ( self , percentile_range : Tuple [ float , float ] = NicheEnvelopeConfig . percentile_range ): \"\"\"Create a niche envelope model estimator. Args: percentile_range: covariate values within this range are flagged as suitable habitat using a narrow range like [10, 90] drops more areas from suitability maps while [0, 100] creates an envelope around the full range of observed covariates at all y==1 locations. \"\"\" self . percentile_range_ = percentile_range self . categorical_estimator = _features . CategoricalTransformer () def _format_covariate_data ( self , x : ArrayLike ) -> Tuple [ np . array , np . array ]: \"\"\"Reads input x data and formats it to consistent array dtypes. Args: x: array-like of shape (n_samples, n_features) Returns: (continuous, categorical) tuple of ndarrays with continuous and categorical covariate data. \"\"\" if isinstance ( x , np . ndarray ): if self . categorical_ is None : con = x cat = None else : con = x [:, self . continuous_ ] cat = x [:, self . categorical_ ] elif isinstance ( x , pd . DataFrame ): con = x [ self . continuous_pd_ ] . to_numpy () if len ( self . categorical_pd_ ) > 0 : cat = x [ self . categorical_pd_ ] . to_numpy () else : cat = None else : raise TypeError ( f \"Unsupported x dtype: { type ( x ) } . Must be pd.DataFrame or np.array\" ) return con , cat def _format_labels_and_dtypes ( self , x : ArrayLike , categorical : list = None , labels : list = None ) -> None : \"\"\"Read input x data and lists of categorical data indices and band labels to format and store this info for later indexing. Args: s: array-like of shape (n_samples, n_features) categorical: indices indicating which x columns are categorical labels: covariate column labels. ignored if x is a pandas DataFrame \"\"\" if isinstance ( x , np . ndarray ): nrows , ncols = x . shape if categorical is None : continuous = list ( range ( ncols )) else : continuous = list ( set ( range ( ncols )) . difference ( set ( categorical ))) self . labels_ = labels or make_band_labels ( ncols ) self . categorical_ = categorical self . continuous_ = continuous elif isinstance ( x , pd . DataFrame ): x . drop ([ \"geometry\" ], axis = 1 , errors = \"ignore\" , inplace = True ) self . labels_ = labels or list ( x . columns ) # store both pandas and numpy indexing of these values self . continuous_pd_ = list ( x . select_dtypes ( exclude = \"category\" ) . columns ) self . categorical_pd_ = list ( x . select_dtypes ( include = \"category\" ) . columns ) all_columns = list ( x . columns ) self . continuous_ = [ all_columns . index ( item ) for item in self . continuous_pd_ if item in all_columns ] if len ( self . categorical_pd_ ) != 0 : self . categorical_ = [ all_columns . index ( item ) for item in self . categorical_pd_ if item in all_columns ] else : self . categorical_ = None def fit ( self , x : ArrayLike , y : ArrayLike , categorical : list = None , labels : list = None ) -> None : \"\"\"Fits a niche envelope model using a set of covariates and presence/background points. Args: x: array-like of shape (n_samples, n_features) with covariate data y: array-like of shape (n_samples,) with binary presence/background (1/0) values categorical: indices for which columns are categorical labels: covariate labels. ignored if x is a pandas DataFrame \"\"\" # format the input x data self . _format_labels_and_dtypes ( x , categorical = categorical , labels = labels ) con , cat = self . _format_covariate_data ( x ) # estimate the feature range of the continuous data self . feature_mins_ = np . percentile ( con [ y == 1 ], self . percentile_range_ [ 0 ], axis = 0 ) self . feature_maxs_ = np . percentile ( con [ y == 1 ], self . percentile_range_ [ 1 ], axis = 0 ) # one-hot encode the categorical data and label the classes with if cat is not None : ohe = self . categorical_estimator . fit_transform ( cat ) self . in_categorical_ = np . any ( ohe [ y == 1 ], axis = 0 ) def predict ( self , x : ArrayLike , overlay : str = \"average\" ) -> np . ndarray : \"\"\"Applies a model to a set of covariates or features. Requires that a model has been fit. Args: x: array-like of shape (n_samples, n_features) with covariate data overlay: niche envelope overlap type. select from [\"average\", \"intersection\", \"union\"] Returns: array-like of shape (n_samples,) with model predictions \"\"\" overlay = overlay . lower () overlay_options = [ \"average\" , \"intersection\" , \"union\" ] assert overlay in overlay_options , f \"overlay must be one of { ', ' . join ( overlay_options ) } \" # format the input data con , cat = self . _format_covariate_data ( x ) nrows , ncols = x . shape # any value within the transformed range is considered within the envelope in_range = ( con >= self . feature_mins_ ) * ( con <= self . feature_maxs_ ) # map the class locations where the species has been observed if cat is not None : ohe = self . categorical_estimator . transform ( cat ) should_be_here = ohe [:, self . in_categorical_ ] . any ( axis = 1 ) . reshape (( nrows , 1 )) shouldnt_be_here = ( ~ ohe [:, ~ self . in_categorical_ ] . any ( axis = 1 )) . reshape (( nrows , 1 )) in_range = np . concatenate (( in_range , should_be_here , shouldnt_be_here ), axis = 1 ) # comput envelope based on the overlay method if overlay == \"average\" : ypred = np . mean ( in_range , axis = 1 , dtype = \"float32\" ) elif overlay == \"intersection\" : ypred = np . all ( in_range , axis = 1 ) . astype ( \"uint8\" ) elif overlay == \"union\" : ypred = np . any ( in_range , axis = 1 ) . astype ( \"uint8\" ) return ypred def fit_predict ( self , x : ArrayLike , y : ArrayLike , categorical : list = None , labels : list = None , overlay : str = \"average\" ) -> np . ndarray : \"\"\"Trains and applies a model to x/y data. Args: x: array-like of shape (n_samples, n_features) with covariate data y: array-like of shape (n_samples,) with binary presence/background (1/0) values categorical: column indices indicating which columns are categorical labels: Covariate labels. Ignored if x is a pandas DataFrame overlay: maxent model transformation type. select from [\"average\", \"intersection\", \"union\"] Returns: array-like of shape (n_samples,) with model predictions \"\"\" self . fit ( x , y , categorical = categorical , labels = labels ) return self . predict ( x , overlay = overlay )","title":"NicheEnvelopeModel"},{"location":"module/models/#elapid.models.NicheEnvelopeModel.__init__","text":"Create a niche envelope model estimator. Parameters: Name Type Description Default percentile_range Tuple[float, float] covariate values within this range are flagged as suitable habitat using a narrow range like [10, 90] drops more areas from suitability maps while [0, 100] creates an envelope around the full range of observed covariates at all y==1 locations. [2.5, 97.5] Source code in elapid/models.py def __init__ ( self , percentile_range : Tuple [ float , float ] = NicheEnvelopeConfig . percentile_range ): \"\"\"Create a niche envelope model estimator. Args: percentile_range: covariate values within this range are flagged as suitable habitat using a narrow range like [10, 90] drops more areas from suitability maps while [0, 100] creates an envelope around the full range of observed covariates at all y==1 locations. \"\"\" self . percentile_range_ = percentile_range self . categorical_estimator = _features . CategoricalTransformer ()","title":"__init__()"},{"location":"module/models/#elapid.models.NicheEnvelopeModel.fit","text":"Fits a niche envelope model using a set of covariates and presence/background points. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) with covariate data required y Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples,) with binary presence/background (1/0) values required categorical list indices for which columns are categorical None labels list covariate labels. ignored if x is a pandas DataFrame None Source code in elapid/models.py def fit ( self , x : ArrayLike , y : ArrayLike , categorical : list = None , labels : list = None ) -> None : \"\"\"Fits a niche envelope model using a set of covariates and presence/background points. Args: x: array-like of shape (n_samples, n_features) with covariate data y: array-like of shape (n_samples,) with binary presence/background (1/0) values categorical: indices for which columns are categorical labels: covariate labels. ignored if x is a pandas DataFrame \"\"\" # format the input x data self . _format_labels_and_dtypes ( x , categorical = categorical , labels = labels ) con , cat = self . _format_covariate_data ( x ) # estimate the feature range of the continuous data self . feature_mins_ = np . percentile ( con [ y == 1 ], self . percentile_range_ [ 0 ], axis = 0 ) self . feature_maxs_ = np . percentile ( con [ y == 1 ], self . percentile_range_ [ 1 ], axis = 0 ) # one-hot encode the categorical data and label the classes with if cat is not None : ohe = self . categorical_estimator . fit_transform ( cat ) self . in_categorical_ = np . any ( ohe [ y == 1 ], axis = 0 )","title":"fit()"},{"location":"module/models/#elapid.models.NicheEnvelopeModel.fit_predict","text":"Trains and applies a model to x/y data. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) with covariate data required y Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples,) with binary presence/background (1/0) values required categorical list column indices indicating which columns are categorical None labels list Covariate labels. Ignored if x is a pandas DataFrame None overlay str maxent model transformation type. select from [\"average\", \"intersection\", \"union\"] 'average' Returns: Type Description ndarray array-like of shape (n_samples,) with model predictions Source code in elapid/models.py def fit_predict ( self , x : ArrayLike , y : ArrayLike , categorical : list = None , labels : list = None , overlay : str = \"average\" ) -> np . ndarray : \"\"\"Trains and applies a model to x/y data. Args: x: array-like of shape (n_samples, n_features) with covariate data y: array-like of shape (n_samples,) with binary presence/background (1/0) values categorical: column indices indicating which columns are categorical labels: Covariate labels. Ignored if x is a pandas DataFrame overlay: maxent model transformation type. select from [\"average\", \"intersection\", \"union\"] Returns: array-like of shape (n_samples,) with model predictions \"\"\" self . fit ( x , y , categorical = categorical , labels = labels ) return self . predict ( x , overlay = overlay )","title":"fit_predict()"},{"location":"module/models/#elapid.models.NicheEnvelopeModel.predict","text":"Applies a model to a set of covariates or features. Requires that a model has been fit. Parameters: Name Type Description Default x Union[numpy.ndarray, pandas.core.frame.DataFrame] array-like of shape (n_samples, n_features) with covariate data required overlay str niche envelope overlap type. select from [\"average\", \"intersection\", \"union\"] 'average' Returns: Type Description ndarray array-like of shape (n_samples,) with model predictions Source code in elapid/models.py def predict ( self , x : ArrayLike , overlay : str = \"average\" ) -> np . ndarray : \"\"\"Applies a model to a set of covariates or features. Requires that a model has been fit. Args: x: array-like of shape (n_samples, n_features) with covariate data overlay: niche envelope overlap type. select from [\"average\", \"intersection\", \"union\"] Returns: array-like of shape (n_samples,) with model predictions \"\"\" overlay = overlay . lower () overlay_options = [ \"average\" , \"intersection\" , \"union\" ] assert overlay in overlay_options , f \"overlay must be one of { ', ' . join ( overlay_options ) } \" # format the input data con , cat = self . _format_covariate_data ( x ) nrows , ncols = x . shape # any value within the transformed range is considered within the envelope in_range = ( con >= self . feature_mins_ ) * ( con <= self . feature_maxs_ ) # map the class locations where the species has been observed if cat is not None : ohe = self . categorical_estimator . transform ( cat ) should_be_here = ohe [:, self . in_categorical_ ] . any ( axis = 1 ) . reshape (( nrows , 1 )) shouldnt_be_here = ( ~ ohe [:, ~ self . in_categorical_ ] . any ( axis = 1 )) . reshape (( nrows , 1 )) in_range = np . concatenate (( in_range , should_be_here , shouldnt_be_here ), axis = 1 ) # comput envelope based on the overlay method if overlay == \"average\" : ypred = np . mean ( in_range , axis = 1 , dtype = \"float32\" ) elif overlay == \"intersection\" : ypred = np . all ( in_range , axis = 1 ) . astype ( \"uint8\" ) elif overlay == \"union\" : ypred = np . any ( in_range , axis = 1 ) . astype ( \"uint8\" ) return ypred","title":"predict()"},{"location":"module/models/#elapid.models.maxent_alpha","text":"Compute the sum-to-one alpha maxent model parameter. Parameters: Name Type Description Default raw ndarray uncalibrated maxent raw (exponential) model output required Returns: Type Description alpha the output sum-to-one scaling factor Source code in elapid/models.py def maxent_alpha ( raw : np . ndarray ) -> float : \"\"\"Compute the sum-to-one alpha maxent model parameter. Args: raw: uncalibrated maxent raw (exponential) model output Returns: alpha: the output sum-to-one scaling factor \"\"\" return - np . log ( np . sum ( raw ))","title":"maxent_alpha()"},{"location":"module/models/#elapid.models.maxent_entropy","text":"Compute the maxent model entropy score for scaling the logistic output Parameters: Name Type Description Default raw ndarray uncalibrated maxent raw (exponential) model output required Returns: Type Description entropy background distribution entropy score Source code in elapid/models.py def maxent_entropy ( raw : np . ndarray ) -> float : \"\"\"Compute the maxent model entropy score for scaling the logistic output Args: raw: uncalibrated maxent raw (exponential) model output Returns: entropy: background distribution entropy score \"\"\" scaled = raw / np . sum ( raw ) return - np . sum ( scaled * np . log ( scaled ))","title":"maxent_entropy()"},{"location":"module/stats/","text":"elapid.stats \u00b6 Utilities for calculating zonal statistics RasterStat \u00b6 Utility class to iterate over and apply reductions to multiband arrays Source code in elapid/stats.py class RasterStat : \"\"\"Utility class to iterate over and apply reductions to multiband arrays\"\"\" name : str = None method : Callable = None dtype : str = None kwargs : dict = None def __init__ ( self , name : str , method : Callable , dtype : str = None , ** kwargs ): \"\"\"Create a RasterStat object Args: name: the label to prepend to the output column method: function to reduce a 2d ndarray to an (nbands,) shape array dtype: the output array data type **kwargs: additional arguments to pass to `method` \"\"\" self . name = name self . method = method self . dtype = dtype self . kwargs = kwargs def format ( self , x : np . ndarray ) -> np . ndarray : \"\"\"Format the array data into an array of shape [nbands, n_valid_pixels] Args: x: ndarray of shape (nbands, nrows, ncols) or (nbands, n_valid_pixels) Returns: 2d ndarray \"\"\" if x . ndim == 3 : bands , rows , cols = x . shape x = x . reshape (( bands , rows * cols )) return x def reduce ( self , x : np . ndarray ) -> np . ndarray : \"\"\"Reduce an array using the objects `method` function Args: x: ndarray of shape (nbands, n_valid_pixels) Returns: ndarray of shape (nbands,) \"\"\" return self . method ( self . format ( x ), ** self . kwargs ) __init__ ( self , name , method , dtype = None , ** kwargs ) special \u00b6 Create a RasterStat object Parameters: Name Type Description Default name str the label to prepend to the output column required method Callable function to reduce a 2d ndarray to an (nbands,) shape array required dtype str the output array data type None **kwargs additional arguments to pass to method {} Source code in elapid/stats.py def __init__ ( self , name : str , method : Callable , dtype : str = None , ** kwargs ): \"\"\"Create a RasterStat object Args: name: the label to prepend to the output column method: function to reduce a 2d ndarray to an (nbands,) shape array dtype: the output array data type **kwargs: additional arguments to pass to `method` \"\"\" self . name = name self . method = method self . dtype = dtype self . kwargs = kwargs format ( self , x ) \u00b6 Format the array data into an array of shape [nbands, n_valid_pixels] Parameters: Name Type Description Default x ndarray ndarray of shape (nbands, nrows, ncols) or (nbands, n_valid_pixels) required Returns: Type Description ndarray 2d ndarray Source code in elapid/stats.py def format ( self , x : np . ndarray ) -> np . ndarray : \"\"\"Format the array data into an array of shape [nbands, n_valid_pixels] Args: x: ndarray of shape (nbands, nrows, ncols) or (nbands, n_valid_pixels) Returns: 2d ndarray \"\"\" if x . ndim == 3 : bands , rows , cols = x . shape x = x . reshape (( bands , rows * cols )) return x reduce ( self , x ) \u00b6 Reduce an array using the objects method function Parameters: Name Type Description Default x ndarray ndarray of shape (nbands, n_valid_pixels) required Returns: Type Description ndarray ndarray of shape (nbands,) Source code in elapid/stats.py def reduce ( self , x : np . ndarray ) -> np . ndarray : \"\"\"Reduce an array using the objects `method` function Args: x: ndarray of shape (nbands, n_valid_pixels) Returns: ndarray of shape (nbands,) \"\"\" return self . method ( self . format ( x ), ** self . kwargs ) get_raster_stats_methods ( mean = True , min = False , max = False , count = False , sum = False , stdv = False , skew = False , kurtosis = False , mode = False , percentiles = [], all = False ) \u00b6 Return RasterStat configs for the requested stats calculations Source code in elapid/stats.py def get_raster_stats_methods ( mean : bool = True , min : bool = False , max : bool = False , count : bool = False , sum : bool = False , stdv : bool = False , skew : bool = False , kurtosis : bool = False , mode : bool = False , percentiles : list = [], all : bool = False , ) -> List [ RasterStat ]: \"\"\"Return RasterStat configs for the requested stats calculations\"\"\" methods = [] if mean or all : methods . append ( RasterStat ( name = \"mean\" , method = raster_mean , dtype = \"float32\" )) if min or all : methods . append ( RasterStat ( name = \"min\" , method = raster_min )) if max or all : methods . append ( RasterStat ( name = \"max\" , method = raster_max )) if count or all : methods . append ( RasterStat ( name = \"count\" , method = raster_count , dtype = \"int16\" )) if sum or all : methods . append ( RasterStat ( name = \"sum\" , method = raster_sum )) if stdv or all : methods . append ( RasterStat ( name = \"stdv\" , method = raster_stdv )) if skew or all : methods . append ( RasterStat ( name = \"skew\" , method = raster_skew )) if kurtosis or all : methods . append ( RasterStat ( name = \"kurt\" , method = raster_kurtosis )) if mode or all : methods . append ( RasterStat ( name = \"mode\" , method = raster_mode )) if len ( percentiles ) > 0 : for percentile in percentiles : methods . append ( RasterStat ( name = f \" { percentile } pct\" , method = raster_percentile , pctile = percentile )) return methods","title":"elapid.stats"},{"location":"module/stats/#elapidstats","text":"Utilities for calculating zonal statistics","title":"elapid.stats"},{"location":"module/stats/#elapid.stats.RasterStat","text":"Utility class to iterate over and apply reductions to multiband arrays Source code in elapid/stats.py class RasterStat : \"\"\"Utility class to iterate over and apply reductions to multiband arrays\"\"\" name : str = None method : Callable = None dtype : str = None kwargs : dict = None def __init__ ( self , name : str , method : Callable , dtype : str = None , ** kwargs ): \"\"\"Create a RasterStat object Args: name: the label to prepend to the output column method: function to reduce a 2d ndarray to an (nbands,) shape array dtype: the output array data type **kwargs: additional arguments to pass to `method` \"\"\" self . name = name self . method = method self . dtype = dtype self . kwargs = kwargs def format ( self , x : np . ndarray ) -> np . ndarray : \"\"\"Format the array data into an array of shape [nbands, n_valid_pixels] Args: x: ndarray of shape (nbands, nrows, ncols) or (nbands, n_valid_pixels) Returns: 2d ndarray \"\"\" if x . ndim == 3 : bands , rows , cols = x . shape x = x . reshape (( bands , rows * cols )) return x def reduce ( self , x : np . ndarray ) -> np . ndarray : \"\"\"Reduce an array using the objects `method` function Args: x: ndarray of shape (nbands, n_valid_pixels) Returns: ndarray of shape (nbands,) \"\"\" return self . method ( self . format ( x ), ** self . kwargs )","title":"RasterStat"},{"location":"module/stats/#elapid.stats.RasterStat.__init__","text":"Create a RasterStat object Parameters: Name Type Description Default name str the label to prepend to the output column required method Callable function to reduce a 2d ndarray to an (nbands,) shape array required dtype str the output array data type None **kwargs additional arguments to pass to method {} Source code in elapid/stats.py def __init__ ( self , name : str , method : Callable , dtype : str = None , ** kwargs ): \"\"\"Create a RasterStat object Args: name: the label to prepend to the output column method: function to reduce a 2d ndarray to an (nbands,) shape array dtype: the output array data type **kwargs: additional arguments to pass to `method` \"\"\" self . name = name self . method = method self . dtype = dtype self . kwargs = kwargs","title":"__init__()"},{"location":"module/stats/#elapid.stats.RasterStat.format","text":"Format the array data into an array of shape [nbands, n_valid_pixels] Parameters: Name Type Description Default x ndarray ndarray of shape (nbands, nrows, ncols) or (nbands, n_valid_pixels) required Returns: Type Description ndarray 2d ndarray Source code in elapid/stats.py def format ( self , x : np . ndarray ) -> np . ndarray : \"\"\"Format the array data into an array of shape [nbands, n_valid_pixels] Args: x: ndarray of shape (nbands, nrows, ncols) or (nbands, n_valid_pixels) Returns: 2d ndarray \"\"\" if x . ndim == 3 : bands , rows , cols = x . shape x = x . reshape (( bands , rows * cols )) return x","title":"format()"},{"location":"module/stats/#elapid.stats.RasterStat.reduce","text":"Reduce an array using the objects method function Parameters: Name Type Description Default x ndarray ndarray of shape (nbands, n_valid_pixels) required Returns: Type Description ndarray ndarray of shape (nbands,) Source code in elapid/stats.py def reduce ( self , x : np . ndarray ) -> np . ndarray : \"\"\"Reduce an array using the objects `method` function Args: x: ndarray of shape (nbands, n_valid_pixels) Returns: ndarray of shape (nbands,) \"\"\" return self . method ( self . format ( x ), ** self . kwargs )","title":"reduce()"},{"location":"module/stats/#elapid.stats.get_raster_stats_methods","text":"Return RasterStat configs for the requested stats calculations Source code in elapid/stats.py def get_raster_stats_methods ( mean : bool = True , min : bool = False , max : bool = False , count : bool = False , sum : bool = False , stdv : bool = False , skew : bool = False , kurtosis : bool = False , mode : bool = False , percentiles : list = [], all : bool = False , ) -> List [ RasterStat ]: \"\"\"Return RasterStat configs for the requested stats calculations\"\"\" methods = [] if mean or all : methods . append ( RasterStat ( name = \"mean\" , method = raster_mean , dtype = \"float32\" )) if min or all : methods . append ( RasterStat ( name = \"min\" , method = raster_min )) if max or all : methods . append ( RasterStat ( name = \"max\" , method = raster_max )) if count or all : methods . append ( RasterStat ( name = \"count\" , method = raster_count , dtype = \"int16\" )) if sum or all : methods . append ( RasterStat ( name = \"sum\" , method = raster_sum )) if stdv or all : methods . append ( RasterStat ( name = \"stdv\" , method = raster_stdv )) if skew or all : methods . append ( RasterStat ( name = \"skew\" , method = raster_skew )) if kurtosis or all : methods . append ( RasterStat ( name = \"kurt\" , method = raster_kurtosis )) if mode or all : methods . append ( RasterStat ( name = \"mode\" , method = raster_mode )) if len ( percentiles ) > 0 : for percentile in percentiles : methods . append ( RasterStat ( name = f \" { percentile } pct\" , method = raster_percentile , pctile = percentile )) return methods","title":"get_raster_stats_methods()"},{"location":"module/types/","text":"elapid.types \u00b6 Custom maxent and typing data types. to_iterable ( var ) \u00b6 Checks and converts variables to an iterable type. Parameters: Name Type Description Default var Any the input variable to check and convert. required Returns: Type Description list var wrapped in a list. Source code in elapid/types.py def to_iterable ( var : Any ) -> list : \"\"\"Checks and converts variables to an iterable type. Args: var: the input variable to check and convert. Returns: `var` wrapped in a list. \"\"\" if not hasattr ( var , \"__iter__\" ): return [ var ] elif isinstance ( var , ( str )): return [ var ] else : return var validate_boolean ( var ) \u00b6 Evaluates whether an argument is boolean True/False Parameters: Name Type Description Default var Any the input argument to validate required Returns: Type Description var the value if it passes validation Exceptions: Type Description AssertionError var was not boolean Source code in elapid/types.py def validate_boolean ( var : Any ) -> bool : \"\"\"Evaluates whether an argument is boolean True/False Args: var: the input argument to validate Returns: var: the value if it passes validation Raises: AssertionError: `var` was not boolean \"\"\" assert isinstance ( var , bool ), \"Argument must be True/False\" return var validate_feature_types ( features ) \u00b6 Ensures the feature classes passed are maxent-legible Parameters: Name Type Description Default features Union[str, list] List or string that must be in [\"linear\", \"quadratic\", \"product\", \"hinge\", \"threshold\", \"auto\"] or string \"lqphta\" required Returns: Type Description valid_features List of formatted valid feature values Source code in elapid/types.py def validate_feature_types ( features : Union [ str , list ]) -> list : \"\"\"Ensures the feature classes passed are maxent-legible Args: features: List or string that must be in [\"linear\", \"quadratic\", \"product\", \"hinge\", \"threshold\", \"auto\"] or string \"lqphta\" Returns: valid_features: List of formatted valid feature values \"\"\" valid_list = get_feature_types ( return_string = False ) valid_string = get_feature_types ( return_string = True ) valid_features = list () # ensure the string features are valid, and translate to a standard feature list if type ( features ) is str : for feature in features : if feature == \"a\" : return valid_list assert feature in valid_string , \"Invalid feature passed: {} \" . format ( feature ) if feature == \"l\" : valid_features . append ( \"linear\" ) elif feature == \"q\" : valid_features . append ( \"quadratic\" ) elif feature == \"p\" : valid_features . append ( \"product\" ) elif feature == \"h\" : valid_features . append ( \"hinge\" ) elif feature == \"t\" : valid_features . append ( \"threshold\" ) # or ensure the list features are valid elif type ( features ) is list : for feature in features : if feature == \"auto\" : return valid_list assert feature in valid_list , \"Invalid feature passed: {} \" . format ( feature ) valid_features . append ( feature ) return valid_features validate_numeric_scalar ( var ) \u00b6 Evaluates whether an argument is a single numeric value. Parameters: Name Type Description Default var Any the input argument to validate required Returns: Type Description var the value if it passes validation Exceptions: Type Description AssertionError var was not numeric. Source code in elapid/types.py def validate_numeric_scalar ( var : Any ) -> bool : \"\"\"Evaluates whether an argument is a single numeric value. Args: var: the input argument to validate Returns: var: the value if it passes validation Raises: AssertionError: `var` was not numeric. \"\"\" assert isinstance ( var , ( int , float )), \"Argument must be single numeric value\" return var","title":"elapid.types"},{"location":"module/types/#elapidtypes","text":"Custom maxent and typing data types.","title":"elapid.types"},{"location":"module/types/#elapid.types.to_iterable","text":"Checks and converts variables to an iterable type. Parameters: Name Type Description Default var Any the input variable to check and convert. required Returns: Type Description list var wrapped in a list. Source code in elapid/types.py def to_iterable ( var : Any ) -> list : \"\"\"Checks and converts variables to an iterable type. Args: var: the input variable to check and convert. Returns: `var` wrapped in a list. \"\"\" if not hasattr ( var , \"__iter__\" ): return [ var ] elif isinstance ( var , ( str )): return [ var ] else : return var","title":"to_iterable()"},{"location":"module/types/#elapid.types.validate_boolean","text":"Evaluates whether an argument is boolean True/False Parameters: Name Type Description Default var Any the input argument to validate required Returns: Type Description var the value if it passes validation Exceptions: Type Description AssertionError var was not boolean Source code in elapid/types.py def validate_boolean ( var : Any ) -> bool : \"\"\"Evaluates whether an argument is boolean True/False Args: var: the input argument to validate Returns: var: the value if it passes validation Raises: AssertionError: `var` was not boolean \"\"\" assert isinstance ( var , bool ), \"Argument must be True/False\" return var","title":"validate_boolean()"},{"location":"module/types/#elapid.types.validate_feature_types","text":"Ensures the feature classes passed are maxent-legible Parameters: Name Type Description Default features Union[str, list] List or string that must be in [\"linear\", \"quadratic\", \"product\", \"hinge\", \"threshold\", \"auto\"] or string \"lqphta\" required Returns: Type Description valid_features List of formatted valid feature values Source code in elapid/types.py def validate_feature_types ( features : Union [ str , list ]) -> list : \"\"\"Ensures the feature classes passed are maxent-legible Args: features: List or string that must be in [\"linear\", \"quadratic\", \"product\", \"hinge\", \"threshold\", \"auto\"] or string \"lqphta\" Returns: valid_features: List of formatted valid feature values \"\"\" valid_list = get_feature_types ( return_string = False ) valid_string = get_feature_types ( return_string = True ) valid_features = list () # ensure the string features are valid, and translate to a standard feature list if type ( features ) is str : for feature in features : if feature == \"a\" : return valid_list assert feature in valid_string , \"Invalid feature passed: {} \" . format ( feature ) if feature == \"l\" : valid_features . append ( \"linear\" ) elif feature == \"q\" : valid_features . append ( \"quadratic\" ) elif feature == \"p\" : valid_features . append ( \"product\" ) elif feature == \"h\" : valid_features . append ( \"hinge\" ) elif feature == \"t\" : valid_features . append ( \"threshold\" ) # or ensure the list features are valid elif type ( features ) is list : for feature in features : if feature == \"auto\" : return valid_list assert feature in valid_list , \"Invalid feature passed: {} \" . format ( feature ) valid_features . append ( feature ) return valid_features","title":"validate_feature_types()"},{"location":"module/types/#elapid.types.validate_numeric_scalar","text":"Evaluates whether an argument is a single numeric value. Parameters: Name Type Description Default var Any the input argument to validate required Returns: Type Description var the value if it passes validation Exceptions: Type Description AssertionError var was not numeric. Source code in elapid/types.py def validate_numeric_scalar ( var : Any ) -> bool : \"\"\"Evaluates whether an argument is a single numeric value. Args: var: the input argument to validate Returns: var: the value if it passes validation Raises: AssertionError: `var` was not numeric. \"\"\" assert isinstance ( var , ( int , float )), \"Argument must be single numeric value\" return var","title":"validate_numeric_scalar()"},{"location":"module/utils/","text":"elapid.utils \u00b6 Backend helper and convenience functions. check_raster_alignment ( raster_paths ) \u00b6 Checks whether the extent, resolution and projection of multiple rasters match exactly. Parameters: Name Type Description Default raster_paths list a list of raster covariate paths required Returns: Type Description bool whether all rasters align Source code in elapid/utils.py def check_raster_alignment ( raster_paths : list ) -> bool : \"\"\"Checks whether the extent, resolution and projection of multiple rasters match exactly. Args: raster_paths: a list of raster covariate paths Returns: whether all rasters align \"\"\" first = raster_paths [ 0 ] rest = raster_paths [ 1 :] with rio . open ( first ) as src : res = src . res bounds = src . bounds transform = src . transform for path in rest : with rio . open ( path ) as src : if src . res != res or src . bounds != bounds or src . transform != transform : return False return True count_raster_bands ( raster_paths ) \u00b6 Returns the total number of bands from a list of rasters. Parameters: Name Type Description Default raster_paths list List of raster data file paths. required Returns: Type Description n_bands total band count. Source code in elapid/utils.py def count_raster_bands ( raster_paths : list ) -> int : \"\"\"Returns the total number of bands from a list of rasters. Args: raster_paths: List of raster data file paths. Returns: n_bands: total band count. \"\"\" n_bands = 0 for path in raster_paths : with rio . open ( path ) as src : n_bands += src . count return n_bands create_output_raster_profile ( raster_paths , template_idx = 0 , windowed = True , nodata = None , count = 1 , compress = None , driver = 'GTiff' , bigtiff = True , dtype = 'float32' ) \u00b6 Gets parameters for windowed reading/writing to output rasters. Parameters: Name Type Description Default raster_paths list raster paths of covariates to apply the model to required template_idx int index of the raster file to use as a template. template_idx=0 sets the first raster as template 0 windowed bool perform a block-by-block data read. slower, but reduces memory use True nodata Union[int, float] output nodata value None count int number of bands in the prediction output 1 output_driver output raster file format (from rasterio.drivers.raster_driver_extensions()) required compress str compression type to apply to the output file None bigtiff bool specify the output file as a bigtiff (for rasters > 2GB) True dtype str rasterio data type string 'float32' Returns: Type Description (windows, profile) an iterable and a dictionary for the window reads and the raster profile Source code in elapid/utils.py def create_output_raster_profile ( raster_paths : list , template_idx : int = 0 , windowed : bool = True , nodata : Number = None , count : int = 1 , compress : str = None , driver : str = \"GTiff\" , bigtiff : bool = True , dtype : str = \"float32\" , ) -> Tuple [ Iterable , Dict ]: \"\"\"Gets parameters for windowed reading/writing to output rasters. Args: raster_paths: raster paths of covariates to apply the model to template_idx: index of the raster file to use as a template. template_idx=0 sets the first raster as template windowed: perform a block-by-block data read. slower, but reduces memory use nodata: output nodata value count: number of bands in the prediction output output_driver: output raster file format (from rasterio.drivers.raster_driver_extensions()) compress: compression type to apply to the output file bigtiff: specify the output file as a bigtiff (for rasters > 2GB) dtype: rasterio data type string Returns: (windows, profile): an iterable and a dictionary for the window reads and the raster profile \"\"\" with rio . open ( raster_paths [ template_idx ]) as src : if windowed : windows = [ window for _ , window in src . block_windows ()] else : windows = [ rio . windows . Window ( 0 , 0 , src . width , src . height )] dst_profile = src . profile . copy () dst_profile . update ( count = count , dtype = dtype , nodata = nodata , compress = compress , driver = driver , ) if bigtiff and driver == \"GTiff\" : dst_profile . update ( BIGTIFF = \"YES\" ) return windows , dst_profile format_band_labels ( raster_paths , labels = None ) \u00b6 Verifies whether a list of band labels matches the band count, or creates labels when none are passed. Parameters: Name Type Description Default raster_paths list required Source code in elapid/utils.py def format_band_labels ( raster_paths : list , labels : list = None ): \"\"\"Verifies whether a list of band labels matches the band count, or creates labels when none are passed. Args: raster_paths: \"\"\" n_bands = count_raster_bands ( raster_paths ) if labels is None : labels = make_band_labels ( n_bands ) n_labels = len ( labels ) assert n_labels == n_bands , f \"number of band labels ( { n_labels } ) != n_bands ( { n_bands } )\" return labels get_raster_band_indexes ( raster_paths ) \u00b6 Counts the number raster bands to index multi-source, multi-band covariates. Parameters: Name Type Description Default raster_paths list a list of raster paths required Returns: Type Description (nbands, band_idx) int and list of the total number of bands and the 0-based start/stop band index for each path Source code in elapid/utils.py def get_raster_band_indexes ( raster_paths : list ) -> Tuple [ int , list ]: \"\"\"Counts the number raster bands to index multi-source, multi-band covariates. Args: raster_paths: a list of raster paths Returns: (nbands, band_idx): int and list of the total number of bands and the 0-based start/stop band index for each path \"\"\" nbands = 0 band_idx = [ 0 ] for i , raster_path in enumerate ( raster_paths ): with rio . open ( raster_path ) as src : nbands += src . count band_idx . append ( band_idx [ i ] + src . count ) return nbands , band_idx get_tqdm () \u00b6 Returns a context-appropriate tqdm progress tracking function. Determines the appropriate tqdm based on the user context, as behavior changes inside/outside of jupyter notebooks. Returns: Type Description tqdm the context-specific tqdm module Source code in elapid/utils.py def get_tqdm () -> Callable : \"\"\"Returns a context-appropriate tqdm progress tracking function. Determines the appropriate tqdm based on the user context, as behavior changes inside/outside of jupyter notebooks. Returns: tqdm: the context-specific tqdm module \"\"\" if in_notebook (): from tqdm.notebook import tqdm else : from tqdm import tqdm return tqdm in_notebook () \u00b6 Evaluate whether the module is currently running in a jupyter notebook. Source code in elapid/utils.py def in_notebook () -> bool : \"\"\"Evaluate whether the module is currently running in a jupyter notebook.\"\"\" return \"ipykernel\" in sys . modules load_object ( path , compressed = True ) \u00b6 Reads a python object into memory that's been saved to disk. Parameters: Name Type Description Default path str the file path of the object to load required compressed bool flag to specify whether the file was compressed prior to saving True Returns: Type Description obj the python object that has been saved (e.g., a MaxentModel() instance) Source code in elapid/utils.py def load_object ( path : str , compressed : bool = True ) -> Any : \"\"\"Reads a python object into memory that's been saved to disk. Args: path: the file path of the object to load compressed: flag to specify whether the file was compressed prior to saving Returns: obj: the python object that has been saved (e.g., a MaxentModel() instance) \"\"\" with open ( path , \"rb\" ) as f : obj = f . read () if compressed : obj = gzip . decompress ( obj ) return pickle . loads ( obj ) load_sample_data ( name = 'bradypus' ) \u00b6 Loads example species presence/background and covariate data. Parameters: Name Type Description Default name str the sample dataset to load. options currently include [\"bradypus\"], from the R 'maxnet' package 'bradypus' Returns: Type Description (x, y) a tuple of dataframes containing covariate and response data, respectively Source code in elapid/utils.py def load_sample_data ( name : str = \"bradypus\" ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\"Loads example species presence/background and covariate data. Args: name: the sample dataset to load. options currently include [\"bradypus\"], from the R 'maxnet' package Returns: (x, y): a tuple of dataframes containing covariate and response data, respectively \"\"\" assert name . lower () in [ \"bradypus\" ], \"Invalid sample data requested\" package_path = os . path . realpath ( __file__ ) package_dir = os . path . dirname ( package_path ) if name . lower () == \"bradypus\" : file_path = os . path . join ( package_dir , \"data\" , \"bradypus.csv.gz\" ) df = pd . read_csv ( file_path , compression = \"gzip\" ) . astype ( \"int64\" ) y = df [ \"presence\" ] . astype ( \"int8\" ) x = df . drop ( columns = [ \"presence\" ]) . astype ({ \"ecoreg\" : \"category\" }) return x , y make_band_labels ( n_bands ) \u00b6 Creates a list of band names to assign as dataframe columns. Parameters: Name Type Description Default n_bands int total number of raster bands to create labels for. required Returns: Type Description labels list of column names. Source code in elapid/utils.py def make_band_labels ( n_bands : int ) -> list : \"\"\"Creates a list of band names to assign as dataframe columns. Args: n_bands: total number of raster bands to create labels for. Returns: labels: list of column names. \"\"\" n_zeros = n_digits ( n_bands ) labels = [ \"b{band_number:0 {n_zeros} d}\" . format ( band_number = i + 1 , n_zeros = n_zeros ) for i in range ( n_bands )] return labels n_digits ( number ) \u00b6 Counts the number of significant integer digits of a number. Parameters: Name Type Description Default number Union[int, float] the number to evaluate. required Returns: Type Description order number of digits required to represent a number Source code in elapid/utils.py def n_digits ( number : Number ) -> int : \"\"\"Counts the number of significant integer digits of a number. Args: number: the number to evaluate. Returns: order: number of digits required to represent a number \"\"\" if number == 0 : order = 1 else : order = np . floor ( np . log10 ( number )) . astype ( int ) + 1 return order repeat_array ( x , length = 1 , axis = 0 ) \u00b6 Repeats a 1D numpy array along an axis to an arbitrary length Parameters: Name Type Description Default x <built-in function array> the n-dimensional array to repeat required length int the number of times to repeat the array 1 axis int the axis along which to repeat the array (valid values include 0 to n+1) 0 Returns: Type Description ndarray An n+1 dimensional numpy array Source code in elapid/utils.py def repeat_array ( x : np . array , length : int = 1 , axis : int = 0 ) -> np . ndarray : \"\"\"Repeats a 1D numpy array along an axis to an arbitrary length Args: x: the n-dimensional array to repeat length: the number of times to repeat the array axis: the axis along which to repeat the array (valid values include 0 to n+1) Returns: An n+1 dimensional numpy array \"\"\" return np . expand_dims ( x , axis = axis ) . repeat ( length , axis = axis ) save_object ( obj , path , compress = True ) \u00b6 Writes a python object to disk for later access. Parameters: Name Type Description Default obj object a python object or variable to be saved (e.g., a MaxentModel() instance) required path str the output file path required Source code in elapid/utils.py def save_object ( obj : object , path : str , compress : bool = True ) -> None : \"\"\"Writes a python object to disk for later access. Args: obj: a python object or variable to be saved (e.g., a MaxentModel() instance) path: the output file path \"\"\" obj = pickle . dumps ( obj ) if compress : obj = gzip . compress ( obj ) with open ( path , \"wb\" ) as f : f . write ( obj )","title":"elapid.utils"},{"location":"module/utils/#elapidutils","text":"Backend helper and convenience functions.","title":"elapid.utils"},{"location":"module/utils/#elapid.utils.check_raster_alignment","text":"Checks whether the extent, resolution and projection of multiple rasters match exactly. Parameters: Name Type Description Default raster_paths list a list of raster covariate paths required Returns: Type Description bool whether all rasters align Source code in elapid/utils.py def check_raster_alignment ( raster_paths : list ) -> bool : \"\"\"Checks whether the extent, resolution and projection of multiple rasters match exactly. Args: raster_paths: a list of raster covariate paths Returns: whether all rasters align \"\"\" first = raster_paths [ 0 ] rest = raster_paths [ 1 :] with rio . open ( first ) as src : res = src . res bounds = src . bounds transform = src . transform for path in rest : with rio . open ( path ) as src : if src . res != res or src . bounds != bounds or src . transform != transform : return False return True","title":"check_raster_alignment()"},{"location":"module/utils/#elapid.utils.count_raster_bands","text":"Returns the total number of bands from a list of rasters. Parameters: Name Type Description Default raster_paths list List of raster data file paths. required Returns: Type Description n_bands total band count. Source code in elapid/utils.py def count_raster_bands ( raster_paths : list ) -> int : \"\"\"Returns the total number of bands from a list of rasters. Args: raster_paths: List of raster data file paths. Returns: n_bands: total band count. \"\"\" n_bands = 0 for path in raster_paths : with rio . open ( path ) as src : n_bands += src . count return n_bands","title":"count_raster_bands()"},{"location":"module/utils/#elapid.utils.create_output_raster_profile","text":"Gets parameters for windowed reading/writing to output rasters. Parameters: Name Type Description Default raster_paths list raster paths of covariates to apply the model to required template_idx int index of the raster file to use as a template. template_idx=0 sets the first raster as template 0 windowed bool perform a block-by-block data read. slower, but reduces memory use True nodata Union[int, float] output nodata value None count int number of bands in the prediction output 1 output_driver output raster file format (from rasterio.drivers.raster_driver_extensions()) required compress str compression type to apply to the output file None bigtiff bool specify the output file as a bigtiff (for rasters > 2GB) True dtype str rasterio data type string 'float32' Returns: Type Description (windows, profile) an iterable and a dictionary for the window reads and the raster profile Source code in elapid/utils.py def create_output_raster_profile ( raster_paths : list , template_idx : int = 0 , windowed : bool = True , nodata : Number = None , count : int = 1 , compress : str = None , driver : str = \"GTiff\" , bigtiff : bool = True , dtype : str = \"float32\" , ) -> Tuple [ Iterable , Dict ]: \"\"\"Gets parameters for windowed reading/writing to output rasters. Args: raster_paths: raster paths of covariates to apply the model to template_idx: index of the raster file to use as a template. template_idx=0 sets the first raster as template windowed: perform a block-by-block data read. slower, but reduces memory use nodata: output nodata value count: number of bands in the prediction output output_driver: output raster file format (from rasterio.drivers.raster_driver_extensions()) compress: compression type to apply to the output file bigtiff: specify the output file as a bigtiff (for rasters > 2GB) dtype: rasterio data type string Returns: (windows, profile): an iterable and a dictionary for the window reads and the raster profile \"\"\" with rio . open ( raster_paths [ template_idx ]) as src : if windowed : windows = [ window for _ , window in src . block_windows ()] else : windows = [ rio . windows . Window ( 0 , 0 , src . width , src . height )] dst_profile = src . profile . copy () dst_profile . update ( count = count , dtype = dtype , nodata = nodata , compress = compress , driver = driver , ) if bigtiff and driver == \"GTiff\" : dst_profile . update ( BIGTIFF = \"YES\" ) return windows , dst_profile","title":"create_output_raster_profile()"},{"location":"module/utils/#elapid.utils.format_band_labels","text":"Verifies whether a list of band labels matches the band count, or creates labels when none are passed. Parameters: Name Type Description Default raster_paths list required Source code in elapid/utils.py def format_band_labels ( raster_paths : list , labels : list = None ): \"\"\"Verifies whether a list of band labels matches the band count, or creates labels when none are passed. Args: raster_paths: \"\"\" n_bands = count_raster_bands ( raster_paths ) if labels is None : labels = make_band_labels ( n_bands ) n_labels = len ( labels ) assert n_labels == n_bands , f \"number of band labels ( { n_labels } ) != n_bands ( { n_bands } )\" return labels","title":"format_band_labels()"},{"location":"module/utils/#elapid.utils.get_raster_band_indexes","text":"Counts the number raster bands to index multi-source, multi-band covariates. Parameters: Name Type Description Default raster_paths list a list of raster paths required Returns: Type Description (nbands, band_idx) int and list of the total number of bands and the 0-based start/stop band index for each path Source code in elapid/utils.py def get_raster_band_indexes ( raster_paths : list ) -> Tuple [ int , list ]: \"\"\"Counts the number raster bands to index multi-source, multi-band covariates. Args: raster_paths: a list of raster paths Returns: (nbands, band_idx): int and list of the total number of bands and the 0-based start/stop band index for each path \"\"\" nbands = 0 band_idx = [ 0 ] for i , raster_path in enumerate ( raster_paths ): with rio . open ( raster_path ) as src : nbands += src . count band_idx . append ( band_idx [ i ] + src . count ) return nbands , band_idx","title":"get_raster_band_indexes()"},{"location":"module/utils/#elapid.utils.get_tqdm","text":"Returns a context-appropriate tqdm progress tracking function. Determines the appropriate tqdm based on the user context, as behavior changes inside/outside of jupyter notebooks. Returns: Type Description tqdm the context-specific tqdm module Source code in elapid/utils.py def get_tqdm () -> Callable : \"\"\"Returns a context-appropriate tqdm progress tracking function. Determines the appropriate tqdm based on the user context, as behavior changes inside/outside of jupyter notebooks. Returns: tqdm: the context-specific tqdm module \"\"\" if in_notebook (): from tqdm.notebook import tqdm else : from tqdm import tqdm return tqdm","title":"get_tqdm()"},{"location":"module/utils/#elapid.utils.in_notebook","text":"Evaluate whether the module is currently running in a jupyter notebook. Source code in elapid/utils.py def in_notebook () -> bool : \"\"\"Evaluate whether the module is currently running in a jupyter notebook.\"\"\" return \"ipykernel\" in sys . modules","title":"in_notebook()"},{"location":"module/utils/#elapid.utils.load_object","text":"Reads a python object into memory that's been saved to disk. Parameters: Name Type Description Default path str the file path of the object to load required compressed bool flag to specify whether the file was compressed prior to saving True Returns: Type Description obj the python object that has been saved (e.g., a MaxentModel() instance) Source code in elapid/utils.py def load_object ( path : str , compressed : bool = True ) -> Any : \"\"\"Reads a python object into memory that's been saved to disk. Args: path: the file path of the object to load compressed: flag to specify whether the file was compressed prior to saving Returns: obj: the python object that has been saved (e.g., a MaxentModel() instance) \"\"\" with open ( path , \"rb\" ) as f : obj = f . read () if compressed : obj = gzip . decompress ( obj ) return pickle . loads ( obj )","title":"load_object()"},{"location":"module/utils/#elapid.utils.load_sample_data","text":"Loads example species presence/background and covariate data. Parameters: Name Type Description Default name str the sample dataset to load. options currently include [\"bradypus\"], from the R 'maxnet' package 'bradypus' Returns: Type Description (x, y) a tuple of dataframes containing covariate and response data, respectively Source code in elapid/utils.py def load_sample_data ( name : str = \"bradypus\" ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\"Loads example species presence/background and covariate data. Args: name: the sample dataset to load. options currently include [\"bradypus\"], from the R 'maxnet' package Returns: (x, y): a tuple of dataframes containing covariate and response data, respectively \"\"\" assert name . lower () in [ \"bradypus\" ], \"Invalid sample data requested\" package_path = os . path . realpath ( __file__ ) package_dir = os . path . dirname ( package_path ) if name . lower () == \"bradypus\" : file_path = os . path . join ( package_dir , \"data\" , \"bradypus.csv.gz\" ) df = pd . read_csv ( file_path , compression = \"gzip\" ) . astype ( \"int64\" ) y = df [ \"presence\" ] . astype ( \"int8\" ) x = df . drop ( columns = [ \"presence\" ]) . astype ({ \"ecoreg\" : \"category\" }) return x , y","title":"load_sample_data()"},{"location":"module/utils/#elapid.utils.make_band_labels","text":"Creates a list of band names to assign as dataframe columns. Parameters: Name Type Description Default n_bands int total number of raster bands to create labels for. required Returns: Type Description labels list of column names. Source code in elapid/utils.py def make_band_labels ( n_bands : int ) -> list : \"\"\"Creates a list of band names to assign as dataframe columns. Args: n_bands: total number of raster bands to create labels for. Returns: labels: list of column names. \"\"\" n_zeros = n_digits ( n_bands ) labels = [ \"b{band_number:0 {n_zeros} d}\" . format ( band_number = i + 1 , n_zeros = n_zeros ) for i in range ( n_bands )] return labels","title":"make_band_labels()"},{"location":"module/utils/#elapid.utils.n_digits","text":"Counts the number of significant integer digits of a number. Parameters: Name Type Description Default number Union[int, float] the number to evaluate. required Returns: Type Description order number of digits required to represent a number Source code in elapid/utils.py def n_digits ( number : Number ) -> int : \"\"\"Counts the number of significant integer digits of a number. Args: number: the number to evaluate. Returns: order: number of digits required to represent a number \"\"\" if number == 0 : order = 1 else : order = np . floor ( np . log10 ( number )) . astype ( int ) + 1 return order","title":"n_digits()"},{"location":"module/utils/#elapid.utils.repeat_array","text":"Repeats a 1D numpy array along an axis to an arbitrary length Parameters: Name Type Description Default x <built-in function array> the n-dimensional array to repeat required length int the number of times to repeat the array 1 axis int the axis along which to repeat the array (valid values include 0 to n+1) 0 Returns: Type Description ndarray An n+1 dimensional numpy array Source code in elapid/utils.py def repeat_array ( x : np . array , length : int = 1 , axis : int = 0 ) -> np . ndarray : \"\"\"Repeats a 1D numpy array along an axis to an arbitrary length Args: x: the n-dimensional array to repeat length: the number of times to repeat the array axis: the axis along which to repeat the array (valid values include 0 to n+1) Returns: An n+1 dimensional numpy array \"\"\" return np . expand_dims ( x , axis = axis ) . repeat ( length , axis = axis )","title":"repeat_array()"},{"location":"module/utils/#elapid.utils.save_object","text":"Writes a python object to disk for later access. Parameters: Name Type Description Default obj object a python object or variable to be saved (e.g., a MaxentModel() instance) required path str the output file path required Source code in elapid/utils.py def save_object ( obj : object , path : str , compress : bool = True ) -> None : \"\"\"Writes a python object to disk for later access. Args: obj: a python object or variable to be saved (e.g., a MaxentModel() instance) path: the output file path \"\"\" obj = pickle . dumps ( obj ) if compress : obj = gzip . compress ( obj ) with open ( path , \"wb\" ) as f : f . write ( obj )","title":"save_object()"},{"location":"sdm/maxent/","text":"Maxent SDM \u00b6 Maxent is a species distribution modeling (SDM) system, which uses species observations and environmental data to predict where a species might be found under past, present or future environmental conditions. Its a presence/background model, meaning it uses data on where a species is present and, instead of data on where it is absent, a random sample of the region where you might expect to find that species. This is convenient because it reduces data collection burdens (absence data isn't required) but it also introduces some unique challenges for fitting and interpreting models. Formally, Maxent estimates habitat suitability (i.e. the fundamental niche) using species occurrence records ( y = 1 ), randomly sampled \"background\" location records ( y = 0 ), and environmental covariate data ( x ) for each location of y . Maxent doesn't directly estimate relationships between presence/background data and environmental covariates (so, not just y ~ x ). Instead, it fits a series of feature transformatons ( z ) to the covariate data (e.g. computing pairwise products between covariates, setting random covariate thresholds). Maxent then estimates the conditional probability of finding a species given a set of environmental conditions as: Pr(y = 1 | f(z)) = (f1(z) * Pr(y = 1)) / f(z) elapid provides python tools for fitting Maxent models, computing features, and applying models to raster data. Below are some instructions for the first two; the latter is reviewed here . Maxent in elapid \u00b6 Training models \u00b6 There are two primary Maxent functions: fitting features and fitting models. You can do it all at once with: import elapid x , y = elapid . load_sample_data () model = elapid . MaxentModel () model . fit ( x , y ) Where: x is an ndarray or dataframe of environmental covariates of shape ( n_samples , n_covariates ) y is an ndarray or series of species presence/background labels (rows labeled 1 or 0 ) The elapid.MaxentModel() object takes these data, fits features from covariate data, computes sample weights and feature regularization, fits a series of models, and returns an estimator that can be used for applying predictions to new data. MaxentModel() behaves like an sklearn estimator class. Use model.fit(x, y) to train a model, and model.predict(x) to generate model predictions. Feature transformations \u00b6 You can also generate and evaluate features before passing them to the model: features = elapid . MaxentFeatureTransformer () z = features . fit_transform ( x ) model . fit ( z , y , is_features = True ) MaxentFeatureTransformer() behaves like an sklearn preprocessing class. Use features.fit(x_train) to fit features, features.transform(x_test) to apply to new covariate data, or features.fit_transform(x) to fit features and return the transformed data. Setting the is_features=True flag is important here because, by default, the MaxentModel() class will automatically fit and apply feature tranformations Configuration \u00b6 The base Maxent classes can be modified with parameters that are available in other Maxent implementations: model = elapid . MaxentModel ( feature_types = [ 'linear' , 'hinge' , 'product' ], # the feature transformations tau = 0.5 , # prevalence scaler clamp = True , # set covariate min/max based on range of training data scorer = 'roc_auc' , # metric to optimize (from sklearn.metrics.SCORERS) beta_multiplier = 1.0 , # regularization scaler (high values drop more features) beta_lqp = 1.0 , # linear, quadratic, product regularization scaler beta_hinge = 1.0 , # hinge regularization scaler beta_threshold = 1.0 , # threshold regularization scaler beta_categorical = 1.0 , # categorical regularization scaler n_hinge_features = 10 , # number of hinge features to compute n_threshold_features = 10 , # number of threshold features to compute convergence_tolerance = 1e-07 , # model fit convergence threshold use_lambdas = 'best' , # set to 'best' (least overfit), 'last' (highest score) n_cpus = 4 , # number of cpu cores to use ) You can find the default configuration parameters in elapid/config . Additional reading \u00b6 A practical guide to MaxEnt for modeling species\u2019 distributions: what it does, and why inputs and settings matter, Merow et al. 2013 [ pdf ] A statistical explanation of MaxEnt for ecologists, Elith et al. 2011 [ pdf ] Opening the black box: an open-source release of Maxent, Phillips et al. 2017 [ pdf ] Modeling of species distributions with Maxent: new extensions and a comprehensive evaluation, Philliips et al. 2008 [ pdf ]","title":"Maxent"},{"location":"sdm/maxent/#maxent-sdm","text":"Maxent is a species distribution modeling (SDM) system, which uses species observations and environmental data to predict where a species might be found under past, present or future environmental conditions. Its a presence/background model, meaning it uses data on where a species is present and, instead of data on where it is absent, a random sample of the region where you might expect to find that species. This is convenient because it reduces data collection burdens (absence data isn't required) but it also introduces some unique challenges for fitting and interpreting models. Formally, Maxent estimates habitat suitability (i.e. the fundamental niche) using species occurrence records ( y = 1 ), randomly sampled \"background\" location records ( y = 0 ), and environmental covariate data ( x ) for each location of y . Maxent doesn't directly estimate relationships between presence/background data and environmental covariates (so, not just y ~ x ). Instead, it fits a series of feature transformatons ( z ) to the covariate data (e.g. computing pairwise products between covariates, setting random covariate thresholds). Maxent then estimates the conditional probability of finding a species given a set of environmental conditions as: Pr(y = 1 | f(z)) = (f1(z) * Pr(y = 1)) / f(z) elapid provides python tools for fitting Maxent models, computing features, and applying models to raster data. Below are some instructions for the first two; the latter is reviewed here .","title":"Maxent SDM"},{"location":"sdm/maxent/#maxent-in-elapid","text":"","title":"Maxent in elapid"},{"location":"sdm/maxent/#training-models","text":"There are two primary Maxent functions: fitting features and fitting models. You can do it all at once with: import elapid x , y = elapid . load_sample_data () model = elapid . MaxentModel () model . fit ( x , y ) Where: x is an ndarray or dataframe of environmental covariates of shape ( n_samples , n_covariates ) y is an ndarray or series of species presence/background labels (rows labeled 1 or 0 ) The elapid.MaxentModel() object takes these data, fits features from covariate data, computes sample weights and feature regularization, fits a series of models, and returns an estimator that can be used for applying predictions to new data. MaxentModel() behaves like an sklearn estimator class. Use model.fit(x, y) to train a model, and model.predict(x) to generate model predictions.","title":"Training models"},{"location":"sdm/maxent/#feature-transformations","text":"You can also generate and evaluate features before passing them to the model: features = elapid . MaxentFeatureTransformer () z = features . fit_transform ( x ) model . fit ( z , y , is_features = True ) MaxentFeatureTransformer() behaves like an sklearn preprocessing class. Use features.fit(x_train) to fit features, features.transform(x_test) to apply to new covariate data, or features.fit_transform(x) to fit features and return the transformed data. Setting the is_features=True flag is important here because, by default, the MaxentModel() class will automatically fit and apply feature tranformations","title":"Feature transformations"},{"location":"sdm/maxent/#configuration","text":"The base Maxent classes can be modified with parameters that are available in other Maxent implementations: model = elapid . MaxentModel ( feature_types = [ 'linear' , 'hinge' , 'product' ], # the feature transformations tau = 0.5 , # prevalence scaler clamp = True , # set covariate min/max based on range of training data scorer = 'roc_auc' , # metric to optimize (from sklearn.metrics.SCORERS) beta_multiplier = 1.0 , # regularization scaler (high values drop more features) beta_lqp = 1.0 , # linear, quadratic, product regularization scaler beta_hinge = 1.0 , # hinge regularization scaler beta_threshold = 1.0 , # threshold regularization scaler beta_categorical = 1.0 , # categorical regularization scaler n_hinge_features = 10 , # number of hinge features to compute n_threshold_features = 10 , # number of threshold features to compute convergence_tolerance = 1e-07 , # model fit convergence threshold use_lambdas = 'best' , # set to 'best' (least overfit), 'last' (highest score) n_cpus = 4 , # number of cpu cores to use ) You can find the default configuration parameters in elapid/config .","title":"Configuration"},{"location":"sdm/maxent/#additional-reading","text":"A practical guide to MaxEnt for modeling species\u2019 distributions: what it does, and why inputs and settings matter, Merow et al. 2013 [ pdf ] A statistical explanation of MaxEnt for ecologists, Elith et al. 2011 [ pdf ] Opening the black box: an open-source release of Maxent, Phillips et al. 2017 [ pdf ] Modeling of species distributions with Maxent: new extensions and a comprehensive evaluation, Philliips et al. 2008 [ pdf ]","title":"Additional reading"},{"location":"sdm/nicheenvelope/","text":"Niche Envelope Models \u00b6 Docs coming soon.","title":"NicheEnvelope"},{"location":"sdm/nicheenvelope/#niche-envelope-models","text":"Docs coming soon.","title":"Niche Envelope Models"}]}